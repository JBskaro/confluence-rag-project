#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –æ–∂–∏–¥–∞–µ–º—ã–º–∏
"""
import json
import re
from typing import Dict, List, Tuple

EXPECTED_RESULTS = {
    "test_1": {
        "query": "–ü—Ä–æ–≤–æ–∂—É –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ –±–ª–æ–∫—É –°–∫–ª–∞–¥, –∞ —Ç–æ—á–Ω–µ–µ –£—á–µ—Ç –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã. –ü–æ–¥–≥–æ—Ç–æ–≤—å —Å–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤.",
        "expected_page_id": "18153754",
        "expected_keywords": [
            "–Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä", "–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü", "—Å–µ—Ä–∏–∏", "—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫", 
            "–≥–∞–±–∞—Ä–∏—Ç", "—à—Ç—Ä–∏—Ö–∫–æ–¥", "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã",
            "–£—á–µ—Ç –¥–≤–∏–∂–µ–Ω–∏—è –∏ –æ—Å—Ç–∞—Ç–∫–æ–≤", "–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã"
        ],
        "expected_questions_count": 30,
        "expected_sections": [
            "1. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã",
            "2. –£—á–µ—Ç –¥–≤–∏–∂–µ–Ω–∏—è –∏ –æ—Å—Ç–∞—Ç–∫–æ–≤",
            "3. –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã",
            "4. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ —Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–≤–∞—Ä–æ–≤",
            "5. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã"
        ]
    },
    "test_2": {
        "query": "–ö–∫–æ–π —Ç—Ö–Ω–æ–ª–æ–≥–∏–µ—Å–∫–∏–π —Å—Ç–µ–∫ –∏—Å–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –ø—Ä–æ–µ–∫—Ç —Ä–∞—É –∏–∏.",
        "expected_page_id": "18153591",
        "expected_keywords": [
            "Ollama", "OpenRouter", "LiteLLM", "MCP", 
            "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏", "—Å—Ç–µ–∫", "Syntaxcheck", "Docsearch",
            "Codesearch", "Templatesearch", "Open WebUI"
        ],
        "expected_sections": [
            "–†–∞–±–æ—Ç–∞ —Å –ò–ò –º–æ–¥–µ–ª—è–º–∏",
            "–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∏ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è"
        ]
    }
}

def analyze_result(test_name: str, actual_text: str) -> Dict:
    """–ê–Ω–∞–ª–∏–∑ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
    expected = EXPECTED_RESULTS[test_name]
    
    print(f"\n{'='*80}")
    print(f"–ê–ù–ê–õ–ò–ó: {test_name.upper()}")
    print(f"{'='*80}")
    print(f"\n–ó–∞–ø—Ä–æ—Å: {expected['query'][:80]}...")
    print(f"–î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {len(actual_text)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ page_id
    page_found = expected['expected_page_id'] in actual_text
    print(f"\n[{'‚úÖ' if page_found else '‚ùå'}] –°—Ç—Ä–∞–Ω–∏—Ü–∞ {expected['expected_page_id']}: {'–ù–ê–ô–î–ï–ù–ê' if page_found else '–ù–ï –ù–ê–ô–î–ï–ù–ê'}")
    
    # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö page_id –≤ —Ç–µ–∫—Å—Ç–µ
    page_ids = re.findall(r'\d{8,}', actual_text)
    if page_ids:
        print(f"  –ù–∞–π–¥–µ–Ω–Ω—ã–µ page_id –≤ –æ—Ç–≤–µ—Ç–µ: {', '.join(set(page_ids))}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    print(f"\n–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤:")
    found_keywords = []
    missing_keywords = []
    
    for keyword in expected['expected_keywords']:
        if keyword.lower() in actual_text.lower():
            found_keywords.append(keyword)
        else:
            missing_keywords.append(keyword)
    
    print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ: {len(found_keywords)}/{len(expected['expected_keywords'])}")
    if found_keywords:
        print(f"     {', '.join(found_keywords[:10])}")
    if missing_keywords:
        print(f"  ‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç: {', '.join(missing_keywords[:10])}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–¥–µ–ª–æ–≤ (–¥–ª—è —Ç–µ—Å—Ç–∞ 1)
    if 'expected_sections' in expected:
        print(f"\n–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–¥–µ–ª–æ–≤:")
        found_sections = []
        for section in expected['expected_sections']:
            if section.lower() in actual_text.lower():
                found_sections.append(section)
        
        print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ä–∞–∑–¥–µ–ª–æ–≤: {len(found_sections)}/{len(expected['expected_sections'])}")
        if found_sections:
            for section in found_sections:
                print(f"     - {section}")
    
    # –ü–æ–¥—Å—á–µ—Ç –≤–æ–ø—Ä–æ—Å–æ–≤ (–¥–ª—è —Ç–µ—Å—Ç–∞ 1)
    if 'expected_questions_count' in expected:
        question_pattern = r'\d+\.\s+[–ê-–Ø–Å]'
        questions = re.findall(question_pattern, actual_text)
        print(f"\n  üìä –ù–∞–π–¥–µ–Ω–æ –≤–æ–ø—Ä–æ—Å–æ–≤ (–ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É): {len(questions)}")
        print(f"  –û–∂–∏–¥–∞–ª–æ—Å—å –º–∏–Ω–∏–º—É–º: {expected['expected_questions_count']}")
    
    # –ü–æ–∏—Å–∫ page_id –≤ —Ç–µ–∫—Å—Ç–µ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    if expected['expected_page_id'] in actual_text:
        idx = actual_text.find(expected['expected_page_id'])
        context = actual_text[max(0, idx-100):idx+200]
        print(f"\n  –ö–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ page_id:")
        print(f"     ...{context}...")
    else:
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞—á–∞–ª–æ –æ—Ç–≤–µ—Ç–∞
        print(f"\n  –ù–∞—á–∞–ª–æ –æ—Ç–≤–µ—Ç–∞:")
        print(f"     {actual_text[:300]}...")
    
    # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞
    score = 0
    max_score = 100
    
    if page_found:
        score += 40
    else:
        print(f"\n  ‚ö†Ô∏è  –ö–†–ò–¢–ò–ß–ù–û: –û–∂–∏–¥–∞–µ–º–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
    
    keyword_score = (len(found_keywords) / len(expected['expected_keywords'])) * 40
    score += keyword_score
    
    if 'expected_sections' in expected:
        section_score = (len(found_sections) / len(expected['expected_sections'])) * 20
        score += section_score
    
    print(f"\n{'='*80}")
    print(f"–û–¶–ï–ù–ö–ê: {score:.1f}/100")
    print(f"{'='*80}")
    
    if score >= 80:
        print("‚úÖ –†–ï–ó–£–õ–¨–¢–ê–¢: –û–¢–õ–ò–ß–ù–û")
    elif score >= 60:
        print("‚ö†Ô∏è  –†–ï–ó–£–õ–¨–¢–ê–¢: –•–û–†–û–®–û (–µ—Å—Ç—å —É–ª—É—á—à–µ–Ω–∏—è)")
    elif score >= 40:
        print("‚ùå –†–ï–ó–£–õ–¨–¢–ê–¢: –ü–õ–û–•–û (–Ω—É–∂–Ω—ã —É–ª—É—á—à–µ–Ω–∏—è)")
    else:
        print("‚ùå –†–ï–ó–£–õ–¨–¢–ê–¢: –û–¢–í–†–ê–¢–ò–¢–ï–õ–¨–ù–û (–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã)")
    
    return {
        "page_found": page_found,
        "keywords_found": len(found_keywords),
        "keywords_total": len(expected['expected_keywords']),
        "score": score,
        "found_page_ids": list(set(page_ids)) if page_ids else []
    }

def main():
    print("\n" + "="*80)
    print("–ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ü–û–ò–°–ö–ê")
    print("="*80)
    print("\n–í—Å—Ç–∞–≤—å—Ç–µ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ –∏–∑ Open WebUI.")
    print("–î–ª—è –≤—ã—Ö–æ–¥–∞ –≤–≤–µ–¥–∏—Ç–µ 'exit'")
    
    results = {}
    
    for test_name, expected in EXPECTED_RESULTS.items():
        print(f"\n{'='*80}")
        print(f"–¢–ï–°–¢: {test_name}")
        print(f"{'='*80}")
        print(f"–ó–∞–ø—Ä–æ—Å: {expected['query']}")
        print(f"–û–∂–∏–¥–∞–µ–º–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {expected['expected_page_id']}")
        
        user_input = input("\n–í—Å—Ç–∞–≤—å—Ç–µ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ (–∏–ª–∏ 'skip' –¥–ª—è –ø—Ä–æ–ø—É—Å–∫–∞):\n")
        
        if user_input.lower() in ['exit', 'quit']:
            break
        if user_input.lower() == 'skip':
            continue
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ JSON (–µ—Å–ª–∏ —ç—Ç–æ –æ—Ç–≤–µ—Ç –æ—Ç MCP)
        try:
            json_data = json.loads(user_input)
            if 'result' in json_data and 'content' in json_data['result']:
                actual_text = json_data['result']['content'][0].get('text', '')
            else:
                actual_text = user_input
        except:
            actual_text = user_input
        
        results[test_name] = analyze_result(test_name, actual_text)
    
    # –ò—Ç–æ–≥–∏
    if results:
        print(f"\n{'='*80}")
        print("–ò–¢–û–ì–û–í–ê–Ø –°–í–û–î–ö–ê")
        print(f"{'='*80}")
        
        for test_name, result in results.items():
            status = "‚úÖ" if result['page_found'] else "‚ùå"
            print(f"{status} {test_name}: {result['score']:.1f}/100 "
                  f"(keywords: {result['keywords_found']}/{result['keywords_total']}, "
                  f"pages: {', '.join(result['found_page_ids']) if result['found_page_ids'] else '–Ω–µ—Ç'})")
        
        avg_score = sum(r['score'] for r in results.values()) / len(results)
        print(f"\n–°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {avg_score:.1f}/100")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        print(f"\n{'='*80}")
        print("–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
        print(f"{'='*80}")
        
        if avg_score < 40:
            print("‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ü–†–û–ë–õ–ï–ú–´:")
            print("   1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –∏–Ω–¥–µ–∫—Å –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω (Documents > 0)")
            print("   2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ —É–∫–∞–∑–∞–Ω –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π space (RAUII, –∞ –Ω–µ Surveys)")
            print("   3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ—Ä–æ–≥–∏ reranking (RERANK_THRESHOLD_*)")
            print("   4. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ Query Rewriting —Ä–∞–±–æ—Ç–∞–µ—Ç (Ollama fallback)")
        elif avg_score < 60:
            print("‚ö†Ô∏è  –ù–£–ñ–ù–´ –£–õ–£–ß–®–ï–ù–ò–Ø:")
            print("   1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ—Ä–æ–≥–∏ reranking - –≤–æ–∑–º–æ–∂–Ω–æ, –æ–Ω–∏ —Å–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–µ")
            print("   2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ Query Rewriting —Ä–∞–±–æ—Ç–∞–µ—Ç")
            print("   3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ —É–∫–∞–∑–∞–Ω –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π space")

if __name__ == "__main__":
    main()

