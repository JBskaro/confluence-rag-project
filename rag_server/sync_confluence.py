"""
–°–µ—Ä–≤–∏—Å —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ Confluence –≤ PostgreSQL + Qdrant.
–í—ã–ø–æ–ª–Ω—è–µ—Ç –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—É—é —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —É–º–Ω—ã–º chunking.
–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: Confluence ‚Üí PostgreSQL ‚Üí Qdrant
"""
import os
import sys
import json
import time
import logging
import re
from typing import List, Dict, Any, Optional
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache
from collections import OrderedDict
from threading import RLock

import html2text
import requests
import urllib3
from atlassian import Confluence
# llama-index –∏–º–ø–æ—Ä—Ç—ã —É–¥–∞–ª–µ–Ω—ã - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º—É—é —Ä–∞–±–æ—Ç—É —Å Qdrant
# QdrantVectorStore –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤ qdrant_storage.py –¥–ª—è –ø–æ–∏—Å–∫–∞ (get_qdrant_vector_store)
from qdrant_storage import (
    insert_chunk_to_qdrant,
    insert_chunks_batch_to_qdrant,
    init_qdrant_client
)
from embeddings import generate_query_embedding
from tenacity import retry, stop_after_attempt, wait_exponential
from bs4 import BeautifulSoup, NavigableString
import html  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –î–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è HTML entities

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è logger (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)
logger = logging.getLogger(__name__)

# === SEMANTIC CHUNKING (LangChain) ===
try:
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    HAS_LANGCHAIN = True
except ImportError:
    HAS_LANGCHAIN = False
    logger.warning("langchain not available, using basic chunking")

# === METADATA EXTRACTION LIMITS ===
# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –∏–∑–≤–ª–µ–∫–∞–µ–º—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
MAX_HEADINGS_EXTRACT = int(os.getenv('MAX_HEADINGS_EXTRACT', '50'))
MAX_BREADCRUMB_LENGTH = int(os.getenv('MAX_BREADCRUMB_LENGTH', '200'))
MAX_BREADCRUMB_LEVELS = int(os.getenv('MAX_BREADCRUMB_LEVELS', '5'))
MAX_HEADINGS_STRING_LENGTH = int(os.getenv('MAX_HEADINGS_STRING_LENGTH', '2000'))

# === METADATA SANITIZATION ===
MAX_METADATA_SIZE = 1000  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Å—Ç—Ä–æ–∫–æ–≤–æ–≥–æ –ø–æ–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
MAX_METADATA_LIST_SIZE = 10  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Å–ø–∏—Å–∫–∞ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö

def sanitize_metadata(metadata: Dict[str, Any]) -> Dict[str, Any]:
    """
    –û–±—Ä–µ–∑–∞–µ—Ç –±–æ–ª—å—à–∏–µ –ø–æ–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∏–∑–±—ã—Ç–æ—á–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è.
    
    –ö–†–ò–¢–ò–ß–ù–û: –ü–æ–ª–µ 'text' –ù–ï –æ–±—Ä–µ–∑–∞–µ—Ç—Å—è, —Ç.–∫. —ç—Ç–æ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —á–∞–Ω–∫–∞.
    
    Args:
        metadata: –ò—Å—Ö–æ–¥–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    
    Returns:
        –û—á–∏—â–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º –ø–æ–ª–µ–π
    """
    sanitized = {}
    trimmed_fields = []
    trimmed_lists = []
    
    for key, value in metadata.items():
        if isinstance(value, str):
            # –ö–†–ò–¢–ò–ß–ù–û: –ü–æ–ª–µ 'text' –Ω–µ –æ–±—Ä–µ–∑–∞–µ–º - —ç—Ç–æ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —á–∞–Ω–∫–∞
            if key == 'text':
                sanitized[key] = value
            # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
            elif len(value) > MAX_METADATA_SIZE:
                trimmed_fields.append(f"{key}:{len(value)}‚Üí{MAX_METADATA_SIZE}")
                sanitized[key] = value[:MAX_METADATA_SIZE]
            else:
                sanitized[key] = value
        elif isinstance(value, list):
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä —Å–ø–∏—Å–∫–æ–≤
            if len(value) > MAX_METADATA_LIST_SIZE:
                trimmed_lists.append(f"{key}:{len(value)}‚Üí{MAX_METADATA_LIST_SIZE}")
                sanitized[key] = value[:MAX_METADATA_LIST_SIZE]
            else:
                sanitized[key] = value
        elif isinstance(value, dict):
            # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞—Ä–∏
            sanitized[key] = sanitize_metadata(value)
        else:
            sanitized[key] = value
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    if trimmed_fields or trimmed_lists:
        logger.debug(
            f"Trimmed metadata: fields={trimmed_fields}, lists={trimmed_lists}"
        )
    
    return sanitized

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥—É–ª–∏ –¥–ª—è PostgreSQL –∏ Qdrant
sys.path.insert(0, os.path.dirname(__file__))
from postgres_storage import (
    init_postgres_schema,
    save_page_to_postgres,
    get_pages_from_postgres,
    mark_as_indexed,
    cleanup_deleted_pages_postgres,
    get_postgres_stats
)
from qdrant_storage import (
    init_qdrant_client,
    init_qdrant_collection,
    get_qdrant_vector_store,
    delete_points_by_page_id,
    get_qdrant_count
)

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑ ENV
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
logging.basicConfig(
    level=getattr(logging, LOG_LEVEL, logging.INFO), 
    format='%(asctime)s - %(levelname)s - %(message)s'
)
# logger —É–∂–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω –≤—ã—à–µ (—Å—Ç—Ä–æ–∫–∞ 30)

# –û—Ç–∫–ª—é—á–∞–µ–º –∏–∑–±—ã—Ç–æ—á–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ HTTP –∑–∞–ø—Ä–æ—Å–æ–≤ –æ—Ç httpx/openai
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)
logging.getLogger("openai").setLevel(logging.WARNING)

CONFLUENCE_URL = os.getenv("CONFLUENCE_URL")
CONFLUENCE_TOKEN = os.getenv("CONFLUENCE_TOKEN")

if not CONFLUENCE_URL or not CONFLUENCE_TOKEN:
    logger.error("CONFLUENCE_URL and CONFLUENCE_TOKEN required")
    sys.exit(1)

def get_int_env(name: str, default: int) -> int:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ integer ENV –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π."""
    try:
        value = int(os.getenv(name, str(default)))
        if value <= 0:
            logger.warning(f"{name}={value} –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É—é default={default}")
            return default
        return value
    except (ValueError, TypeError):
        logger.warning(f"{name} –Ω–µ–≤–∞–ª–∏–¥–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É—é default={default}")
        return default

STATE_FILE = os.getenv("STATE_FILE", "./data/sync_state.json")
USE_OLLAMA = os.getenv("USE_OLLAMA", "false").lower() == "true"
# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–æ–¥—É–ª—å embeddings
# sys —É–∂–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –≤ –Ω–∞—á–∞–ª–µ —Ñ–∞–π–ª–∞ (—Å—Ç—Ä–æ–∫–∞ 7)
sys.path.insert(0, os.path.dirname(__file__))
from embeddings import get_embed_model, EMBED_MODEL, USE_OLLAMA, OLLAMA_URL
MAX_SPACES = get_int_env("MAX_SPACES", 10)
# –§–∏–ª—å—Ç—Ä –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤: –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω CONFLUENCE_SPACES, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –≤–º–µ—Å—Ç–æ MAX_SPACES
CONFLUENCE_SPACES = os.getenv("CONFLUENCE_SPACES", "").strip()
MAX_CHUNK_SIZE = get_int_env("MAX_CHUNK_SIZE", 1200)  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è Qwen3-8B
MIN_TEXT_LEN = get_int_env("MIN_TEXT_LEN", 50)
VERIFY_SSL = os.getenv("VERIFY_SSL", "true").lower() == "true"
BATCH_SIZE = get_int_env("BATCH_SIZE", 50)
SYNC_INTERVAL = get_int_env("SYNC_INTERVAL", 3600)

def get_bool_env(name: str, default: bool = False) -> bool:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ boolean ENV –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π."""
    value = os.getenv(name, str(default)).lower()
    return value in ('true', '1', 'yes', 'on')

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π –Ω–∞—Ä–µ–∑–∫–∏
MAX_TABLE_SIZE = get_int_env("MAX_TABLE_SIZE", 2048)
CHUNK_OVERLAP = get_int_env("CHUNK_OVERLAP", 100)

# === SEMANTIC CHUNKING CONFIGURATION ===
CHUNK_SIZE = int(os.getenv('CHUNK_SIZE', str(MAX_CHUNK_SIZE)))
CHUNK_OVERLAP_SIZE = int(os.getenv('CHUNK_OVERLAP', str(CHUNK_OVERLAP)))
MIN_CHUNK_SIZE = int(os.getenv('MIN_CHUNK_SIZE', '100'))
PRESERVE_STRUCTURE = os.getenv('PRESERVE_STRUCTURE', 'true').lower() == 'true'

# Initialize semantic chunker if available
SEMANTIC_SPLITTER = None
if HAS_LANGCHAIN:
    try:
        SEMANTIC_SPLITTER = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP_SIZE,
            separators=["\n\n\n", "\n\n", "\n", ". ", "! ", "? ", "; ", ", ", " ", ""],
            length_function=len,
            is_separator_regex=False
        )
        logger.info(f"‚úÖ Semantic chunker initialized: size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP_SIZE}")
    except Exception as e:
        logger.warning(f"Failed to initialize semantic chunker: {e}")
        SEMANTIC_SPLITTER = None
        HAS_LANGCHAIN = False

# –¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º
TEST_MODE = get_bool_env("TEST_MODE", False)
TEST_MAX_PAGES = get_int_env("TEST_MAX_PAGES", 10)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ tqdm –¥–ª—è progress bars
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    logger.warning("tqdm not available, progress bars disabled")

logger.info("Starting Confluence RAG sync (optimized for large instances)")

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
def get_page(confluence: Confluence, page_id: str) -> Dict[str, Any]:
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ Confluence —Å retry –ª–æ–≥–∏–∫–æ–π –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏.
    
    Args:
        confluence: Confluence API client
        page_id: ID —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    
    Returns:
        –î–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å body, version, ancestors, labels, children
    """
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    return confluence.get_page_by_id(
        page_id, 
        expand='body.storage,version,ancestors,metadata.labels,children.page,space'
    )

def get_timestamp(page: Dict[str, Any]) -> int:
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ timestamp –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏.
    
    Args:
        page: –û–±—ä–µ–∫—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã Confluence
    
    Returns:
        Timestamp –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYYMMDD –∏–ª–∏ 0 –ø—Ä–∏ –æ—à–∏–±–∫–µ
    """
    try:
        ts = page.get('version', {}).get('when', '')
        return int(ts[:10].replace('-', '')) if ts else 0
    except Exception as e:
        logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ timestamp: {e}")
        return 0

def get_page_attachments(confluence: Confluence, page_id: str) -> List[str]:
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –≤–ª–æ–∂–µ–Ω–∏–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
    
    Args:
        confluence: Confluence API client
        page_id: ID —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    
    Returns:
        –°–ø–∏—Å–æ–∫ –∏–º—ë–Ω —Ñ–∞–π–ª–æ–≤ –≤–ª–æ–∂–µ–Ω–∏–π
    """
    try:
        url = f"{confluence.url}/rest/api/content/{page_id}/child/attachment"
        response = requests.get(url, headers=confluence.default_headers, verify=VERIFY_SSL)
        response.raise_for_status()
        data = response.json()
        attachments = data.get('results', [])
        return [att.get('title', '') for att in attachments if att.get('title')]
    except Exception as e:
        logger.debug(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è attachments –¥–ª—è {page_id}: {e}")
        return []

# ============ Smart Caching –¥–ª—è get_page() ============
_page_cache_lock = RLock()
_page_cache = OrderedDict()  # LRU —á–µ—Ä–µ–∑ OrderedDict
_cache_stats = {"hits": 0, "misses": 0}
_cache_max_size = 1000  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞

def get_page_cached(confluence: Confluence, page_id: str, expand: str = "body.storage,version,ancestors,metadata.labels,children.page,space") -> Dict[str, Any]:
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º (LRU eviction + double-checked locking).
    
    –ö—ç—à–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã get_page –¥–ª—è —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è API –∑–∞–ø—Ä–æ—Å–æ–≤.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç LRU (Least Recently Used) eviction –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è memory leaks.
    Thread-safe —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç race conditions.
    
    Args:
        confluence: Confluence API client
        page_id: ID —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        expand: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
    
    Returns:
        –î–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    """
    cache_key = f"{page_id}:{expand}"
    
    # –ü–µ—Ä–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞ (thread-safe)
    with _page_cache_lock:
        if cache_key in _page_cache:
            _cache_stats["hits"] += 1
            # LRU: move_to_end –ø—Ä–∏ hit
            _page_cache.move_to_end(cache_key)
            logger.debug(f"Cache HIT for page {page_id}")
            return _page_cache[cache_key]
        
        _cache_stats["misses"] += 1
    
    logger.debug(f"Cache MISS for page {page_id}, fetching from Confluence...")
    
    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å retry (–≤–Ω–µ lock –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏)
    page = get_page(confluence, page_id)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à —Å double-checked locking (thread-safe)
    with _page_cache_lock:
        # Double-checked locking: –ø—Ä–æ–≤–µ—Ä—è–µ–º –µ—â—ë —Ä–∞–∑ (–¥—Ä—É–≥–æ–π –ø–æ—Ç–æ–∫ –º–æ–≥ —É–∂–µ –¥–æ–±–∞–≤–∏—Ç—å)
        if cache_key in _page_cache:
            logger.debug(f"Cache populated by another thread for page {page_id}")
            return _page_cache[cache_key]
        
        # Evict oldest if full (FIFO —á–µ—Ä–µ–∑ popitem(last=False))
        if len(_page_cache) >= _cache_max_size:
            oldest_key, _ = _page_cache.popitem(last=False)  # Remove oldest
            logger.debug(f"Cache full, removed oldest entry: {oldest_key}")
        
        _page_cache[cache_key] = page
        # Move to end (most recently used)
        _page_cache.move_to_end(cache_key)
    
    return page

def clear_page_cache():
    """–û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à —Å—Ç—Ä–∞–Ω–∏—Ü (thread-safe)"""
    global _page_cache, _cache_stats
    with _page_cache_lock:
        logger.info(f"Clearing page cache. Stats: hits={_cache_stats['hits']}, misses={_cache_stats['misses']}")
        _page_cache.clear()
        _cache_stats = {"hits": 0, "misses": 0}

def get_cache_stats() -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–∞ (thread-safe)"""
    with _page_cache_lock:
        total = _cache_stats["hits"] + _cache_stats["misses"]
        hit_rate = (_cache_stats["hits"] / total * 100) if total > 0 else 0.0
        return {
            "hits": _cache_stats["hits"],
            "misses": _cache_stats["misses"],
            "hit_rate": f"{hit_rate:.1f}%",
            "cache_size": len(_page_cache),
            "cache_max_size": _cache_max_size
        }

def build_breadcrumb(space_key: str, parent_titles: List[str], current_title: str, 
                     max_levels: int = None, max_length: int = None) -> str:
    """
    –ü–æ—Å—Ç—Ä–æ–∏—Ç—å breadcrumb –ø—É—Ç—å —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π.
    
    –°–æ–∑–¥–∞–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –ø—É—Ç—å –≤–∏–¥–∞ "Space > Parent1 > Parent2 > Current".
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–µ–∑–∞–µ—Ç –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —É—Ä–æ–≤–Ω–µ–π –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π.
    
    Args:
        space_key: –ö–ª—é—á –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ Confluence (–Ω–∞–ø—Ä–∏–º–µ—Ä, "RAUII")
        parent_titles: –°–ø–∏—Å–æ–∫ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü (–æ—Ç –∫–æ—Ä–Ω—è –∫ —Ç–µ–∫—É—â–µ–π)
        current_title: –¢–µ–∫—É—â–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        max_levels: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Ä–æ–≤–Ω–µ–π (default: –∏–∑ env –∏–ª–∏ 5)
        max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å—Ç—Ä–æ–∫–∏ –≤ —Å–∏–º–≤–æ–ª–∞—Ö (default: –∏–∑ env –∏–ª–∏ 200)
    
    Returns:
        –°—Ç—Ä–æ–∫–∞ breadcrumb path –∏–ª–∏ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –µ—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö
    
    Examples:
        >>> build_breadcrumb("RAUII", ["Dev", "API"], "Guide")
        'RAUII > Dev > API > Guide'
        
        >>> build_breadcrumb("RAUII", ["A"]*10, "B", max_levels=3)
        'RAUII > ... > A > A > B'
        
        >>> build_breadcrumb("SPACE", [], "Page", max_length=10)
        'SPACE > ...'
    """
    if max_levels is None:
        max_levels = MAX_BREADCRUMB_LEVELS
    if max_length is None:
        max_length = MAX_BREADCRUMB_LENGTH
    parts = []
    if space_key:
        parts.append(space_key)
    parts.extend(parent_titles)
    if current_title:
        parts.append(current_title)
    
    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ —É—Ä–æ–≤–Ω—è–º
    if len(parts) > max_levels:
        parts = parts[:1] + ['...'] + parts[-(max_levels-1):]
    
    breadcrumb = ' > '.join(parts) if parts else ''
    
    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ –¥–ª–∏–Ω–µ
    if len(breadcrumb) > max_length:
        breadcrumb = breadcrumb[:max_length-3] + "..."
    
    return breadcrumb


def build_page_path(space_key: str, parent_titles: List[str]) -> str:
    """
    –ü–æ—Å—Ç—Ä–æ–∏—Ç—å URL-friendly –ø—É—Ç—å –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.
    
    –°–æ–∑–¥–∞–µ—Ç –ø—É—Ç—å –≤–∏–¥–∞ "Space/Parent1/Parent2" –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —Ñ–∏–ª—å—Ç—Ä–∞—Ö Qdrant.
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —ç–∫—Ä–∞–Ω–∏—Ä—É–µ—Ç —Å–∏–º–≤–æ–ª—ã "/" –∏ "\\" –≤ –Ω–∞–∑–≤–∞–Ω–∏—è—Ö.
    
    Args:
        space_key: –ö–ª—é—á –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ Confluence (–Ω–∞–ø—Ä–∏–º–µ—Ä, "RAUII")
        parent_titles: –°–ø–∏—Å–æ–∫ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü (–æ—Ç –∫–æ—Ä–Ω—è –∫ —Ç–µ–∫—É—â–µ–π)
    
    Returns:
        –ü—É—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ "Space/Parent1/Parent2" –∏–ª–∏ space_key –µ—Å–ª–∏ –Ω–µ—Ç —Ä–æ–¥–∏—Ç–µ–ª–µ–π
    
    Examples:
        >>> build_page_path("RAUII", ["Dev", "API"])
        'RAUII/Dev/API'
        
        >>> build_page_path("RAUII", ["Dev/API", "Guide\\Test"])
        'RAUII/Dev_API/Guide_Test'
        
        >>> build_page_path("SPACE", [])
        'SPACE'
    """
    path_parts = []
    if space_key:
        path_parts.append(space_key)
    
    # –≠–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ "/" –≤ –Ω–∞–∑–≤–∞–Ω–∏—è—Ö
    safe_parent_titles = [str(t).replace('/', '_').replace('\\', '_') for t in parent_titles]
    path_parts.extend(safe_parent_titles)
    
    return '/'.join(path_parts) if path_parts else space_key

def extract_page_metadata(page_data: Dict[str, Any], space_key: str = '') -> Dict[str, Any]:
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ü–û–õ–ù–´–• –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ Confluence —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –æ—à–∏–±–æ–∫.
    
    –ò–°–ü–†–ê–í–õ–ï–ù–û: –î–æ–±–∞–≤–ª–µ–Ω—ã status, type, hierarchy_depth, created, modified –¥–ª—è metadata indexing.
    –î–û–ë–ê–í–õ–ï–ù–û: breadcrumb, page_path, headings –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.
    
    Args:
        page_data: –ü–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ API
        space_key: –ö–ª—é—á –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ Confluence (–¥–ª—è breadcrumb)
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ (–≤—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ –∫–ª—é—á–∏)
    """
    metadata = {
        'labels': [],
        'parent_id': '',
        'parent_title': '',
        'page_path': '',  # –ù–û–í–û–ï: –ø–æ–ª–Ω—ã–π –ø—É—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã (URL-friendly)
        'breadcrumb': '',  # –ù–û–í–û–ï: –ø–æ–ª–Ω—ã–π –ø—É—Ç—å —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏ >
        'version': 1,
        'created_by': '',
        'modified_date': '',
        'has_children': False,
        'children_count': 0,
        'attachments': [],
        # –ù–û–í–´–ï –ü–û–õ–Ø –¥–ª—è metadata indexing:
        'status': 'current',  # current, archived, draft
        'type': 'page',      # page, blogpost, attachment
        'hierarchy_depth': 0,
        'created': '',       # ISO format –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        'modified': '',      # ISO format –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        'modified_by': '',   # –ö—Ç–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–ª
        # –ù–û–í–´–ï –ü–û–õ–Ø –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤:
        'headings': '',
        'headings_list': [],
        'heading_hierarchy': [],
        'heading_count': 0,
        'parent_titles': [],
    }
    
    if not page_data or not isinstance(page_data, dict):
        logger.debug("Invalid page_data structure")
        return metadata
    
    # Status (current, archived, draft)
    try:
        # Confluence API –º–æ–∂–µ—Ç –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å status –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
        status = page_data.get('status', 'current')
        if isinstance(status, str):
            metadata['status'] = status.lower()
        else:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ _expandable
            expandable = page_data.get('_expandable', {})
            if 'status' in expandable:
                # Status –¥–æ—Å—Ç—É–ø–µ–Ω —á–µ—Ä–µ–∑ –æ—Ç–¥–µ–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å, –Ω–æ –æ–±—ã—á–Ω–æ 'current'
                metadata['status'] = 'current'
    except Exception as e:
        logger.debug(f"Error extracting status: {e}")
    
    # Type (page, blogpost, attachment)
    try:
        page_type = page_data.get('type', 'page')
        if isinstance(page_type, str):
            metadata['type'] = page_type.lower()
    except Exception as e:
        logger.debug(f"Error extracting type: {e}")
    
    # Labels (–º–µ—Ç–∫–∏) - —É–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
    try:
        labels_data = page_data.get('metadata', {}).get('labels', {})
        if isinstance(labels_data, dict):
            labels = labels_data.get('results', [])
        elif isinstance(labels_data, list):
            labels = labels_data
        else:
            labels = []
        label_names = [
            label.get('name', '') for label in labels 
            if isinstance(label, dict) and label.get('name')
        ]
        metadata['labels'] = label_names
    except Exception as e:
        logger.debug(f"Error extracting labels: {e}")
    
    # Ancestors (hierarchy) - —É–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
    try:
        ancestors = page_data.get('ancestors', [])
        parent_titles = []  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫
        
        if ancestors and isinstance(ancestors, list):
            if len(ancestors) > 0:
                parent = ancestors[-1]  # –ë–ª–∏–∂–∞–π—à–∏–π —Ä–æ–¥–∏—Ç–µ–ª—å
                if isinstance(parent, dict):
                    metadata['parent_id'] = str(parent.get('id', ''))
                    metadata['parent_title'] = str(parent.get('title', ''))
            
            # Hierarchy depth
            metadata['hierarchy_depth'] = len(ancestors)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ parent_titles
            for ancestor in ancestors:
                if isinstance(ancestor, dict):
                    ancestor_title = ancestor.get('title', '')
                    if ancestor_title:
                        parent_titles.append(ancestor_title)
        else:
            metadata['hierarchy_depth'] = 0
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º parent_titles –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
        metadata['parent_titles'] = parent_titles
    except Exception as e:
        logger.debug(f"Error extracting ancestors: {e}")
        metadata['hierarchy_depth'] = 0
        metadata['parent_titles'] = []
    
    # === –ù–û–í–û–ï: –ü–û–õ–ù–´–ô –ü–£–¢–¨ –° SPACE (BREADCRUMB) ===
    try:
        current_title = page_data.get('title', '')
        parent_titles = metadata.get('parent_titles', [])
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (–∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã —É–∂–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –≤ –Ω–∞—á–∞–ª–µ —Ñ–∞–π–ª–∞)
        metadata['breadcrumb'] = build_breadcrumb(
            space_key, 
            parent_titles, 
            current_title
        )
        
        metadata['page_path'] = build_page_path(space_key, parent_titles)
    except Exception as e:
        logger.debug(f"Error building breadcrumb: {e}")
        current_title = page_data.get('title', '')
        metadata['breadcrumb'] = f"{space_key} > {current_title}" if current_title else space_key
        metadata['page_path'] = space_key
    
    # === –ù–û–í–û–ï: –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –í–°–ï–• –ó–ê–ì–û–õ–û–í–ö–û–í –ò–ó HTML ===
    try:
        body = page_data.get('body', {})
        storage = body.get('storage', {})
        content_html = storage.get('value', '')
        
        if content_html:
            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            headings_start = time.time()
            
            soup = BeautifulSoup(content_html, 'html.parser')
            
            # –ò–∑–≤–ª–µ—á—å –≤—Å–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ (h1-h6) —Å –ª–∏–º–∏—Ç–æ–º
            headings = []
            heading_hierarchy = []
            current_path = []
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É –∏–∑ –Ω–∞—á–∞–ª–∞ —Ñ–∞–π–ª–∞
            MAX_HEADINGS = MAX_HEADINGS_EXTRACT
            
            for i, heading_tag in enumerate(soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])):
                # –ò–°–ü–†–ê–í–õ–ï–ù–û: –õ–∏–º–∏—Ç –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
                if i >= MAX_HEADINGS:
                    logger.debug(f"Truncated headings extraction at {MAX_HEADINGS} for page {page_data.get('id', 'unknown')}")
                    break
                
                # –ò–°–ü–†–ê–í–õ–ï–ù–û: HTML entities –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
                heading_text = heading_tag.get_text(strip=True)
                heading_text = html.unescape(heading_text)  # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å &lt; –≤ <
                
                if not heading_text:
                    continue
                
                heading_level = int(heading_tag.name[1])  # h1 -> 1, h2 -> 2, etc.
                
                headings.append({
                    'text': heading_text,
                    'level': heading_level
                })
                
                # –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–ª—É—á—à–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –∏–µ—Ä–∞—Ä—Ö–∏–∏
                # –û–±—Ä–µ–∑–∞—Ç—å path –¥–æ —Ç–µ–∫—É—â–µ–≥–æ —É—Ä–æ–≤–Ω—è
                while len(current_path) > 0 and len(current_path) >= heading_level:
                    current_path.pop()
                
                # –ò–°–ü–†–ê–í–õ–ï–ù–û: –î–æ–ø–æ–ª–Ω–∏—Ç—å path –µ—Å–ª–∏ –ø—Ä–æ–ø—É—â–µ–Ω—ã —É—Ä–æ–≤–Ω–∏
                # –ï—Å–ª–∏ –±—ã–ª h1, –ø–æ—Ç–æ–º —Å—Ä–∞–∑—É h3, —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ - –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
                # –ù–µ –¥–æ–±–∞–≤–ª—è–µ–º placeholder, —á—Ç–æ–±—ã –Ω–µ –∏—Å–∫–∞–∂–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É
                
                current_path.append(heading_text)
                heading_hierarchy.append({
                    'text': heading_text,
                    'level': heading_level,
                    'path': ' > '.join(current_path)
                })
            
            # –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (–¥–ª—è –ø–æ–∏—Å–∫–∞)
            all_headings = [h['text'] for h in headings]
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –¥–ª—è headings —Å—Ç—Ä–æ–∫–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É)
            headings_string = ' | '.join(all_headings)
            if len(headings_string) > MAX_HEADINGS_STRING_LENGTH:
                # –û–±—Ä–µ–∑–∞–µ–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º "..."
                truncated = headings_string[:MAX_HEADINGS_STRING_LENGTH]
                last_pipe = truncated.rfind(' | ')
                if last_pipe > 0:
                    headings_string = truncated[:last_pipe] + " | ..."
                else:
                    headings_string = truncated + "..."
            
            metadata['headings'] = headings_string
            metadata['headings_list'] = all_headings
            metadata['heading_hierarchy'] = heading_hierarchy
            metadata['heading_count'] = len(all_headings)
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            headings_time = (time.time() - headings_start) * 1000  # –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö
            if headings_time > 100:  # –ú–µ–¥–ª–µ–Ω–Ω–µ–µ 100ms
                logger.warning(
                    f"‚ö†Ô∏è Slow headings extraction: {headings_time:.0f}ms "
                    f"for page {page_data.get('id', 'unknown')} "
                    f"({metadata['heading_count']} headings, "
                    f"{len(content_html)} chars HTML)"
                )
            elif headings_time > 50:  # –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å
                logger.debug(
                    f"Headings extraction: {headings_time:.0f}ms "
                    f"for {metadata['heading_count']} headings"
                )
        else:
            metadata['headings'] = ''
            metadata['headings_list'] = []
            metadata['heading_hierarchy'] = []
            metadata['heading_count'] = 0
    except Exception as e:
        logger.debug(f"Error extracting headings: {e}")
        metadata['headings'] = ''
        metadata['headings_list'] = []
        metadata['heading_hierarchy'] = []
        metadata['heading_count'] = 0
    
    # Version info (created, modified, authors) - —É–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
    try:
        version = page_data.get('version', {})
        if isinstance(version, dict):
            metadata['version'] = int(version.get('number', 1))
            
            # Modified date
            modified_when = version.get('when', '')
            if modified_when:
                metadata['modified_date'] = str(modified_when)
                metadata['modified'] = modified_when  # ISO format –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            
            # Modified by
            by_info = version.get('by', {})
            if isinstance(by_info, dict):
                metadata['modified_by'] = str(by_info.get('displayName', ''))
        
        # History –¥–ª—è created date
        history = page_data.get('history', {})
        if isinstance(history, dict):
            created_date = history.get('createdDate', '')
            if created_date:
                metadata['created'] = created_date
            else:
                # Fallback –Ω–∞ version.when –µ—Å–ª–∏ history –Ω–µ—Ç
                metadata['created'] = metadata.get('modified', '')
        
        # Created by –∏–∑ history
        if isinstance(history, dict):
            created_by_info = history.get('createdBy', {})
            if isinstance(created_by_info, dict):
                metadata['created_by'] = str(created_by_info.get('displayName', ''))
            else:
                # Fallback –Ω–∞ version.by
                by_info = version.get('by', {})
                if isinstance(by_info, dict):
                    metadata['created_by'] = str(by_info.get('displayName', ''))
    except Exception as e:
        logger.debug(f"Error extracting version/history info: {e}")
    
    # Child pages count
    try:
        children_data = page_data.get('children', {})
        if isinstance(children_data, dict):
            page_info = children_data.get('page', {})
            if isinstance(page_info, dict):
                children = int(page_info.get('size', 0))
                metadata['has_children'] = children > 0
                metadata['children_count'] = children
    except Exception as e:
        logger.debug(f"Error extracting children info: {e}")
    
    return metadata

def extract_macro_body(macro_html: str) -> str:
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ç–µ–ª–∞ Confluence –º–∞–∫—Ä–æ—Å–∞.
    
    Args:
        macro_html: HTML –º–∞–∫—Ä–æ—Å–∞
    
    Returns:
        –¢–µ–∫—Å—Ç –∏–∑ rich-text-body –º–∞–∫—Ä–æ—Å–∞
    """
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ <ac:rich-text-body>...</ac:rich-text-body>
    body_match = re.search(r'<ac:rich-text-body>(.*?)</ac:rich-text-body>', macro_html, re.DOTALL)
    if body_match:
        return body_match.group(1)
    return macro_html

def preprocess_confluence_macros(html: str) -> str:
    """
    –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ Confluence –º–∞–∫—Ä–æ—Å–æ–≤ –¥–ª—è –ª—É—á—à–µ–π –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ —Ç–µ–∫—Å—Ç.
    
    Args:
        html: HTML —Å Confluence –º–∞–∫—Ä–æ—Å–∞–º–∏
    
    Returns:
        HTML —Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ –º–∞–∫—Ä–æ—Å–∞–º–∏
    """
    
    # Info –º–∞–∫—Ä–æ—Å: <ac:structured-macro ac:name="info">
    html = re.sub(
        r'<ac:structured-macro[^>]*ac:name="info"[^>]*>(.*?)</ac:structured-macro>',
        lambda m: f'\n\nüí° **INFO:** {extract_macro_body(m.group(1))}\n\n',
        html,
        flags=re.DOTALL
    )
    
    # Warning –º–∞–∫—Ä–æ—Å
    html = re.sub(
        r'<ac:structured-macro[^>]*ac:name="warning"[^>]*>(.*?)</ac:structured-macro>',
        lambda m: f'\n\n‚ö†Ô∏è **WARNING:** {extract_macro_body(m.group(1))}\n\n',
        html,
        flags=re.DOTALL
    )
    
    # Note –º–∞–∫—Ä–æ—Å
    html = re.sub(
        r'<ac:structured-macro[^>]*ac:name="note"[^>]*>(.*?)</ac:structured-macro>',
        lambda m: f'\n\nüìù **NOTE:** {extract_macro_body(m.group(1))}\n\n',
        html,
        flags=re.DOTALL
    )
    
    # Tip –º–∞–∫—Ä–æ—Å
    html = re.sub(
        r'<ac:structured-macro[^>]*ac:name="tip"[^>]*>(.*?)</ac:structured-macro>',
        lambda m: f'\n\nüí° **TIP:** {extract_macro_body(m.group(1))}\n\n',
        html,
        flags=re.DOTALL
    )
    
    # Expand –º–∞–∫—Ä–æ—Å (—Å–∫—Ä—ã–≤–∞–µ–º—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç)
    html = re.sub(
        r'<ac:structured-macro[^>]*ac:name="expand"[^>]*>(.*?)</ac:structured-macro>',
        lambda m: f'\n\nüîΩ **EXPAND:** {extract_macro_body(m.group(1))}\n\n',
        html,
        flags=re.DOTALL
    )
    
    # Code –º–∞–∫—Ä–æ—Å —Å —è–∑—ã–∫–æ–º
    def replace_code_macro(match):
        full_macro = match.group(0)
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —è–∑—ã–∫
        lang_match = re.search(r'<ac:parameter[^>]*ac:name="language"[^>]*>([^<]*)</ac:parameter>', full_macro)
        language = lang_match.group(1) if lang_match else ''
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–¥
        code_match = re.search(r'<ac:plain-text-body><!\[CDATA\[(.*?)\]\]></ac:plain-text-body>', full_macro, re.DOTALL)
        code = code_match.group(1) if code_match else extract_macro_body(full_macro)
        return f'\n\n```{language}\n{code}\n```\n\n'
    
    html = re.sub(
        r'<ac:structured-macro[^>]*ac:name="code"[^>]*>.*?</ac:structured-macro>',
        replace_code_macro,
        html,
        flags=re.DOTALL
    )
    
    # Panel –º–∞–∫—Ä–æ—Å
    html = re.sub(
        r'<ac:structured-macro[^>]*ac:name="panel"[^>]*>(.*?)</ac:structured-macro>',
        lambda m: f'\n\nüìã **PANEL:** {extract_macro_body(m.group(1))}\n\n',
        html,
        flags=re.DOTALL
    )
    
    # Status –º–∞–∫—Ä–æ—Å
    html = re.sub(
        r'<ac:structured-macro[^>]*ac:name="status"[^>]*>.*?<ac:parameter[^>]*ac:name="title"[^>]*>([^<]*)</ac:parameter>.*?</ac:structured-macro>',
        lambda m: f'[STATUS: {m.group(1)}]',
        html,
        flags=re.DOTALL
    )
    
    # TOC (Table of Contents) - —É–¥–∞–ª—è–µ–º, —Ç.–∫. —ç—Ç–æ –∞–≤—Ç–æ–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–µ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ
    html = re.sub(
        r'<ac:structured-macro[^>]*ac:name="toc"[^>]*>.*?</ac:structured-macro>',
        '',
        html,
        flags=re.DOTALL
    )
    
    # Excerpt –º–∞–∫—Ä–æ—Å (–∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ)
    html = re.sub(
        r'<ac:structured-macro[^>]*ac:name="excerpt"[^>]*>(.*?)</ac:structured-macro>',
        lambda m: f'\n\nüìå **EXCERPT:** {extract_macro_body(m.group(1))}\n\n',
        html,
        flags=re.DOTALL
    )
    
    # Quote –º–∞–∫—Ä–æ—Å
    html = re.sub(
        r'<ac:structured-macro[^>]*ac:name="quote"[^>]*>(.*?)</ac:structured-macro>',
        lambda m: f'\n\n> {extract_macro_body(m.group(1))}\n\n',
        html,
        flags=re.DOTALL
    )
    
    # Page Properties –º–∞–∫—Ä–æ—Å (—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)
    def extract_page_properties(match):
        full_macro = match.group(0)
        # –ò—â–µ–º –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        props = []
        prop_pattern = re.findall(r'<ac:parameter[^>]*ac:name="([^"]*)"[^>]*>([^<]*)</ac:parameter>', full_macro)
        for key, value in prop_pattern:
            if key and value:
                props.append(f"{key}: {value}")
        if props:
            return f'\n\nüìä **PAGE PROPERTIES:**\n' + '\n'.join([f'  ‚Ä¢ {p}' for p in props]) + '\n\n'
        return ''
    
    html = re.sub(
        r'<ac:structured-macro[^>]*ac:name="details"[^>]*>.*?</ac:structured-macro>',
        extract_page_properties,
        html,
        flags=re.DOTALL
    )
    
    # Include Page –º–∞–∫—Ä–æ—Å (—Ç—Ä–∞–Ω—Å–ª—é–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç)
    def extract_include_page(match):
        full_macro = match.group(0)
        # –ò—â–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
        page_match = re.search(r'<ri:page[^>]*ri:content-title="([^"]*)"', full_macro)
        if page_match:
            page_title = page_match.group(1)
            return f'\n\nüîó **INCLUDES PAGE:** "{page_title}"\n\n'
        return ''
    
    html = re.sub(
        r'<ac:structured-macro[^>]*ac:name="include"[^>]*>.*?</ac:structured-macro>',
        extract_include_page,
        html,
        flags=re.DOTALL
    )
    
    # Children Display –º–∞–∫—Ä–æ—Å (—Å–ø–∏—Å–æ–∫ –¥–æ—á–µ—Ä–Ω–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü)
    html = re.sub(
        r'<ac:structured-macro[^>]*ac:name="children"[^>]*>.*?</ac:structured-macro>',
        '\n\nüìë **CHILD PAGES LIST**\n\n',
        html,
        flags=re.DOTALL
    )
    
    # Recently Updated –º–∞–∫—Ä–æ—Å
    html = re.sub(
        r'<ac:structured-macro[^>]*ac:name="recently-updated"[^>]*>.*?</ac:structured-macro>',
        '',
        html,
        flags=re.DOTALL
    )
    
    # Confluence —Ç–∞–±–ª–∏—Ü—ã: –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º <ac:table> –≤ <table>
    html = re.sub(r'<ac:table>', '<table>', html)
    html = re.sub(r'</ac:table>', '</table>', html)
    html = re.sub(r'<ac:tr>', '<tr>', html)
    html = re.sub(r'</ac:tr>', '</tr>', html)
    html = re.sub(r'<ac:td>', '<td>', html)
    html = re.sub(r'</ac:td>', '</td>', html)
    html = re.sub(r'<ac:th>', '<th>', html)
    html = re.sub(r'</ac:th>', '</th>', html)
    
    return html

def convert_table_to_markdown(table_element) -> tuple[str, str]:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç HTML —Ç–∞–±–ª–∏—Ü—É –≤ markdown —Ñ–æ—Ä–º–∞—Ç. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (markdown, html)."""
    try:
        table_html = str(table_element)
        rows = []
        for tr in table_element.find_all('tr'):
            cells = []
            for td in tr.find_all(['td', 'th']):
                cell_text = td.get_text(separator=' ', strip=True)
                cell_text = cell_text.replace('|', '\\|')
                cells.append(cell_text)
            if cells:
                rows.append('| ' + ' | '.join(cells) + ' |')
        
        if not rows or len(rows) < 2:
            return "", ""
        
        num_cols = len(rows[0].split('|')) - 2
        if num_cols > 0 and len(rows) > 1:
            separator = '| ' + ' | '.join(['---'] * num_cols) + ' |'
            rows.insert(1, separator)
        
        markdown = '\n'.join(rows) if rows else ""
        return markdown, table_html
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ —Ç–∞–±–ª–∏—Ü—ã: {e}")
        plain = table_element.get_text(separator=' ', strip=True)
        return plain, str(table_element)

def extract_list_text(list_element, tag: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ —Å–ø–∏—Å–∫–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –º–∞—Ä–∫–µ—Ä–∞–º–∏."""
    try:
        items = []
        for li in list_element.find_all('li', recursive=False):
            item_text = li.get_text(separator=' ', strip=True)
            if item_text:
                items.append(item_text)
        
        if not items:
            return ""
        
        if tag == 'ul':
            return '\n'.join([f"- {item}" for item in items])
        else:
            return '\n'.join([f"{i+1}. {item}" for i, item in enumerate(items)])
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞: {e}")
        return list_element.get_text(separator='\n', strip=True)

def extract_structural_blocks(html_content: str) -> List[Dict[str, Any]]:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–Ω–∞—è –Ω–∞—Ä–µ–∑–∫–∞ HTML –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –±–ª–æ–∫–∏ (—Ç–∞–±–ª–∏—Ü—ã, —Å–ø–∏—Å–∫–∏, —Ç–µ–∫—Å—Ç)."""
    if not html_content:
        return []
    
    try:
        html_content = preprocess_confluence_macros(html_content)
        soup = BeautifulSoup(html_content, 'html.parser')
        blocks = []
        heading_stack = []
        
        def create_block(block_type: str, content: str, heading_stack: list, html: Optional[str] = None) -> Dict[str, Any]:
            parent_path = " > ".join([h['text'] for h in heading_stack[:-1]]) if len(heading_stack) > 1 else ""
            current_h = heading_stack[-1]['text'] if heading_stack else ""
            block = {
                "type": block_type,
                "content": content,
                "heading": current_h,
                "level": heading_stack[-1]['level'] if heading_stack else 0,
                "parent_path": parent_path,
                "size": len(content)
            }
            if html:
                block["html"] = html
            return block
        
        def walk_tree(element):
            if isinstance(element, NavigableString):
                return
            tag = element.name
            if not tag:
                return
            
            if tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                level = int(tag[1])
                heading_text = element.get_text(strip=True)
                if heading_text:
                    heading_stack[:] = [h for h in heading_stack if h['level'] < level]
                    heading_stack.append({"text": heading_text, "level": level})
                for child in element.children:
                    walk_tree(child)
                return
            
            if tag == 'table':
                table_md, table_html = convert_table_to_markdown(element)
                if table_md:
                    blocks.append(create_block("table", table_md, heading_stack, table_html))
                    logger.debug(f"‚úì Table block (size={len(table_md)} chars): '{heading_stack[-1]['text'] if heading_stack else 'no heading'}'")
                return
            
            if tag in ['ul', 'ol']:
                list_text = extract_list_text(element, tag)
                if list_text:
                    blocks.append(create_block("list", list_text, heading_stack))
                return
            
            if tag in ['p', 'div', 'section', 'article']:
                text = element.get_text(separator=' ', strip=True)
                if text and len(text) > 20:
                    blocks.append(create_block("text", text, heading_stack))
                return
            
            for child in element.children:
                walk_tree(child)
        
        root = soup.body if soup.body else soup
        for child in root.children:
            walk_tree(child)
        
        return blocks
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π –Ω–∞—Ä–µ–∑–∫–∏: {e}", exc_info=True)
        text = html_to_text(html_content)
        return [{"type": "text", "content": text, "heading": "", "level": 0, "parent_path": "", "size": len(text)}]

def smart_chunk_with_context(blocks: List[Dict[str, Any]], max_size: int = CHUNK_SIZE) -> List[Dict[str, Any]]:
    """–£–º–Ω–∞—è –Ω–∞—Ä–µ–∑–∫–∞: —Ç–∞–±–ª–∏—Ü—ã –∏ —Å–ø–∏—Å–∫–∏ —Ü–µ–ª–∏–∫–æ–º, —Ç–µ–∫—Å—Ç –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º."""
    chunks = []
    
    for block in blocks:
        block_type = block['type']
        heading = block['heading']
        level = block['level']
        content = block['content']
        size = block['size']
        parent_path = block.get('parent_path', '')
        
        context_prefix = ""
        if parent_path:
            context_prefix = f"{parent_path} > {heading}\n\n" if heading else f"{parent_path}\n\n"
        elif heading:
            context_prefix = f"{heading}\n\n"
        
        if block_type in ['table', 'list']:
            chunk = {
                "text": context_prefix + content if context_prefix else content,
                "heading": heading,
                "level": level,
                "type": block_type,
                "parent_path": parent_path,
                "size": size
            }
            if block_type == 'table' and 'html' in block:
                chunk['html'] = block['html']
            chunks.append(chunk)
            logger.info(f"‚úì {block_type.capitalize()} block (size={size} chars): '{heading}' in {parent_path or 'root'}")
            continue
        
        if block_type == 'text':
            if size <= max_size:
                # –ö–†–ò–¢–ò–ß–ù–û: –û–±—ä–µ–¥–∏–Ω—è–µ–º –º–∞–ª–µ–Ω—å–∫–∏–µ blocks –ø–æ–¥ –æ–¥–Ω–∏–º heading
                if chunks and chunks[-1].get('heading') == heading and chunks[-1].get('type') == 'text':
                    last_chunk = chunks[-1]
                    last_size = last_chunk.get('size', 0)
                    
                    # –ï—Å–ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–π chunk –º–∞–ª–µ–Ω—å–∫–∏–π (< 600 chars) –∏ –≤–º–µ—Å—Ç–µ –æ–Ω–∏ < max_size - –æ–±—ä–µ–¥–∏–Ω—è–µ–º
                    combined_size = last_size + size + len(context_prefix)
                    if last_size < 600 and combined_size <= max_size:
                        # –û–±—ä–µ–¥–∏–Ω—è–µ–º chunks
                        new_content = context_prefix + content if context_prefix else content
                        combined_text = last_chunk['text'] + "\n\n" + new_content
                        last_chunk['text'] = combined_text
                        last_chunk['size'] = len(combined_text)
                        logger.debug(f"üì¶ –û–±—ä–µ–¥–∏–Ω–µ–Ω—ã blocks: {last_size} + {size} = {len(combined_text)} chars –ø–æ–¥ heading '{heading}'")
                        continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ chunk
                
                # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π chunk
                chunk = {
                    "text": context_prefix + content if context_prefix else content,
                    "heading": heading,
                    "level": level,
                    "type": block_type,
                    "parent_path": parent_path,
                    "size": size
                }
                chunks.append(chunk)
            else:
                logger.info(f"‚ö† Text block too large ({size} > {max_size}), splitting: '{heading}'")
                
                # === –ù–û–í–û–ï: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å RecursiveCharacterTextSplitter –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω ===
                if SEMANTIC_SPLITTER and PRESERVE_STRUCTURE:
                    try:
                        # Semantic chunking —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
                        text_chunks = SEMANTIC_SPLITTER.split_text(content)
                        
                        for i, chunk_text in enumerate(text_chunks):
                            if len(chunk_text.strip()) < MIN_CHUNK_SIZE:
                                continue
                            
                            chunk = {
                                "text": context_prefix + chunk_text.strip() if context_prefix else chunk_text.strip(),
                                "heading": heading,
                                "level": level,
                                "type": block_type,
                                "parent_path": parent_path,
                                "size": len(chunk_text)
                            }
                            chunks.append(chunk)
                        
                        logger.debug(f"‚úÖ Semantic chunking: {size} chars ‚Üí {len(text_chunks)} chunks")
                        continue
                    except Exception as e:
                        logger.warning(f"Semantic chunking failed, using fallback: {e}")
                
                # Fallback: existing sentence-based splitting
                import re
                sentences = re.split(r'(?<=[.!?])\s+', content)
                current = ""
                overlap_buffer = ""
                
                for sent in sentences:
                    if len(current) + len(sent) + 1 < max_size:
                        current += sent + " "
                    else:
                        if current.strip():
                            chunk_text = context_prefix + (overlap_buffer + current).strip() if context_prefix else (overlap_buffer + current).strip()
                            chunk = {
                                "text": chunk_text,
                                "heading": heading,
                                "level": level,
                                "type": block_type,
                                "parent_path": parent_path,
                                "size": len(chunk_text)
                            }
                            chunks.append(chunk)
                            overlap_buffer = current[-CHUNK_OVERLAP_SIZE:] if len(current) > CHUNK_OVERLAP_SIZE else current
                        current = sent + " "
                
                if current.strip():
                    chunk_text = context_prefix + (overlap_buffer + current).strip() if context_prefix else (overlap_buffer + current).strip()
                    chunk = {
                        "text": chunk_text,
                        "heading": heading,
                        "level": level,
                        "type": block_type,
                        "parent_path": parent_path,
                        "size": len(chunk_text)
                    }
                    chunks.append(chunk)
    
    return chunks if chunks else []

def html_to_text(html: str, max_len: int = 50000) -> str:
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è HTML Confluence –≤ plain text —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ –º–∞–∫—Ä–æ—Å–æ–≤.
    
    Args:
        html: HTML –∫–æ–Ω—Ç–µ–Ω—Ç
        max_len: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    
    Returns:
        Plain text –∏–ª–∏ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –ø—Ä–∏ –æ—à–∏–±–∫–µ
    """
    if not html:
        return ""
    try:
        if len(html) > max_len:
            html = html[:max_len]
            logger.warning(f"HTML –æ–±—Ä–µ–∑–∞–Ω –¥–æ {max_len} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ Confluence –º–∞–∫—Ä–æ—Å–æ–≤
        html = preprocess_confluence_macros(html)
        
        h = html2text.HTML2Text()
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è Confluence
        h.ignore_links = False
        h.body_width = 0
        h.unicode_snob = True
        h.ignore_images = False  # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        h.ignore_emphasis = False  # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (–∂–∏—Ä–Ω—ã–π, –∫—É—Ä—Å–∏–≤)
        h.skip_internal_links = False  # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏
        h.inline_links = False  # –°—Å—ã–ª–∫–∏ –≤ –≤–∏–¥–µ [text](url)
        h.mark_code = True  # –û—Ç–º–µ—á–∞—Ç—å –∫–æ–¥ –±–ª–æ–∫–∏
        h.wrap_links = False  # –ù–µ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å —Å—Å—ã–ª–∫–∏
        h.default_image_alt = "[–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ]"  # –ê–ª—å—Ç –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        
        text = h.handle(html).strip()
        
        # –û—á–∏—Å—Ç–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫ (–±–æ–ª—å—à–µ 2 –ø–æ–¥—Ä—è–¥)
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        return text
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ HTML: {e}")
        return ""

def extract_sections(text: str) -> List[Dict[str, Any]]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–µ–∫—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º (markdown —Ñ–æ—Ä–º–∞—Ç) —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–µ—Ä–∞—Ä—Ö–∏–∏.
    
    Args:
        text: –¢–µ–∫—Å—Ç –≤ markdown —Ñ–æ—Ä–º–∞—Ç–µ (–ø–æ—Å–ª–µ html2text)
    
    Returns:
        –°–ø–∏—Å–æ–∫ —Å–µ–∫—Ü–∏–π —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏, –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º –∏ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
    """
    lines = text.split('\n')
    sections = []
    current_section = {"heading": "", "level": 0, "content": [], "parent_headings": []}
    heading_stack = []  # –°—Ç–µ–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏–µ—Ä–∞—Ä—Ö–∏–∏
    
    heading_pattern = re.compile(r'^(#{1,6})\s+(.+)$')
    
    for line in lines:
        match = heading_pattern.match(line)
        if match:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Å–µ–∫—Ü–∏—é
            if current_section["content"]:
                sections.append(current_section)
            
            # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é —Å–µ–∫—Ü–∏—é
            level = len(match.group(1))
            heading = match.group(2).strip()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–µ–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤: —É–¥–∞–ª—è–µ–º –≤—Å–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Ç–æ–≥–æ –∂–µ –∏–ª–∏ –±–æ–ª–µ–µ –Ω–∏–∑–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è
            heading_stack = [h for h in heading_stack if h['level'] < level]
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å–µ–∫—Ü–∏—é —Å —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
            parent_headings = [h['text'] for h in heading_stack]
            current_section = {
                "heading": heading, 
                "level": level, 
                "content": [line],
                "parent_headings": parent_headings
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ —Å—Ç–µ–∫
            heading_stack.append({'level': level, 'text': heading})
        else:
            current_section["content"].append(line)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å–µ–∫—Ü–∏—é
    if current_section["content"]:
        sections.append(current_section)
    
    return sections

def chunk_text(text: str, size: int = CHUNK_SIZE) -> List[Dict[str, Any]]:
    """
    –£–º–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏ —Å —É—á—ë—Ç–æ–º –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤.
    
    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç (markdown –ø–æ—Å–ª–µ html2text)
        size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
    
    Returns:
        –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å metadata (heading, content)
    """
    if not text or len(text) < 100:
        return [{"text": text, "heading": "", "level": 0}] if text else []
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–µ–∫—Ü–∏–∏ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º
    sections = extract_sections(text)
    
    if not sections:
        # Fallback: —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º
        paras = [p.strip() for p in text.split('\n\n') if p.strip() and len(p.strip()) > 5]
        chunks = []
        current = ""
        for para in paras:
            if len(current) + len(para) + 2 < size:
                current += para + "\n\n"
            else:
                if current.strip():
                    chunks.append({"text": current.strip(), "heading": "", "level": 0})
                current = para + "\n\n"
        if current.strip():
            chunks.append({"text": current.strip(), "heading": "", "level": 0})
        return chunks if chunks else [{"text": text, "heading": "", "level": 0}]
    
    # –†–∞–∑–±–∏–≤–∞–µ–º —Å–µ–∫—Ü–∏–∏ –Ω–∞ —á–∞–Ω–∫–∏
    chunks = []
    for section in sections:
        heading = section["heading"]
        level = section["level"]
        parent_headings = section.get("parent_headings", [])
        content = '\n'.join(section["content"])
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–µ—Ñ–∏–∫—Å –∏–∑ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (—Ç–æ–ª—å–∫–æ –¥–ª—è —É—Ä–æ–≤–Ω—è 3+)
        context_prefix = ""
        if level >= 3 and parent_headings:
            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            context_prefix = " > ".join(parent_headings) + "\n\n"
        
        # –ï—Å–ª–∏ —Å–µ–∫—Ü–∏—è —Ü–µ–ª–∏–∫–æ–º –≤–ª–µ–∑–∞–µ—Ç –≤ —á–∞–Ω–∫
        if len(content) <= size:
            chunk_text = context_prefix + content.strip() if context_prefix else content.strip()
            chunks.append({
                "text": chunk_text,
                "heading": heading,
                "level": level
            })
        else:
            # –†–∞–∑–±–∏–≤–∞–µ–º –±–æ–ª—å—à—É—é —Å–µ–∫—Ü–∏—é –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º, —Å–æ—Ö—Ä–∞–Ω—è—è –∑–∞–≥–æ–ª–æ–≤–æ–∫
            paras = [p.strip() for p in content.split('\n\n') if p.strip()]
            current = ""
            for para in paras:
                if len(current) + len(para) + 2 < size:
                    current += para + "\n\n"
                else:
                    if current.strip():
                        chunk_text = context_prefix + current.strip() if context_prefix else current.strip()
                        chunks.append({
                            "text": chunk_text,
                            "heading": heading,
                            "level": level
                        })
                    current = para + "\n\n"
            if current.strip():
                chunk_text = context_prefix + current.strip() if context_prefix else current.strip()
                chunks.append({
                    "text": chunk_text,
                    "heading": heading,
                    "level": level
                })
    
    return chunks if chunks else [{"text": text, "heading": "", "level": 0}]

def load_state() -> Dict[str, Any]:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –∏–∑ —Ñ–∞–π–ª–∞.
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å last_sync –∏ pages
    """
    if os.path.exists(STATE_FILE):
        try:
            with open(STATE_FILE, 'r', encoding='utf-8') as f:
                state = json.load(f)
                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å–æ—Å—Ç–æ—è–Ω–∏–µ: {len(state.get('pages', {}))} —Å—Ç—Ä–∞–Ω–∏—Ü")
                return state
        except json.JSONDecodeError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ state file: {e}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è: {e}")
    
    logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è")
    return {"last_sync": 0, "pages": {}}

def save_state(state: Dict[str, Any]) -> None:
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –≤ —Ñ–∞–π–ª.
    
    Args:
        state: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è
    """
    try:
        with open(STATE_FILE, 'w', encoding='utf-8') as f:
            json.dump(state, f, indent=2, ensure_ascii=False)
        logger.debug(f"–°–æ—Å—Ç–æ—è–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {len(state.get('pages', {}))} —Å—Ç—Ä–∞–Ω–∏—Ü")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è: {e}")

# init_embeddings() —Ç–µ–ø–µ—Ä—å –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è –∏–∑ embeddings.py –∫–∞–∫ get_embed_model()

def get_all_pages_generator(confluence: Confluence, space_key: str, batch_size: int = 50):
    """
    –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞.
    
    –í–º–µ—Å—Ç–æ –∑–∞–≥—Ä—É–∑–∫–∏ –í–°–ï —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –ø–∞–º—è—Ç–∏, –∑–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ –±–∞—Ç—á–∞–º.
    –≠–∫–æ–Ω–æ–º–∏—Ç RAM –Ω–∞ –±–æ–ª—å—à–∏—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞—Ö (1000+ —Å—Ç—Ä–∞–Ω–∏—Ü).
    
    Args:
        confluence: Confluence –∫–ª–∏–µ–Ω—Ç
        space_key: –ö–ª—é—á –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ (RAUII, Surveys)
        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è
        
    Yields:
        Dict: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
    """
    start = 0
    total_yielded = 0
    
    while True:
        try:
            logger.debug(f"Fetching pages from {space_key} starting at {start}")
            
            # –ü–æ–ª—É—á–∞–µ–º –±–∞—Ç—á
            batch = list(confluence.get_all_pages_from_space(
                space_key,
                start=start,
                limit=batch_size,
                expand='history.lastUpdated,version.number'
            ))
            
            if not batch:
                logger.info(f"No more pages for {space_key}. Total yielded: {total_yielded}")
                break
            
            # –í—ã–¥–∞–µ–º –ø–æ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
            for page in batch:
                yield page
                total_yielded += 1
            
            start += batch_size
            
        except Exception as e:
            logger.error(f"Error fetching pages for {space_key}: {e}")
            break

# ‚úÖ NEW: Simple Bloom Filter –¥–ª—è duplicate detection (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
USE_BLOOM_FILTER = os.getenv("USE_BLOOM_FILTER", "false").lower() == "true"
BLOOM_FILTER_SIZE = int(os.getenv("BLOOM_FILTER_SIZE", "100000"))  # –û–∂–∏–¥–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤

try:
    from pybloom_live import BloomFilter
    HAS_BLOOM_FILTER = True
except ImportError:
    HAS_BLOOM_FILTER = False
    if USE_BLOOM_FILTER:
        logger.warning("pybloom_live not available, falling back to set() for duplicate detection")


class BatchProcessor:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –±–∞—Ç—á–µ–π —Å recovery –º–µ—Ö–∞–Ω–∏–∑–º–æ–º"""
    
    def __init__(self, max_retries: int = 3):
        self.max_retries = max_retries
        # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º Bloom Filter –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        if USE_BLOOM_FILTER and HAS_BLOOM_FILTER:
            self.processed_ids = BloomFilter(capacity=BLOOM_FILTER_SIZE, error_rate=0.001)
            self._use_bloom = True
            logger.debug(f"Using Bloom Filter for duplicate detection (capacity={BLOOM_FILTER_SIZE})")
        else:
            self.processed_ids = set()  # Fallback –Ω–∞ set() –¥–ª—è –º–∞–ª—ã—Ö –æ–±—ä–µ–º–æ–≤
            self._use_bloom = False
        self.failed_ids = {}  # –ù–µ—É–¥–∞—á–Ω—ã–µ —Å –ø—Ä–∏—á–∏–Ω–æ–π
    
    def process_batch_safe(self, qdrant_client: Any, confluence: Confluence, 
                          batch: List[Dict[str, Any]], state: Dict[str, Any], space_key: str) -> tuple[int, int, int, list]:
        """
        –ü—Ä–æ—Ü–µ—Å—Å–∏—Ä–æ–≤–∞—Ç—å –±–∞—Ç—á —Å –≥–∞—Ä–∞–Ω—Ç–∏–µ–π —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏.
        
        –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ ‚Üí –æ—Ç–∫–∞—Ç—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ —ç—Ç—É —Å—Ç—Ä–∞–Ω–∏—Ü—É, –Ω–µ –≤–µ—Å—å –±–∞—Ç—á.
        """
        updated, errors, skipped = 0, 0, 0
        error_details = []
        
        for page in batch:
            page_id = str(page.get('id', ''))
            if not page_id:
                skipped += 1
                continue
            
            title = page.get('title', 'Unknown')
            ts = get_timestamp(page)
            
            # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ (Bloom Filter –∏–ª–∏ set())
            if page_id in self.processed_ids:
                skipped += 1
                continue
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –Ω–µ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å
            if page_id in state['pages'] and state['pages'][page_id].get('updated') == ts:
                skipped += 1
                continue
            
            try:
                # –ü—ã—Ç–∞–µ–º—Å—è –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å retry
                for attempt in range(self.max_retries):
                    try:
                        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É (—Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º)
                        page_data = get_page_cached(confluence, page_id)
                        
                        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                        page_metadata = extract_page_metadata(page_data, space_key=space_key)
                        page_metadata['attachments'] = get_page_attachments(confluence, page_id)
                        
                        html = page_data.get('body', {}).get('storage', {}).get('value', '')
                        if not html or len(html) < MIN_TEXT_LEN:
                            skipped += 1
                            break
                        
                        # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω–∞—è –Ω–∞—Ä–µ–∑–∫–∞
                        blocks = extract_structural_blocks(html)
                        if not blocks:
                            skipped += 1
                            break
                        
                        chunks = smart_chunk_with_context(blocks, max_size=CHUNK_SIZE)
                        
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º HTML –≤ markdown
                        h = html2text.HTML2Text()
                        h.ignore_links = False
                        h.ignore_images = False
                        content_markdown = h.handle(html)
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ PostgreSQL
                        try:
                            version_when = page_data.get('version', {}).get('when', '')
                            if version_when:
                                if version_when.endswith('Z'):
                                    version_when = version_when[:-1] + '+00:00'
                                updated_at = datetime.fromisoformat(version_when)
                            else:
                                updated_at = datetime.now()
                        except Exception as e:
                            logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞—Ç—ã –¥–ª—è {page_id}: {e}")
                            updated_at = datetime.now()
                        
                        if not save_page_to_postgres(
                            page_id=page_id,
                            space_key=space_key,
                            title=title,
                            content_html=html,
                            content_markdown=content_markdown,
                            version=page_data.get('version', {}).get('number', 1),
                            metadata=page_metadata,
                            updated_at=updated_at
                        ):
                            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_id} –≤ PostgreSQL")
                            skipped += 1
                            break
                        
                        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏ –∏–∑ Qdrant
                        try:
                            delete_points_by_page_id(page_id)
                        except Exception as e:
                            logger.warning(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç–∞—Ä—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è {page_id}: {e}")
                        
                        # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —á–∞–Ω–∫–∏ –≤ Qdrant
                        for chunk_idx, chunk in enumerate(chunks):
                            chunk_text = chunk.get('text', '')
                            if not chunk_text:
                                continue
                            
                            # –°–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
                            # –ö–†–ò–¢–ò–ß–ù–û: –î–æ–±–∞–≤–ª—è–µ–º text –≤ metadata, —Ç.–∫. LlamaIndex QdrantVectorStore 
                            # –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç Document.text –≤ payload –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
                            sanitized_metadata = sanitize_metadata({
                                'text': chunk_text,  # –ö–†–ò–¢–ò–ß–ù–û: —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ metadata –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ payload
                                'page_id': page_id,
                                'chunk': chunk_idx,
                                'chunk_type': chunk.get('type', 'text'),
                                'heading': chunk.get('heading', '')[:200],  # –û–±—Ä–µ–∑–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
                                'heading_level': chunk.get('level', 0),
                                'parent_path': chunk.get('parent_path', '')[:200],
                                'space': space_key,
                                'title': title[:200] if title else '',  # –û–±—Ä–µ–∑–∞–µ–º title
                                'url': f"{CONFLUENCE_URL}pages/viewpage.action?pageId={page_id}",
                                # –í–∫–ª—é—á–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–æ–ª—è –∏–∑ page_metadata
                                'created': page_metadata.get('created', ''),
                                'modified': page_metadata.get('modified', ''),
                                'author': page_metadata.get('author', ''),
                                'space_key': page_metadata.get('space_key', space_key),
                                # –ù–ï –≤–∫–ª—é—á–∞–µ–º –ø–æ–ª–Ω—ã–π HTML, breadcrumb, headings_list –∏ —Ç.–¥.!
                            })
                            
                            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding –¥–ª—è chunk
                            embedding = generate_query_embedding(chunk_text)
                            point_id = f"{page_id}_{chunk_idx}"
                            
                            # –í—Å—Ç–∞–≤–ª—è–µ–º –Ω–∞–ø—Ä—è–º—É—é –≤ Qdrant
                            success = insert_chunk_to_qdrant(
                                client=qdrant_client,
                                chunk_text=chunk_text,
                                metadata=sanitized_metadata,
                                embedding=embedding,
                                point_id=point_id
                            )
                            if not success:
                                logger.warning(f"Failed to insert chunk {point_id} for page {page_id}")
                        
                        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
                        state['pages'][page_id] = {'updated': ts, 'title': title}
                        
                        # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤ PostgreSQL
                        mark_as_indexed(page_id)
                        
                        # ‚úÖ –î–æ–±–∞–≤–ª—è–µ–º –≤ Bloom Filter –∏–ª–∏ set()
                        if self._use_bloom:
                            self.processed_ids.add(page_id)
                        else:
                            self.processed_ids.add(page_id)
                        updated += 1
                        break  # –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ
                        
                    except Exception as retry_error:
                        if attempt < self.max_retries - 1:
                            wait_time = 2 ** attempt
                            logger.warning(f"Retry {attempt+1}/{self.max_retries} for page {page_id}: {retry_error}. Waiting {wait_time}s...")
                            time.sleep(wait_time)  # Exponential backoff
                        else:
                            raise
                
            except Exception as e:
                # –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ, –Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–∞—Ç—á
                error_msg = f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_id} ({title}): {e}"
                logger.error(error_msg)
                self.failed_ids[page_id] = str(e)
                errors += 1
                error_details.append({
                    "page_id": page_id,
                    "title": title,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat()
                })
        
        return updated, errors, skipped, error_details
    
    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        # ‚úÖ –î–ª—è Bloom Filter –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä
        if self._use_bloom:
            processed_count = self.processed_ids.count  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        else:
            processed_count = len(self.processed_ids)
        
        return {
            "processed": processed_count,
            "failed": len(self.failed_ids),
            "failed_pages": self.failed_ids,
            "using_bloom_filter": self._use_bloom
        }

def process_batch(qdrant_client: Any, confluence: Confluence, 
                  pages: List[Dict[str, Any]], state: Dict[str, Any], space_key: str) -> tuple[int, int, int, list]:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ batch —Å—Ç—Ä–∞–Ω–∏—Ü Confluence.
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ PostgreSQL, –∑–∞—Ç–µ–º –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –≤ Qdrant.
    
    Args:
        index: Qdrant VectorStoreIndex
        qdrant_client: Qdrant –∫–ª–∏–µ–Ω—Ç
        confluence: Confluence API client
        pages: –°–ø–∏—Å–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        state: –°–æ—Å—Ç–æ—è–Ω–∏–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
        space_key: –ö–ª—é—á –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
    
    Returns:
        Tuple[updated, errors, skipped, error_details] - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    # –¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º
    if TEST_MODE:
        pages = pages[:TEST_MAX_PAGES]
        logger.info(f"üß™ TEST MODE ENABLED - Processing only first {len(pages)} pages")
    
    updated, errors, skipped = 0, 0, 0
    error_details = []  # –°–ø–∏—Å–æ–∫ –¥–µ—Ç–∞–ª–µ–π –æ—à–∏–±–æ–∫
    for page in pages:
        pid = str(page.get('id', ''))
        if not pid:
            skipped += 1
            continue
        title = page.get('title', 'Unknown')
        ts = get_timestamp(page)
        try:
            if pid in state['pages'] and state['pages'][pid].get('updated') == ts:
                skipped += 1
                continue
            try:
                page_data = get_page(confluence, pid)
            except Exception as e:
                error_msg = f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {pid} ({title}): {e}"
                logger.warning(error_msg)
                error_details.append(error_msg)
                skipped += 1
                continue
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            page_metadata = extract_page_metadata(page_data, space_key=space_key)
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤–ª–æ–∂–µ–Ω–∏–π
            page_metadata['attachments'] = get_page_attachments(confluence, pid)
            
            html = page_data.get('body', {}).get('storage', {}).get('value', '')
            if not html or len(html) < MIN_TEXT_LEN:
                skipped += 1
                continue
            
            # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω–∞—è –Ω–∞—Ä–µ–∑–∫–∞
            blocks = extract_structural_blocks(html)
            if not blocks:
                skipped += 1
                continue
            
            chunks = smart_chunk_with_context(blocks, max_size=CHUNK_SIZE)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º HTML –≤ markdown –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è
            h = html2text.HTML2Text()
            h.ignore_links = False
            h.ignore_images = False
            content_markdown = h.handle(html)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤ PostgreSQL
            try:
                version_when = page_data.get('version', {}).get('when', '')
                if version_when:
                    # –ü–∞—Ä—Å–∏–º ISO —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã
                    if version_when.endswith('Z'):
                        version_when = version_when[:-1] + '+00:00'
                    updated_at = datetime.fromisoformat(version_when)
                else:
                    updated_at = datetime.now()
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞—Ç—ã –¥–ª—è {pid}: {e}, –∏—Å–ø–æ–ª—å–∑—É—é —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É")
                updated_at = datetime.now()
            
            if not save_page_to_postgres(
                page_id=pid,
                space_key=space_key,
                title=title,
                content_html=html,
                content_markdown=content_markdown,
                version=page_data.get('version', {}).get('number', 1),
                metadata=page_metadata,
                updated_at=updated_at
            ):
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {pid} –≤ PostgreSQL")
                skipped += 1
                continue
            
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏ –∏–∑ Qdrant
            try:
                delete_points_by_page_id(pid)
            except Exception as e:
                logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏ –¥–ª—è {pid}: {e}")
            
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ chunks –¥–ª—è batch –≤—Å—Ç–∞–≤–∫–∏
            chunks_to_insert = []
            page_url = f"{CONFLUENCE_URL.rstrip('/')}/wiki/spaces/{space_key}/pages/{pid}"
            
            # –ü–æ–ª–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: –±–∞–∑–æ–≤—ã–µ + –∑–∞–≥–æ–ª–æ–≤–∫–∏ + Confluence –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            labels_list = page_metadata.get('labels', [])
            labels_str = ",".join(labels_list) if labels_list else ""
            
            attachments_list = page_metadata.get('attachments', [])
            attachments_str = ",".join(attachments_list) if attachments_list else ""
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –ø–æ–ª–µ–π (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å Qdrant)
            max_str_len = 500
            title_safe = title[:max_str_len] if title else "Unknown"
            parent_safe = page_metadata.get('parent_title', '')[:max_str_len]
            author_safe = page_metadata.get('created_by', '')[:max_str_len]
            attachments_safe = attachments_str[:max_str_len]
            page_path_safe = page_metadata.get('page_path', '')[:max_str_len]
            breadcrumb_safe = page_metadata.get('breadcrumb', '')[:max_str_len]
            
            for i, chunk_data in enumerate(chunks):
                # chunk_data —Ç–µ–ø–µ—Ä—å —Å–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–∞–º–∏: text, heading, level
                if not isinstance(chunk_data, dict):
                    logger.warning(f"Unexpected chunk_data type: {type(chunk_data)}")
                    continue
                
                chunk_content = chunk_data.get("text", "")
                if not chunk_content or len(chunk_content) < 20:
                    continue
                
                heading_safe = chunk_data.get("heading", "")[:max_str_len]
                labels_safe = labels_str[:max_str_len]
                parent_path_safe = chunk_data.get("parent_path", "")[:max_str_len]
                
                # –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                block_type = chunk_data.get("type", "text")
                block_size = chunk_data.get("size", 0)
                is_complete = block_type in ["table", "list"]
                heading_path = (parent_path_safe + " > " + heading_safe if parent_path_safe else heading_safe)[:max_str_len]
                
                metadata = {
                    # –ë–∞–∑–æ–≤—ã–µ
                    "page_id": pid,
                    "chunk": i,
                    "title": title_safe,
                    "space": space_key,
                    "url": page_url,
                    # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    "heading": heading_safe,
                    "heading_level": chunk_data.get("level", 0),
                    # –ù–û–í–´–ï –ü–û–õ–Ø –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π –Ω–∞—Ä–µ–∑–∫–∏
                    "type": block_type,                          # table|list|text (—Ç–∏–ø –±–ª–æ–∫–∞)
                    "parent_path": parent_path_safe,             # –ò–µ—Ä–∞—Ä—Ö–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
                    "block_size": block_size,                    # –†–∞–∑–º–µ—Ä –±–ª–æ–∫–∞
                    "is_complete_block": is_complete,            # –¶–µ–ª—ã–π –±–ª–æ–∫ –∏–ª–∏ —á–∞—Å—Ç—å
                    "has_table": block_type == "table",          # –°–æ–¥–µ—Ä–∂–∏—Ç —Ç–∞–±–ª–∏—Ü—É
                    "heading_path": heading_path,                # –ü–æ–ª–Ω—ã–π –ø—É—Ç—å
                    # Confluence –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    "labels": labels_safe,
                    "parent_title": parent_safe,                 # –†–æ–¥–∏—Ç–µ–ª—å—Å–∫–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ (–±–ª–∏–∂–∞–π—à–∏–π)
                    "page_path": page_path_safe[:200] if page_path_safe else '',  # URL-friendly –ø—É—Ç—å (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π)
                    "breadcrumb": breadcrumb_safe[:200] if breadcrumb_safe else '',  # –ü–æ–ª–Ω—ã–π –ø—É—Ç—å —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏ (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π)
                    "created_by": author_safe,
                    "has_children": page_metadata.get('has_children', False),
                    "version": page_metadata.get('version', 1),
                    "attachments": attachments_safe[:10] if attachments_safe else [],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤–ª–æ–∂–µ–Ω–∏–π
                    # –ù–û–í–´–ï –ü–û–õ–Ø –¥–ª—è metadata filtering (–∏–∑ extract_page_metadata):
                    "status": page_metadata.get('status', 'current'),  # current, archived, draft
                    "content_type": page_metadata.get('type', 'page'),  # page, blogpost, attachment
                    "hierarchy_depth": page_metadata.get('hierarchy_depth', 0),
                    "created": page_metadata.get('created', ''),
                    "modified": page_metadata.get('modified', ''),
                    "modified_by": page_metadata.get('modified_by', ''),
                    "children_count": page_metadata.get('children_count', 0),
                    # === –ü–£–¢–¨ –ò –ó–ê–ì–û–õ–û–í–ö–ò (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ) ===
                    "headings": (page_metadata.get('headings', '') or '')[:500],  # –û–±—Ä–µ–∑–∞–µ–º headings
                    "headings_list": (page_metadata.get('headings_list', []) or [])[:10],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Å–ø–∏—Å–æ–∫
                    "heading_count": page_metadata.get('heading_count', 0)
                }
                
                # –°–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
                # –ö–†–ò–¢–ò–ß–ù–û: –î–æ–±–∞–≤–ª—è–µ–º text –≤ metadata, —Ç.–∫. LlamaIndex QdrantVectorStore 
                # –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç Document.text –≤ payload –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
                metadata['text'] = chunk_content  # –ö–†–ò–¢–ò–ß–ù–û: —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ metadata –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ payload
                sanitized_metadata = sanitize_metadata(metadata)
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding –¥–ª—è chunk
                embedding = generate_query_embedding(chunk_content)
                point_id = f"{pid}_{i}"
                
                chunks_to_insert.append({
                    'text': chunk_content,
                    'metadata': sanitized_metadata,
                    'embedding': embedding,
                    'point_id': point_id
                })
            
            # Batch –≤—Å—Ç–∞–≤–∫–∞ (–µ—Å–ª–∏ chunks > 10) –∏–ª–∏ single
            inserted = 0
            if len(chunks_to_insert) > 10:
                success_count, error_count = insert_chunks_batch_to_qdrant(
                    client=qdrant_client,
                    chunks_data=chunks_to_insert,
                    batch_size=100
                )
                inserted = success_count
                if error_count > 0:
                    logger.warning(f"Failed to insert {error_count} chunks for page {pid}")
            else:
                for chunk_data in chunks_to_insert:
                    success = insert_chunk_to_qdrant(
                        client=qdrant_client,
                        chunk_text=chunk_data['text'],
                        metadata=chunk_data['metadata'],
                        embedding=chunk_data['embedding'],
                        point_id=chunk_data['point_id']
                    )
                    if success:
                        inserted += 1
            if inserted > 0:
                state['pages'][pid] = {'updated': ts, 'chunks': inserted}
                # –ü–æ–º–µ—á–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–∞–∫ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤ PostgreSQL
                mark_as_indexed(pid)
                updated += 1
            else:
                skipped += 1
        except Exception as e:
            import traceback
            error_msg = f"Error processing page {pid} ({title}): {e}"
            logger.error(error_msg)
            logger.error(f"Traceback: {traceback.format_exc()}")
            error_details.append(error_msg)
            errors += 1
    return updated, errors, skipped, error_details

def get_blogposts_from_space(confluence: Confluence, space_key: str, start: int = 0, limit: int = 50) -> List[Dict[str, Any]]:
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ blog posts –∏–∑ space (–æ–±—ë—Ä—Ç–∫–∞ –¥–ª—è API).
    
    Args:
        confluence: Confluence API client
        space_key: –ö–ª—é—á –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
        start: –ù–∞—á–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è
        limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤
    
    Returns:
        –°–ø–∏—Å–æ–∫ blog posts
    """
    try:
        # API –∑–∞–ø—Ä–æ—Å –¥–ª—è –±–ª–æ–≥–æ–≤
        url = f"{confluence.url}/rest/api/content?type=blogpost&spaceKey={space_key}&start={start}&limit={limit}&expand=version"
        response = requests.get(url, headers=confluence.default_headers, verify=VERIFY_SSL)
        response.raise_for_status()
        data = response.json()
        return data.get('results', [])
    except Exception as e:
        logger.debug(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è blogposts –¥–ª—è {space_key}: {e}")
        return []

def cleanup_deleted_pages(qdrant_client: Any, state: Dict[str, Any], current_page_ids: set) -> int:
    """
    –£–¥–∞–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ —É–¥–∞–ª–µ–Ω—ã –≤ Confluence.
    
    Args:
        qdrant_client: Qdrant –∫–ª–∏–µ–Ω—Ç
        state: –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
        current_page_ids: –ù–∞–±–æ—Ä ID —Å—Ç—Ä–∞–Ω–∏—Ü, –∫–æ—Ç–æ—Ä—ã–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –≤ Confluence
    
    Returns:
        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
    """
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∏–∑ postgres_storage –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ PostgreSQL
    deleted_count = cleanup_deleted_pages_postgres(current_page_ids)
    
    # –£–¥–∞–ª—è–µ–º –∏–∑ Qdrant —á–µ—Ä–µ–∑ delete_points_by_page_id
    state_page_ids = set(state.get('pages', {}).keys())
    deleted_page_ids = state_page_ids - current_page_ids
    
    if not deleted_page_ids:
        logger.debug("–ù–µ—Ç —É–¥–∞–ª—ë–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ—á–∏—Å—Ç–∫–∏")
        return deleted_count
    
    logger.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(deleted_page_ids)} —É–¥–∞–ª—ë–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –≤ Confluence")
    
    for page_id in deleted_page_ids:
        try:
            # –£–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤ —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ Qdrant
            if delete_points_by_page_id(page_id):
                logger.info(f"  –£–¥–∞–ª–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_id} –∏–∑ Qdrant")
            
            # –£–¥–∞–ª–µ–Ω–∏–µ –∏–∑ state
            if page_id in state['pages']:
                del state['pages'][page_id]
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_id}: {e}")
    
    return deleted_count

def sync() -> None:
    """–û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ Confluence —Å PostgreSQL + Qdrant."""
    logger.info("Sync started")
    state = load_state()
    start_time = time.time()
    try:
        # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ø–∞—Ä–∞–º–µ—Ç—Ä verify_ssl –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –≤ —Ç–µ–∫—É—â–µ–π –≤–µ—Ä—Å–∏–∏ atlassian-python-api
        confluence = Confluence(url=CONFLUENCE_URL, token=CONFLUENCE_TOKEN)
        logger.info("Connected to Confluence")
    except Exception as e:
        logger.error(f"Confluence error: {e}")
        return
    try:
        from embeddings import get_embedding_dimension
        
        logger.info("–®–∞–≥ 1: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è PostgreSQL...")
        init_postgres_schema()
        logger.info("‚úÖ PostgreSQL schema initialized")
        
        logger.info("–®–∞–≥ 2: –ó–∞–≥—Ä—É–∑–∫–∞ embedding –º–æ–¥–µ–ª–∏ (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å ~60 —Å–µ–∫)...")
        embed_model = get_embed_model()
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {type(embed_model)}")
        
        model_dim = get_embedding_dimension()
        
        logger.info("–®–∞–≥ 3: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Qdrant...")
        qdrant_client = init_qdrant_client()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Qdrant –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        if not init_qdrant_collection(model_dim):
            raise ValueError(
                f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Qdrant –∫–æ–ª–ª–µ–∫—Ü–∏—é. "
                f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {model_dim}D"
            )
        
        logger.info(f"‚úÖ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å embeddings: {model_dim}D")
        logger.info("‚úÖ Qdrant client ready")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        doc_count = get_qdrant_count()
        logger.info(f"–¢–µ–∫—É—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ Qdrant: {doc_count}")
        
    except ValueError as ve:
        # –≠—Ç–æ –æ—à–∏–±–∫–∞ –Ω–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ - –Ω–µ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
        logger.error(f"Sync –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {ve}")
        return
    except Exception as e:
        logger.error(f"Init error: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        return
    total_updated, total_errors, total_skipped = 0, 0, 0
    current_page_ids = set()  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
    
    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞–º
    space_stats = {}
    
    try:
        all_spaces = confluence.get_all_spaces().get('results', [])
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤
        if CONFLUENCE_SPACES:
            # –ü–∞—Ä—Å–∏–º —Å–ø–∏—Å–æ–∫ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤ –∏–∑ ENV (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)
            target_spaces = [s.strip().upper() for s in CONFLUENCE_SPACES.split(',') if s.strip()]
            spaces = [s for s in all_spaces if s.get('key', '').upper() in target_spaces]
            
            # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ, –µ—Å–ª–∏ –æ–±–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ —É–∫–∞–∑–∞–Ω—ã
            if MAX_SPACES != 10:  # –ï—Å–ª–∏ MAX_SPACES –∏–∑–º–µ–Ω–µ–Ω –æ—Ç –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                logger.warning(f"‚ö†Ô∏è  –£–∫–∞–∑–∞–Ω—ã –æ–±–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞: CONFLUENCE_SPACES –∏ MAX_SPACES={MAX_SPACES}")
                logger.warning(f"   –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CONFLUENCE_SPACES (MAX_SPACES –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è)")
            
            logger.info(f"–§–∏–ª—å—Ç—Ä –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤: {len(spaces)} –∏–∑ {len(all_spaces)} (—É–∫–∞–∑–∞–Ω—ã: {CONFLUENCE_SPACES})")
            
            # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ, –µ—Å–ª–∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —É–∫–∞–∑–∞–Ω–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã
            found_keys = {s.get('key', '').upper() for s in spaces}
            not_found = [t for t in target_spaces if t not in found_keys]
            if not_found:
                logger.warning(f"‚ö†Ô∏è  –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ Confluence: {', '.join(not_found)}")
        else:
            # –°—Ç–∞—Ä–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ: MAX_SPACES
            spaces = all_spaces[:MAX_SPACES]
            logger.info(f"Processing {len(spaces)} spaces (MAX_SPACES={MAX_SPACES})")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º tqdm –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        if TQDM_AVAILABLE:
            spaces_iter = tqdm(spaces, desc="Syncing spaces", unit="space")
        else:
            spaces_iter = spaces
        
        for space in spaces_iter:
            key = space.get('key', '')
            if not key:
                continue
            
            space_name = space.get('name', key)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ progress bar
            if TQDM_AVAILABLE:
                spaces_iter.set_description(f"Syncing {space_name}")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
            space_stats[key] = {
                'total_pages': 0,
                'total_blogs': 0,
                'processed': 0,
                'updated': 0,
                'skipped': 0,
                'errors': 0,
                'chunks_created': 0,
                'error_details': []
            }
            
            logger.info(f"üìÇ {space_name}:")
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≤–º–µ—Å—Ç–æ –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
                all_pages = []
                for page in get_all_pages_generator(confluence, key, batch_size=BATCH_SIZE):
                    all_pages.append(page)
                    page_id = str(page.get('id', ''))
                    if page_id:
                        current_page_ids.add(page_id)
                
                space_stats[key]['total_pages'] = len(all_pages)
                logger.info(f"   –°—Ç—Ä–∞–Ω–∏—Ü –Ω–∞–π–¥–µ–Ω–æ: {len(all_pages)} (–±–ª–æ–≥–æ–≤: 0)")
                
                if not all_pages:
                    logger.info(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: 0")
                    logger.info(f"   –û–±–Ω–æ–≤–ª–µ–Ω–æ: 0 | –ü—Ä–æ–ø—É—â–µ–Ω–æ: 0 | –û—à–∏–±–æ–∫: 0")
                    logger.info(f"   Chunks —Å–æ–∑–¥–∞–Ω–æ: 0")
                    continue
                
                # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏
                batches = [all_pages[i:i+BATCH_SIZE] for i in range(0, len(all_pages), BATCH_SIZE)]
                
                # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ —Å ThreadPoolExecutor
                max_workers = get_int_env("PARALLEL_SYNC_MAX_WORKERS", 4)
                batch_processor = BatchProcessor(max_retries=3)
                
                if TQDM_AVAILABLE:
                    batches_iter = tqdm(batches, desc=f"  Processing {space_name}", unit="batch", leave=False)
                else:
                    batches_iter = batches
                
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤—Å–µ –±–∞—Ç—á–∏ –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É
                    future_to_batch = {
                        executor.submit(
                            batch_processor.process_batch_safe,
                            qdrant_client, confluence, batch, state, key
                        ): batch
                        for batch in batches
                    }
                    
                    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    for future in as_completed(future_to_batch):
                        batch = future_to_batch[future]
                        try:
                            batch_updated, batch_errors, batch_skipped, batch_error_details = future.result()
                            total_updated += batch_updated
                            total_errors += batch_errors
                            total_skipped += batch_skipped
                            space_stats[key]['updated'] += batch_updated
                            space_stats[key]['errors'] += batch_errors
                            space_stats[key]['skipped'] += batch_skipped
                            space_stats[key]['processed'] += len(batch)
                            space_stats[key]['error_details'].extend(batch_error_details)
                            
                            if TQDM_AVAILABLE:
                                batches_iter.update(1)
                                batches_iter.set_postfix({
                                    "updated": space_stats[key]['updated'],
                                    "errors": space_stats[key]['errors']
                                })
                        except Exception as e:
                            logger.error(f"Batch processing failed: {e}")
                            total_errors += 1
                            space_stats[key]['errors'] += 1
                            if TQDM_AVAILABLE:
                                batches_iter.update(1)
                
                # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
                logger.info(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {space_stats[key]['processed']}")
                logger.info(f"   –û–±–Ω–æ–≤–ª–µ–Ω–æ: {space_stats[key]['updated']} | –ü—Ä–æ–ø—É—â–µ–Ω–æ: {space_stats[key]['skipped']} | –û—à–∏–±–æ–∫: {space_stats[key]['errors']}")
                
                # –ü–æ–¥—Å—á–µ—Ç chunks (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
                chunks_count = 0
                # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç –∏–∑ Qdrant –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                space_stats[key]['chunks_created'] = chunks_count
                logger.info(f"   Chunks —Å–æ–∑–¥–∞–Ω–æ: {chunks_count}")
                
                # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–∞
                cache_stats = get_cache_stats()
                logger.debug(f"   Cache stats: {cache_stats}")
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ blog posts (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ, –Ω–æ –±–µ–∑ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã)
                try:
                    all_blogs = []
                    blog_start = 0
                    while True:
                        batch_blogs = get_blogposts_from_space(confluence, key, start=blog_start, limit=BATCH_SIZE)
                        if not batch_blogs:
                            break
                        all_blogs.extend(batch_blogs)
                        blog_start += BATCH_SIZE
                    
                    for blog in all_blogs:
                        blog_id = str(blog.get('id', ''))
                        if blog_id:
                            current_page_ids.add(blog_id)
                    
                    space_stats[key]['total_blogs'] = len(all_blogs)
                    if all_blogs:
                        logger.info(f"   –ë–ª–æ–≥–æ–≤ –Ω–∞–π–¥–µ–Ω–æ: {len(all_blogs)}")
                        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–ª–æ–≥–æ–≤ (–º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ–∑–∂–µ)
                        for i in range(0, len(all_blogs), BATCH_SIZE):
                            batch = all_blogs[i:i+BATCH_SIZE]
                            updated, errors, skipped, error_details = batch_processor.process_batch_safe(
                                qdrant_client, confluence, batch, state, key
                            )
                            total_updated += updated
                            total_errors += errors
                            total_skipped += skipped
                            space_stats[key]['updated'] += updated
                            space_stats[key]['errors'] += errors
                            space_stats[key]['skipped'] += skipped
                            space_stats[key]['processed'] += len(batch)
                            space_stats[key]['error_details'].extend(error_details)
                except Exception as blog_err:
                    logger.warning(f"Error processing blogs for {key}: {blog_err}")
                    space_stats[key]['error_details'].append(f"Blog processing error: {str(blog_err)}")
                    
            except Exception as e:
                logger.error(f"Space error for {key}: {e}")
                total_errors += 1
                space_stats[key]['errors'] += 1
                space_stats[key]['error_details'].append(f"Space processing error: {str(e)}")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º postfix –¥–ª—è progress bar
            if TQDM_AVAILABLE:
                spaces_iter.set_postfix({
                    "updated": total_updated,
                    "errors": total_errors
                })
    except Exception as e:
        logger.error(f"Critical: {e}")
        return
    
    # –ü–æ–¥—Å—á–µ—Ç chunks –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
    try:
        from qdrant_storage import get_points_by_filter
        for space_key in space_stats.keys():
            space_data = get_points_by_filter(filter_dict={"space": space_key}, limit=10000)
            space_stats[space_key]['chunks_created'] = len(space_data.get('ids', []))
    except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥—Å—á–∏—Ç–∞—Ç—å chunks –ø–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞–º: {e}")
    
    # –û—á–∏—Å—Ç–∫–∞ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
    deleted_count = cleanup_deleted_pages(qdrant_client, state, current_page_ids)
    
    # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–º–µ–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ Qdrant
    # (—Ä–∞–Ω–µ–µ –±—ã–ª–æ –¥–ª—è ChromaDB, —Ç—Ä–µ–±—É–µ—Ç –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –¥–ª—è Qdrant)
    
    state['last_sync'] = int(time.time())
    save_state(state)
    elapsed = time.time() - start_time
    
    # ============ –î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ü–†–û–°–¢–†–ê–ù–°–¢–í–ê–ú ============
    logger.info("")
    logger.info("=" * 80)
    logger.info("üìä –ò–¢–û–ì–ò –°–ò–ù–•–†–û–ù–ò–ó–ê–¶–ò–ò")
    logger.info("=" * 80)
    logger.info(f"‚è±  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {elapsed:.1f}—Å ({elapsed/60:.1f} –º–∏–Ω)")
    logger.info("")
    logger.info("üìÅ –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ü–†–û–°–¢–†–ê–ù–°–¢–í–ê–ú:")
    logger.info("-" * 80)
    
    for space_key, stats in sorted(space_stats.items()):
        logger.info(f"  üìÇ {space_key}:")
        logger.info(f"     –°—Ç—Ä–∞–Ω–∏—Ü –Ω–∞–π–¥–µ–Ω–æ: {stats['total_pages']} (–±–ª–æ–≥–æ–≤: {stats['total_blogs']})")
        logger.info(f"     –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['processed']}")
        logger.info(f"     –û–±–Ω–æ–≤–ª–µ–Ω–æ: {stats['updated']} | –ü—Ä–æ–ø—É—â–µ–Ω–æ: {stats['skipped']} | –û—à–∏–±–æ–∫: {stats['errors']}")
        logger.info(f"     Chunks —Å–æ–∑–¥–∞–Ω–æ: {stats['chunks_created']}")
        if stats['error_details']:
            logger.warning(f"     ‚ö†Ô∏è  –û—à–∏–±–∫–∏ ({len(stats['error_details'])}):")
            for err in stats['error_details'][:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –æ—à–∏–±–æ–∫
                logger.warning(f"        - {err}")
            if len(stats['error_details']) > 5:
                logger.warning(f"        ... –∏ –µ—â–µ {len(stats['error_details']) - 5} –æ—à–∏–±–æ–∫")
        logger.info("")
    
    logger.info("=" * 80)
    logger.info("üìà –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    logger.info(f"   ‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ: {total_updated}")
    logger.info(f"   ‚è≠  –ü—Ä–æ–ø—É—â–µ–Ω–æ: {total_skipped}")
    logger.info(f"   ‚ùå –û—à–∏–±–æ–∫: {total_errors}")
    logger.info(f"   üóë  –£–¥–∞–ª–µ–Ω–æ: {deleted_count}")
    logger.info(f"   ‚è±  –í—Ä–µ–º—è: {elapsed:.1f}—Å")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ PostgreSQL –∏ Qdrant
    try:
        pg_stats = get_postgres_stats()
        qdrant_count = get_qdrant_count()
        logger.info("")
        logger.info("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –•–†–ê–ù–ò–õ–ò–©:")
        logger.info(f"   PostgreSQL: {pg_stats['total_pages']} —Å—Ç—Ä–∞–Ω–∏—Ü ({pg_stats['not_indexed']} –Ω–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ)")
        logger.info(f"   Qdrant: {qdrant_count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ö—Ä–∞–Ω–∏–ª–∏—â: {e}")
    
    logger.info("=" * 80)

if __name__ == "__main__":
    sync()
    logger.info(f"–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –±—É–¥–µ—Ç –ø–æ–≤—Ç–æ—Ä—è—Ç—å—Å—è –∫–∞–∂–¥—ã–µ {SYNC_INTERVAL} —Å–µ–∫—É–Ω–¥ ({SYNC_INTERVAL/3600:.1f} —á–∞—Å–æ–≤)")
    while True:
        try:
            time.sleep(SYNC_INTERVAL)
            sync()
        except KeyboardInterrupt:
            logger.info("–ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏, –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...")
            break
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –≥–ª–∞–≤–Ω–æ–º —Ü–∏–∫–ª–µ: {e}")
            time.sleep(60)
