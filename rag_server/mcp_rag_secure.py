"""
MCP —Å–µ—Ä–≤–µ—Ä –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ Confluence.
–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è Open WebUI —á–µ—Ä–µ–∑ Model Context Protocol.
"""
from typing import Any, List, Dict
import logging
import os
import re
import sys
import time

from fastmcp import FastMCP
from qdrant_client import QdrantClient

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑ ENV
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
logging.basicConfig(
    level=getattr(logging, LOG_LEVEL, logging.INFO),
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# –û—Ç–∫–ª—é—á–∞–µ–º –∏–∑–±—ã—Ç–æ—á–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ HTTP –∑–∞–ø—Ä–æ—Å–æ–≤ –æ—Ç httpx/openai
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)
logging.getLogger("openai").setLevel(logging.WARNING)

# Qdrant configuration
QDRANT_HOST = os.getenv("QDRANT_HOST", "qdrant")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", "6333"))
QDRANT_COLLECTION = os.getenv("QDRANT_COLLECTION", "confluence")
# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–æ–¥—É–ª—å embeddings
from embeddings import (
    get_embedding_dimension,
    EMBED_MODEL,
    USE_OLLAMA
)

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ –º–æ–¥—É–ª–∏ –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –ø–æ–∏—Å–∫–∞
from synonyms_manager import get_synonyms_manager

from advanced_search import extract_keywords
from query_rewriter import cached_rewrite_query, get_rewriter_stats
from observability import setup_observability
from hybrid_search import init_bm25_retriever

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è reranker (–ª–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è)
reranker = None


def init_reranker():
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è CrossEncoder –¥–ª—è reranking.

    –ú–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ –∏ –∫—ç—à–∏—Ä—É–µ—Ç—Å—è –≥–ª–æ–±–∞–ª—å–Ω–æ.
    –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏ (‚âà30-40 —Å–µ–∫) –ø—Ä–∏ –∫–∞–∂–¥–æ–º –∑–∞–ø—Ä–æ—Å–µ.

    –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Russian MS-MARCO –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å —Ä—É—Å—Å–∫–∏–º —è–∑—ã–∫–æ–º.
    –ú–æ–¥–µ–ª—å –º–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è RE_RANKER_MODEL.
    """
    global reranker
    if reranker is None:
        try:
            start_time = time.time()
            from sentence_transformers import CrossEncoder

            # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ ENV –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç (Russian MS-MARCO)
            model_name = os.getenv(
                'RE_RANKER_MODEL',
                'DiTy/cross-encoder-russian-msmarco'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é Russian MS-MARCO
            )

            logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è CrossEncoder –¥–ª—è reranking...")
            logger.info(f"  –ú–æ–¥–µ–ª—å: {model_name}")

            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö
            model_info = {
                'BAAI/bge-reranker-v2-m3': {
                    'name': 'BAAI bge-reranker-v2-m3',
                    'language': '100+ —è–∑—ã–∫–æ–≤ (–≤–∫–ª—é—á–∞—è —Ä—É—Å—Å–∫–∏–π)',
                    'quality': '95%+ –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤'
                },
                'DiTy/cross-encoder-russian-msmarco': {
                    'name': 'Russian MS-MARCO',
                    'language': '–†—É—Å—Å–∫–∏–π',
                    'quality': '92% –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ'
                },
                'cross-encoder/ms-marco-MiniLM-L-6-v2': {
                    'name': 'MS-MARCO MiniLM',
                    'language': '–ê–Ω–≥–ª–∏–π—Å–∫–∏–π (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è)',
                    'quality': '85% –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ'
                },
                'Qwen/Qwen3-Reranker-8B': {
                    'name': 'Qwen3 Reranker',
                    'language': '100+ —è–∑—ã–∫–æ–≤',
                    'quality': '95% (—Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ —Ä–µ—Å—É—Ä—Å–æ–≤)'
                }
            }

            info = model_info.get(model_name, {})
            if info:
                logger.info(f"  –ù–∞–∑–≤–∞–Ω–∏–µ: {info.get('name', model_name)}")
                logger.info(f"  –Ø–∑—ã–∫: {info.get('language', 'N/A')}")
                logger.info(f"  –ö–∞—á–µ—Å—Ç–≤–æ: {info.get('quality', 'N/A')}")

            reranker = CrossEncoder(model_name)
            elapsed = time.time() - start_time
            logger.info(f"‚úÖ CrossEncoder –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∑–∞ {elapsed:.1f}—Å. –ú–æ–¥–µ–ª—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∞.")
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å reranker: {e}")
            reranker = None
    else:
        logger.debug("–ü–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ CrossEncoder")
    return reranker

def _get_max_variants(query: str) -> int:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è."""
    query_length = len(query.split())
    if query_length <= 2:
        return 5
    elif query_length <= 4:
        return 3
    return 2

def _expand_with_semantic_log(query: str, current_queries: list, max_variants: int):
    """–ò—Å—Ç–æ—á–Ω–∏–∫ 1: Semantic Query Log."""
    if len(current_queries) >= max_variants:
        return

    try:
        from semantic_query_log import get_semantic_query_log
        semantic_log = get_semantic_query_log()
        related_queries = semantic_log.get_related_queries(query, top_n=3)

        for related in related_queries:
            if related not in current_queries:
                current_queries.append(related)
                logger.debug(f"Semantic Query Log: –¥–æ–±–∞–≤–ª–µ–Ω –ø–æ—Ö–æ–∂–∏–π –∑–∞–ø—Ä–æ—Å '{related}'")
                if len(current_queries) >= max_variants:
                    break
    except Exception as e:
        logger.debug(f"Semantic Query Log –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")

def _expand_with_synonyms(query: str, current_queries: list, max_variants: int):
    """–ò—Å—Ç–æ—á–Ω–∏–∫ 2-4: SynonymsManager."""
    if len(current_queries) >= max_variants:
        return

    try:
        synonyms_manager = get_synonyms_manager()
        from synonyms_manager import TERM_BLACKLIST

        keywords = extract_keywords(query)
        query_lower = query.lower().strip()

        for keyword in keywords[:3]:
            if len(current_queries) >= max_variants:
                break

            keyword_lower = keyword.lower()
            if keyword_lower in TERM_BLACKLIST:
                continue

            synonyms = synonyms_manager.get_synonyms(keyword, max_synonyms=2)
            if not synonyms:
                continue

            for synonym in synonyms:
                pattern = r'\b' + re.escape(keyword_lower) + r'\b'
                expanded = re.sub(pattern, synonym.lower(), query_lower, flags=re.IGNORECASE)

                if expanded != query_lower and expanded not in current_queries:
                    current_queries.append(expanded)
                    if len(current_queries) >= max_variants:
                        break
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ SynonymsManager: {e}")

def _expand_with_rewriting(query: str, current_queries: list, max_variants: int):
    """–ò—Å—Ç–æ—á–Ω–∏–∫ 5: Query Rewriting."""
    if len(current_queries) >= max_variants:
        return

    try:
        # –ü–µ—Ä–µ–¥–∞–µ–º None –¥–ª—è semantic_log, —Ç–∞–∫ –∫–∞–∫ –æ–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ cached_rewrite_query –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
        rewrite_variants = cached_rewrite_query(query, semantic_log=None)
        for variant in rewrite_variants[1:]:
            if variant not in current_queries:
                current_queries.append(variant)
                logger.debug(f"Query rewriting variant: {variant}")
                if len(current_queries) >= max_variants:
                    break
    except Exception as e:
        logger.warning(f"Query rewriting failed: {e}")

def expand_query(query: str, space: str = "") -> list[str]:
    """
    –£–º–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å–∏–Ω–æ–Ω–∏–º–æ–≤.
    """
    queries = [query]
    max_variants = _get_max_variants(query)

    # –ò—Å—Ç–æ—á–Ω–∏–∫ 1: Semantic Query Log
    _expand_with_semantic_log(query, queries, max_variants)

    # –ò—Å—Ç–æ—á–Ω–∏–∫ 2-4: SynonymsManager
    _expand_with_synonyms(query, queries, max_variants)

    # –ò—Å—Ç–æ—á–Ω–∏–∫ 5: Query Rewriting
    _expand_with_rewriting(query, queries, max_variants)

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (—Å—Ç–æ–ø-—Å–ª–æ–≤–∞, space, 1–°)
    keywords = extract_keywords(query)
    if len(keywords) >= 2:
        clean_query = ' '.join(keywords)
        if clean_query not in queries:
            queries.append(clean_query)

    query_lower = query.lower()
    if space and len(query_lower.split()) <= 5:
        queries.append(f"{query} {space}")

    if any(term in query_lower for term in ['1—Å', '1c', '–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è']):
        normalized = query.replace('1–°', '1C').replace('1—Å', '1c')
        if normalized != query and normalized not in queries:
            queries.append(normalized)

    # –ò—Ç–æ–≥–æ–≤–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∏ –æ–±—Ä–µ–∑–∫–∞
    result = list(dict.fromkeys(queries))[:max_variants]

    if len(result) < len(queries):
        logger.debug(
            f"Query expansion –æ–≥—Ä–∞–Ω–∏—á–µ–Ω: {len(queries)} -> {len(result)} "
            f"–≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ (len={len(query.split())})"
        )

    return result

def calculate_optimal_candidate_limit(query: str, limit: int) -> int:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è reranking.

    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        limit: –ñ–µ–ª–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    Returns:
        –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
    """
    query_words = len(query.split())

    if query_words <= 2:
        multiplier = 5  # –ö–æ—Ä–æ—Ç–∫–∏–π –∑–∞–ø—Ä–æ—Å ‚Üí –±–æ–ª—å—à–µ —à—É–º–∞, –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
    elif query_words <= 4:
        multiplier = 3  # –°—Ä–µ–¥–Ω–∏–π –∑–∞–ø—Ä–æ—Å
    else:
        multiplier = 2  # –î–ª–∏–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å ‚Üí —É–∂–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π

    return min(limit * multiplier, 50)  # –ú–∞–∫—Å–∏–º—É–º 50 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤

def detect_content_type(text: str) -> str:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ —Ç–µ–∫—Å—Ç–µ.

    Returns:
        'table' | 'list' | 'code' | 'plain'
    """
    import re

    # –¢–∞–±–ª–∏—Ü—ã: | col1 | col2 | –∏–ª–∏ —Å—Ç—Ä–æ–∫–∏ —Å —Ç–∞–±—É–ª—è—Ü–∏–µ–π
    if re.search(r'\|.*\|.*\|', text) or text.count('\t') > 5:
        return 'table'

    # –°–ø–∏—Å–∫–∏: 3+ —Å—Ç—Ä–æ–∫ –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö—Å—è —Å *, -, ‚Ä¢, —Ü–∏—Ñ—Ä
    list_lines = re.findall(r'^\s*[\*\-‚Ä¢][\s\)]|^\s*\d+[\.\)]', text, re.MULTILINE)
    if len(list_lines) >= 3:
        return 'list'

    # –ö–æ–¥: ```code``` –∏–ª–∏ 5+ —Å—Ç—Ä–æ–∫ —Å –æ—Ç—Å—Ç—É–ø–∞–º–∏
    if '```' in text or len(re.findall(r'^\s{4,}', text, re.MULTILINE)) >= 5:
        return 'code'

    return 'plain'

def format_search_results(results: List[Dict[str, Any]], query: str, limit: int) -> str:  # noqa: C901
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –≤ —á–∏—Ç–∞–µ–º—ã–π —Ç–µ–∫—Å—Ç.

    Args:
        results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    Returns:
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    """
    if not results:
        return f"‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'"

    response = [f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:\n"]

    for i, r in enumerate(results[:limit], 1):
        if not r or not isinstance(r, dict):
            continue

        m = r.get('metadata', {})
        if not isinstance(m, dict):
            m = {}

        page_space = m.get('space', 'Unknown')
        page_url = m.get('url', '')
        # Breadcrumb –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
        breadcrumb = (r.get('breadcrumb') or
                     m.get('page_path') or
                     m.get('title') or
                     '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
        chunk_num = m.get('chunk', 0)

        # –¢–µ–∫—Å—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        text = r.get('expanded_text', r.get('text', "[–¢–µ–∫—Å—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω]"))
        text_preview = extract_relevant_snippet(text, query, max_length=800)

        # Score –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        final_score = r.get('boosted_score', r.get('rerank_score', r.get('final_score', 0)))
        score_emoji = "üî•" if final_score > 0.5 else "‚≠ê" if final_score > 0.3 else "‚úì" if final_score > 0.1 else "¬∑"
        score_str = f"{score_emoji} {final_score:.3f}"

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        extra_info = []
        labels = m.get('labels', '')
        if labels:
            extra_info.append(f"üè∑Ô∏è {labels}")
        created_by = m.get('created_by', '')
        if created_by:
            extra_info.append(f"üë§ {created_by}")

        extra_str = " | ".join(extra_info)
        if extra_str:
            extra_str = f" | {extra_str}"

        # –¢–∏–ø –ø–æ–∏—Å–∫–∞
        search_type = r.get('search_type', 'semantic')
        search_type_str = "üîç structural" if search_type == 'structural' else "üîé semantic"

        response.append(
            f"[{i}] {search_type_str} üìç {breadcrumb}\n"
            f"    üìÅ {page_space} | Chunk #{chunk_num} | {score_str}{extra_str}\n"
            f"    üîó {page_url}\n"
            f"    üí¨ {text_preview}\n"
        )

    return "\n".join(response)

def extract_relevant_snippet(text: str, query: str, max_length: int = 400) -> str:  # noqa: C901
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∑–∞–ø—Ä–æ—Å–∞.
    –£–º–µ–µ—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å —Å–ø–∏—Å–∫–∏, —Ç–∞–±–ª–∏—Ü—ã –∏ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –∏—Ö –ø–æ–ª–Ω–æ—Å—Ç—å—é.

    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞

    Returns:
        –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞
    """
    if len(text) <= max_length:
        return text

    import re

    # –ù–û–í–û–ï: –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    content_type = detect_content_type(text)

    # –¢–∞–±–ª–∏—Ü—ã –∏ —Å–ø–∏—Å–∫–∏ –ù–ï –æ–±—Ä–µ–∑–∞–µ–º - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é (–¥–æ —Ä–∞–∑—É–º–Ω–æ–≥–æ –ª–∏–º–∏—Ç–∞)
    if content_type in ['table', 'list']:
        limit = max_length * 6  # –î–æ 2400 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        if len(text) <= limit:
            return text
        else:
            # –û–±—Ä–µ–∑–∞–µ–º, –Ω–æ –ø—ã—Ç–∞–µ–º—Å—è —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            return text[:limit] + "\n... (–æ–±—Ä–µ–∑–∞–Ω–æ)"

    # –î–ª—è –∫–æ–¥–∞ - —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π –ª–∏–º–∏—Ç
    if content_type == 'code':
        limit = max_length * 3  # –î–æ 1200 —Å–∏–º–≤–æ–ª–æ–≤
        if len(text) <= limit:
            return text
        else:
            return text[:limit] + "\n... (–æ–±—Ä–µ–∑–∞–Ω–æ)"

    # –î–ª—è –æ–±—ã—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ª–æ–≥–∏–∫–∞
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–ø–æ —Ç–æ—á–∫–∞–º, –≤–æ–ø—Ä–æ—Å–∞–º, –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏—è–º)
    sentences = re.split(r'[.!?]+', text)
    sentences = [s.strip() for s in sentences if s.strip()]

    if not sentences:
        return text[:max_length] + "..."

    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
    query_words = set(extract_keywords(query))

    # –ù–∞—Ö–æ–¥–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º overlap
    best_idx = 0
    best_score = 0

    for idx, sent in enumerate(sentences):
        sent_words = set(extract_keywords(sent))
        overlap = len(query_words & sent_words)

        if overlap > best_score:
            best_score = overlap
            best_idx = idx

    # –ë–µ—Ä–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ + –∫–æ–Ω—Ç–µ–∫—Å—Ç (1 –¥–æ, 1 –ø–æ—Å–ª–µ)
    start = max(0, best_idx - 1)
    end = min(len(sentences), best_idx + 2)
    snippet = '. '.join(sentences[start:end]).strip()

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
    if len(snippet) > max_length:
        snippet = snippet[:max_length] + "..."

    return snippet

def deduplicate_results(results: list) -> list:
    """
    –£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞.

    Args:
        results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    Returns:
        –°–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    if len(results) <= 1:
        return results

    # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ hash –ø–µ—Ä–≤—ã—Ö 200 —Å–∏–º–≤–æ–ª–æ–≤
    seen_signatures = set()
    unique_results = []

    for r in results:
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∫ —Å–∏–≥–Ω–∞—Ç—É—Ä—É
        text = r.get('text', '')
        signature = text[:200].strip()
        text_hash = hash(signature)

        if text_hash not in seen_signatures:
            seen_signatures.add(text_hash)
            unique_results.append(r)
        else:
            logger.debug(f"–£–¥–∞–ª–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç: {r['metadata'].get('title', 'Unknown')}")

    return unique_results

def expand_context_window(result: dict, window_size: int = 1) -> dict:
    """
    –†–∞—Å—à–∏—Ä—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —á–∞–Ω–∫–∞ —Å–æ—Å–µ–¥–Ω–∏–º–∏ —á–∞–Ω–∫–∞–º–∏.

    Context Window Retrieval - –ø–æ–ø—É–ª—è—Ä–Ω–∞—è —Ç–µ—Ö–Ω–∏–∫–∞ –∏–∑ LangChain –∏ LlamaIndex.
    –ù–∞—Ö–æ–¥–∏–º –º–∞–ª–µ–Ω—å–∫–∏–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —á–∞–Ω–∫, –Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–æ–ª—å—à–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.

    Args:
        result: –ù–∞–π–¥–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        window_size: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –¥–æ/–ø–æ—Å–ª–µ (1 = ¬±1 —á–∞–Ω–∫)

    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
    """
    global qdrant_client

    if qdrant_client is None:
        return result

    try:
        if not result or not isinstance(result, dict):
            return result

        metadata = result.get('metadata')
        if not metadata or not isinstance(metadata, dict):
            return result

        chunk_num = metadata.get('chunk', 0)
        page_id = metadata.get('page_id')

        if not page_id:
            return result

        # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Å–æ—Å–µ–¥–Ω–∏–µ —á–∞–Ω–∫–∏ –∏–∑ —Ç–æ–π –∂–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        min_chunk = max(0, chunk_num - window_size)
        max_chunk = chunk_num + window_size

        # –ü–æ–ª—É—á–∞–µ–º —á–∞–Ω–∫–∏ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ
        from qdrant_storage import get_points_by_filter
        neighbors_raw = get_points_by_filter(
            where_filter={
                '$and': [
                    {'page_id': page_id},
                    {'chunk': {'$gte': min_chunk}},
                    {'chunk': {'$lte': max_chunk}}
                ]
            },
            limit=100,
            collection=QDRANT_COLLECTION
        )
        neighbors = {
            'documents': [r.get('text', '') for r in neighbors_raw],
            'metadatas': [r.get('metadata', {}) for r in neighbors_raw]
        }

        # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: neighbors –º–æ–∂–µ—Ç –±—ã—Ç—å None –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç—å None –ø–æ–ª—è
        if (neighbors and
            neighbors.get('documents') and
            neighbors.get('metadatas') and
            len(neighbors['documents']) == len(neighbors['metadatas'])):
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ chunk_num
            chunk_data = []
            for i, doc in enumerate(neighbors['documents']):
                if i < len(neighbors['metadatas']):
                    chunk_meta = neighbors['metadatas'][i]
                    if chunk_meta and isinstance(chunk_meta, dict):
                        chunk_data.append({
                            'chunk_num': chunk_meta.get('chunk', 0),
                            'text': doc if doc else ''
                        })

            chunk_data.sort(key=lambda x: x['chunk_num'])

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç—ã
            expanded_text = '\n\n'.join([c['text'] for c in chunk_data])
            result['expanded_text'] = expanded_text
            result['context_chunks'] = len(chunk_data)

            logger.debug(f"Context expanded: chunk {chunk_num} + {len(chunk_data)-1} neighbors")
        else:
            result['expanded_text'] = result.get('text', '')
            result['context_chunks'] = 1

    except Exception as e:
        logger.warning(f"Context expansion failed: {e}")

def calculate_hierarchy_boost(metadata: dict) -> float:
    """
    Hierarchy Boost - —Ç–µ—Ö–Ω–∏–∫–∞ –∏–∑ Elasticsearch –∏ Pinecone –¥–ª—è —É—á–µ—Ç–∞
    –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö –ø–æ–ª–æ–∂–µ–Ω–∏—è –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ.

    Args:
        metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞

    Returns:
        –ë—É—Å—Ç –æ—Ç -0.2 –¥–æ +0.8
    """
    boost = 0.0

    # 1. –ö–æ—Ä–Ω–µ–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –≤ space (–Ω–µ—Ç —Ä–æ–¥–∏—Ç–µ–ª—è) - —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ
    if not metadata.get('parent_title'):
        boost += 0.5
        logger.debug(f"Root page boost: +0.5 for {metadata.get('title')}")

    # 2. –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    title = metadata.get('title', '').lower()
    important_keywords = {
        '–æ–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è': 0.3,
        '–≥–ª–∞–≤–Ω–∞—è': 0.3,
        'readme': 0.3,
        'getting started': 0.3,
        '–Ω–∞—á–∞–ª–æ —Ä–∞–±–æ—Ç—ã': 0.3,
        '–æ–±–∑–æ—Ä': 0.2,
        '–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è': 0.2,
        '—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ': 0.2,
    }

    for keyword, value in important_keywords.items():
        if keyword in title:
            boost += value
            logger.debug(f"Title keyword boost: +{value} for '{keyword}'")
            break  # –¢–æ–ª—å–∫–æ –æ–¥–∏–Ω –±—É—Å—Ç –∑–∞ title

    # 3. –£—Ä–æ–≤–µ–Ω—å –∑–∞–≥–æ–ª–æ–≤–∫–∞ (h1 –≤–∞–∂–Ω–µ–µ h4)
    heading_level = metadata.get('heading_level', 0)
    if heading_level == 1:
        boost += 0.2
    elif heading_level == 2:
        boost += 0.1

    # 4. –ù–∞–ª–∏—á–∏–µ –º–µ—Ç–æ–∫ (labeled pages –æ–±—ã—á–Ω–æ –≤–∞–∂–Ω–µ–µ)
    labels_str = metadata.get('labels', '').lower()
    if labels_str:
        # –£–õ–£–ß–®–ï–ù–ò–ï: Metadata Boosting - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –±—É—Å—Ç –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç–æ–∫
        technical_labels = ['api', 'technical', '–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞', 'development',
                           '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', '–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è', 'configuration', '–Ω–∞—Å—Ç—Ä–æ–π–∫–∞']

        has_technical_label = any(label in labels_str for label in technical_labels)
        if has_technical_label:
            boost += 0.3  # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π –±—É—Å—Ç –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
            logger.debug(f"Technical label boost: +0.3 for labels '{labels_str}'")
        else:
            boost += 0.05  # –ë–∞–∑–æ–≤—ã–π –±—É—Å—Ç –∑–∞ –Ω–∞–ª–∏—á–∏–µ –º–µ—Ç–æ–∫

    return min(boost, 0.8)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º—É–º

def calculate_breadcrumb_match_score(query: str, breadcrumb: str) -> float:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å breadcrumb (–ø—É—Ç–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã).

    Path Matching - —Ç–µ—Ö–Ω–∏–∫–∞ –∏–∑ semantic search –¥–ª—è —É—á–µ—Ç–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
    –∑–∞–ø—Ä–æ—Å–∞ —Å –∏–µ—Ä–∞—Ä—Ö–∏–µ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞.

    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        breadcrumb: –ü—É—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã (Space > Parent > Page > Section)

    Returns:
        Score –æ—Ç 0.0 –¥–æ 1.0
    """
    if not breadcrumb:
        return 0.0

    query_words = set(extract_keywords(query))
    breadcrumb_words = set(extract_keywords(breadcrumb))

    if not query_words or not breadcrumb_words:
        return 0.0

    # Jaccard similarity
    intersection = len(query_words & breadcrumb_words)
    union = len(query_words | breadcrumb_words)

    score = intersection / union if union > 0 else 0.0

    if score > 0:
        logger.debug(f"Breadcrumb match: {score:.2f} ({intersection}/{union} words)")

    return score

# ========================================
# –°–¢–†–£–ö–¢–£–†–ù–´–ô –ü–û–ò–°–ö (Structural Navigation Search)
# ========================================

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
_metadata_cache = {}
_cache_timestamp = 0
_cache_ttl = 3600  # 1 —á–∞—Å

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
_structural_cache = {}
_structural_cache_timestamp = {}


def get_all_metadata_cached(ttl_seconds: int = 3600) -> Dict[str, Any]:
    """
    –ö—ç—à–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–ø—Ä–æ—Å–æ–≤.

    Args:
        ttl_seconds: –í—Ä–µ–º—è –∂–∏–∑–Ω–∏ –∫—ç—à–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    """
    global _metadata_cache, _cache_timestamp, _cache_ttl

    current_time = time.time()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
    if (_metadata_cache and
        current_time - _cache_timestamp < ttl_seconds):
        logger.debug(f"‚úÖ Metadata cache hit: {len(_metadata_cache.get('ids', []))} items")
        return _metadata_cache

    # –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à
    logger.info("üìä –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ metadata cache...")
    try:
        from qdrant_storage import get_all_points
        all_points = get_all_points(limit=10000, include_payload=True)
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ ChromaDB —Ñ–æ—Ä–º–∞—Ç
        all_data = {
            'ids': [p.get('id', '') for p in all_points.get('points', [])],
            'metadatas': [p.get('metadata', {}) for p in all_points.get('points', [])]
        }
        _metadata_cache = all_data
        _cache_timestamp = current_time
        _cache_ttl = ttl_seconds

        logger.info(f"‚úÖ Metadata cache updated: {len(all_data.get('ids', []))} items")
        return all_data
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è metadata cache: {e}")
        return _metadata_cache if _metadata_cache else {}

STRUCTURAL_SEPARATORS = ['>', '‚Üí', '‚Üí', ' / ', ' | ']
STRUCTURAL_PATTERNS = [
    (r'–ø–æ\s+–±–ª–æ–∫—É\s+(\w+)(?:\s*,\s*–∞\s+—Ç–æ—á–Ω–µ–µ\s+)?([^\.]+)?', True),
    (r'(\w+)\s*,\s*–∞\s+—Ç–æ—á–Ω–µ–µ\s+([^\.]+)', True),
    (r'–ø–æ\s+–±–ª–æ–∫—É\s+(\w+)', False),
    (r'–≤\s+—Ä–∞–∑–¥–µ–ª–µ\s+(\w+)', False),
    (r'–Ω–∞\s+—Å—Ç—Ä–∞–Ω–∏—Ü–µ\s+(\w+)', False),
]

def _parse_with_separators(query: str) -> list[str]:
    """–ü–∞—Ä—Å–∏–Ω–≥ –ø–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º."""
    for sep in STRUCTURAL_SEPARATORS:
        if sep in query:
            return [p.strip() for p in query.split(sep) if p.strip()]
    return []

def _parse_with_regex(query_lower: str) -> list[str]:
    """–ü–∞—Ä—Å–∏–Ω–≥ –ø–æ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–º –≤—ã—Ä–∞–∂–µ–Ω–∏—è–º."""
    for pattern, extract_multi in STRUCTURAL_PATTERNS:
        match = re.search(pattern, query_lower)
        if match:
            groups = [g.strip() for g in match.groups() if g and g.strip()]

            if extract_multi:
                 if len(groups) >= 2:
                     return groups
                 elif len(groups) == 1:
                     # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ "–∞ —Ç–æ—á–Ω–µ–µ" –µ—Å–ª–∏ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π —Ä–µ–≥—É–ª—è—Ä–∫–µ –Ω–µ –ø–æ–π–º–∞–ª–∏
                     after_match = re.search(r'–∞\s+—Ç–æ—á–Ω–µ–µ\s+([^\.]+)', query_lower)
                     if after_match:
                         return [groups[0], after_match.group(1).strip()]
                     return groups
            else:
                 return groups
    return []

def parse_query_structure(query: str) -> Dict[str, Any]:
    """
    –ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∑–∞–ø—Ä–æ—Å–∞.

    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–º (—Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏ >, >)
    –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —á–∞—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞.

    –ü—Ä–∏–º–µ—Ä—ã:
    - "–°–∫–ª–∞–¥ > –£—á–µ—Ç –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã" > structural
    - "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Å—Ç–µ–∫ RAUII" > –æ–±—ã—á–Ω—ã–π
    - "–û–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ > –°–∫–ª–∞–¥ > –£—á–µ—Ç –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã" > structural

    Args:
        query: –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –∑–∞–ø—Ä–æ—Å–∞
    """
    query_lower = query.lower().strip()

    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π
    parts = _parse_with_separators(query)
    is_structural = bool(parts)

    # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–≥—É–ª—è—Ä–æ–∫ –µ—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
    if not is_structural:
        regex_parts = _parse_with_regex(query_lower)
        if regex_parts:
            parts = regex_parts
            is_structural = True

    result = {
        'is_structural_query': is_structural,
        'parts': parts if parts else [query],
        'original_query': query,
        'query_lower': query_lower
    }

    logger.debug(f"üîç Query structure: is_structural={is_structural}, parts={result['parts']}")

    return result


def _calculate_structural_match(parts: list, metadata: dict) -> tuple[float, list]:
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–∫–æ—Ä —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ–ª–µ–π –∏ –∏—Ö –≤–µ—Å–æ–≤
    FIELD_WEIGHTS = [
        ('page_path', 3.0),
        ('title', 2.0),
        ('heading_path', 1.5),
        ('heading', 1.0),
    ]

    match_score = 0.0
    matches = []

    # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    fields = {
        field: (metadata.get(field, '') or '').lower()
        for field, _ in FIELD_WEIGHTS
    }

    for part_idx, part in enumerate(parts):
        part_lower = part.lower()
        position_weight = len(parts) - part_idx
        matched = False

        for field_name, base_weight in FIELD_WEIGHTS:
            if part_lower in fields[field_name]:
                match_score += base_weight + position_weight
                matches.append({
                    'part': part,
                    'field': field_name,
                    'weight': position_weight
                })
                matched = True
                break

        if not matched:
            return 0.0, []  # –¢—Ä–µ–±—É–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤—Å–µ—Ö —á–∞—Å—Ç–µ–π

    return match_score, matches

def structural_metadata_search(
    collection: Any,
    structure: Dict[str, Any],
    limit: int = 10
) -> List[Dict[str, Any]]:
    """
    –ü–æ–∏—Å–∫ –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∑–∞–ø—Ä–æ—Å–∞.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç in-memory —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é (ChromaDB –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç $contains –Ω–∞–ø—Ä—è–º—É—é).
    –ò—â–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤:
    - page_path (–ø–æ–ª–Ω—ã–π –ø—É—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã)
    - title (–∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã)
    - heading_path (–ø—É—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤)
    - heading (–∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ä–∞–∑–¥–µ–ª–∞)
    - parent_path (–ø—É—Ç—å —Ä–æ–¥–∏—Ç–µ–ª—è)

    Args:
        collection: ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏—è
        structure: –†–µ–∑—É–ª—å—Ç–∞—Ç parse_query_structure()
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    Returns:
        –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å match_score
    """
    if not structure['is_structural_query']:
        return []

    parts = structure['parts']

    # ============ –£–õ–£–ß–®–ï–ù–ò–ï: –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ ============
    search_start = time.time()
    logger.info(f"üîç –ù–∞—á–∞–ª–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: parts={parts}, limit={limit}")

    try:
        # –ü–æ–ª—É—á–∞–µ–º –í–°–ï —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (—Å —Ä–∞–∑—É–º–Ω—ã–º –ª–∏–º–∏—Ç–æ–º)
        max_scan = min(limit * 10, 10000)  # –ù–µ –±–æ–ª–µ–µ 10K –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

        fetch_start = time.time()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        query_key = structure.get('original_query', '')
        current_time = time.time()

        if (query_key in _structural_cache and
            current_time - _structural_cache_timestamp.get(query_key, 0) < 3600):
            logger.debug(f"‚úÖ Structural cache hit for '{query_key}'")
            return _structural_cache[query_key]

        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
        all_data = get_all_metadata_cached()

        if not all_data or not all_data.get('metadatas'):
            return []

        parts = structure.get('parts', [])
        if not parts:
            return []

        logger.info(f"üèóÔ∏è Structural search for parts: {parts}")

        formatted_results = []
        matched_count = 0

        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
        for idx, metadata in enumerate(all_data['metadatas']):
            if matched_count >= limit:
                break

            if not metadata:
                continue

            match_score, matches = _calculate_structural_match(parts, metadata)

            if match_score > 0:
                matched_count += 1
                formatted_results.append({
                    'metadata': metadata,
                    'match_score': match_score,
                    'matches': matches,
                    'text': ''
                })

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ score
        formatted_results.sort(key=lambda x: x['match_score'], reverse=True)

        total_time = time.time() - search_start
        fetch_time = time.time() - fetch_start
        filter_time = total_time - fetch_time
        checked_count = len(all_data.get('metadatas', []))

        logger.info(
            f"‚úÖ Structural search finished: {len(formatted_results)} results "
            f"(–ø—Ä–æ–≤–µ—Ä–µ–Ω–æ {checked_count}, —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π {matched_count}) "
            f"–∑–∞ {total_time:.3f}—Å "
            f"(fetch: {fetch_time:.3f}—Å, filter: {filter_time:.3f}—Å)"
        )

        if formatted_results and logger.isEnabledFor(logging.DEBUG):
            # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ø-3 –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            for i, r in enumerate(formatted_results[:3], 1):
                logger.debug(
                    f"  [{i}] match_score={r['match_score']}, "
                    f"page_id={r['metadata'].get('page_id')}, "
                    f"title={r['metadata'].get('title', '')[:50]}, "
                    f"matches={len(r.get('matches', []))}"
                )

        return formatted_results[:limit]

    except Exception as e:
        total_time = time.time() - search_start
        logger.error(f"–û—à–∏–±–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (–∑–∞ {total_time:.3f}—Å): {e}", exc_info=True)
        return []

def cached_structural_search(
    collection: Any,
    structure: Dict[str, Any],
    limit: int = 100,
    ttl_seconds: int = 300  # 5 –º–∏–Ω—É—Ç
) -> List[Dict[str, Any]]:
    """
    –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞.

    Args:
        collection: ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏—è
        structure: –†–µ–∑—É–ª—å—Ç–∞—Ç parse_query_structure()
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        ttl_seconds: –í—Ä–µ–º—è –∂–∏–∑–Ω–∏ –∫—ç—à–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö

    Returns:
        –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
    """
    cache_key = tuple(sorted(structure['parts']))
    current_time = time.time()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
    if (cache_key in _structural_cache and
        cache_key in _structural_cache_timestamp and
        current_time - _structural_cache_timestamp[cache_key] < ttl_seconds):

        cache_age = current_time - _structural_cache_timestamp[cache_key]
        logger.debug(
            f"‚úÖ Structural cache hit: {cache_key} "
            f"(age: {cache_age:.1f}—Å, results: {len(_structural_cache[cache_key])})"
        )
        return _structural_cache[cache_key]

    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
    search_start = time.time()
    results = structural_metadata_search(collection, structure, limit)
    search_time = time.time() - search_start

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
    _structural_cache[cache_key] = results
    _structural_cache_timestamp[cache_key] = current_time

    logger.info(
        f"üìù Structural cache updated: {cache_key} ‚Üí {len(results)} results "
        f"(search time: {search_time:.3f}—Å)"
    )

    return results

def _find_best_keyword_match(text: str, keywords: list) -> tuple[str, float]:
    """–ù–∞–π—Ç–∏ –ª—É—á—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ."""
    if not text:
        return "", 0.0

    text_lower = text.lower()
    for keyword in keywords:
        if len(keyword) > 3 and keyword in text_lower:
            score = len(keyword) / len(text_lower)
            return keyword, score
    return "", 0.0

def analyze_query_with_metadata(
    query: str
) -> Dict[str, Any]:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –∏ –Ω–∞—Ö–æ–¥–∏—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.

    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º–∏ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    """
    keywords = extract_keywords(query)

    # –ü–æ–ª—É—á–∞–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    all_data = get_all_metadata_cached()

    if not all_data or not all_data.get('metadatas'):
        return {'page_title_matches': [], 'heading_path_matches': [], 'page_path_matches': []}

    matches = {
        'page_title_matches': [],
        'heading_path_matches': [],
        'page_path_matches': []
    }
    seen_pages = set()

    for idx, metadata in enumerate(all_data['metadatas']):
        if not metadata:
            continue

        page_id = metadata.get('page_id')
        if not page_id:
            continue

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ title (—Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã)
        if page_id not in seen_pages:
            title = metadata.get('title', '')
            kw, score = _find_best_keyword_match(title, keywords)
            if score > 0:
                matches['page_title_matches'].append({
                    'page_id': page_id,
                    'page_title': title,
                    'page_path': metadata.get('page_path', ''),
                    'match_keyword': kw,
                    'match_score': score
                })
                seen_pages.add(page_id)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ page_path
        page_path = metadata.get('page_path', '')
        kw, score = _find_best_keyword_match(page_path, keywords)
        if score > 0:
            matches['page_path_matches'].append({
                'page_id': page_id,
                'page_path': page_path,
                'match_keyword': kw,
                'match_score': score
            })

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ heading_path
        heading_path = metadata.get('heading_path', '')
        kw, score = _find_best_keyword_match(heading_path, keywords)
        if score > 0:
            matches['heading_path_matches'].append({
                'page_id': page_id,
                'heading_path': heading_path,
                'match_keyword': kw,
                'match_score': score
            })

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ match_score –∏ –æ–±—Ä–µ–∑–∞–µ–º
    for key in matches:
        matches[key].sort(key=lambda x: x['match_score'], reverse=True)
        matches[key] = matches[key][:10]

    return matches

def apply_metadata_boost(
    results: List[Dict[str, Any]],
    metadata_analysis: Dict[str, Any]
) -> List[Dict[str, Any]]:
    """
    –ü—Ä–∏–º–µ–Ω–∏—Ç—å boost —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π (–Ω–µ –±–æ–ª–µ–µ 30% –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ score).

    Args:
        results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        metadata_analysis: –†–µ–∑—É–ª—å—Ç–∞—Ç analyze_query_with_metadata()

    Returns:
        –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã–º boost
    """
    if not results:
        return results

    for result in results:
        result['metadata_boost'] = 0.0
        page_id = result.get('metadata', {}).get('page_id')

        if not page_id:
            continue

        # Boost –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ page_title
        for match in metadata_analysis.get('page_title_matches', [])[:3]:
            if match['page_id'] == page_id:
                # Boost –Ω–µ –±–æ–ª–µ–µ 30% –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ score
                current_score = result.get('rerank_score', 0)
                boost = current_score * 0.3 * match['match_score']
                result['metadata_boost'] += boost
                logger.debug(f"Page title boost: +{boost:.3f} for {page_id}")
                break

        # Boost –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ heading_path
        heading_path = result.get('metadata', {}).get('heading_path', '')
        if heading_path:
            for match in metadata_analysis.get('heading_path_matches', [])[:3]:
                if (match['page_id'] == page_id and
                    match['heading_path'].lower() in heading_path.lower()):
                    current_score = result.get('rerank_score', 0)
                    boost = current_score * 0.2 * match['match_score']
                    result['metadata_boost'] += boost
                    logger.debug(f"Heading path boost: +{boost:.3f} for {page_id}")
                    break

        # –û–±–Ω–æ–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π score
        result['boosted_score'] = result.get('rerank_score', 0) + result['metadata_boost']

    return results

def get_diversity_limit_for_intent(intent_type: str = None) -> int:
    """
    –ü–æ–ª—É—á–∏—Ç—å –ª–∏–º–∏—Ç diversity filter –¥–ª—è —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞.

    Args:
        intent_type: –¢–∏–ø –∑–∞–ø—Ä–æ—Å–∞ ('navigational', 'exploratory', 'factual', 'howto')

    Returns:
        –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ chunks —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∫–ª—é—á—ë–Ω –ª–∏ —Ñ–∏–ª—å—Ç—Ä
    enable_filter = os.getenv('ENABLE_DIVERSITY_FILTER', 'true').lower() == 'true'
    if not enable_filter:
        return 999  # –û—á–µ–Ω—å –±–æ–ª—å—à–æ–π –ª–∏–º–∏—Ç (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ—Ç —Ñ–∏–ª—å—Ç—Ä)

    # –ü–æ–ª—É—á–∞–µ–º –ª–∏–º–∏—Ç—ã –∏–∑ ENV –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç—ã
    diversity_limits = {
        'navigational': int(os.getenv('DIVERSITY_LIMIT_NAVIGATIONAL', '1')),
        'exploratory': int(os.getenv('DIVERSITY_LIMIT_EXPLORATORY', '4')),
        'factual': int(os.getenv('DIVERSITY_LIMIT_FACTUAL', '2')),
        'howto': int(os.getenv('DIVERSITY_LIMIT_HOWTO', '3')),
    }

    # –ï—Å–ª–∏ —Ç–∏–ø –Ω–µ —É–∫–∞–∑–∞–Ω –∏–ª–∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç –¥–ª—è factual
    if not intent_type or intent_type not in diversity_limits:
        intent_type = 'factual'

    return diversity_limits.get(intent_type, 2)

def _resolve_diversity_limit(max_per_page, query, intent) -> int:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ª–∏–º–∏—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ç–µ–Ω—Ç–∞."""
    if max_per_page is not None:
        return max_per_page

    intent_type = None
    if intent and isinstance(intent, dict):
        intent_type = intent.get('type')
    elif query:
        intent_dict = classify_query_intent(query)
        intent_type = intent_dict.get('type') if intent_dict else None

    limit = get_diversity_limit_for_intent(intent_type)
    logger.debug(f"Diversity filter: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ª–∏–º–∏—Ç {limit} –¥–ª—è intent={intent_type}")
    return limit

def apply_diversity_filter(results: list, limit: int = 5, max_per_page: int = None, query: str = None, intent: dict = None) -> list:
    """
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç diversity filter: –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ chunks —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã.

    –≠—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ (Google, Perplexity): –º–∞–∫—Å–∏–º—É–º 2-3 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    —Å –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –≤ —Ç–æ–ø-5, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –º–µ—Å—Ç–∞ - –¥—Ä—É–≥–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏.

    –ù–û–í–û–ï: –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ª–∏–º–∏—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ query intent —á–µ—Ä–µ–∑ ENV –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ.

    Args:
        results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ score
        limit: –°–∫–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–µ—Ä–Ω—É—Ç—å
        max_per_page: –ú–∞–∫—Å–∏–º—É–º chunks —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–µ—Å–ª–∏ None, –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –ø–æ intent)
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è intent, –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        intent: –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ç–∏–ø–µ –∑–∞–ø—Ä–æ—Å–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

    Returns:
        –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    """
    if not results:
        return []

    limit_per_page = _resolve_diversity_limit(max_per_page, query, intent)

    filtered_results = []
    page_counts = {}

    for result in results:
        if not result or not isinstance(result, dict):
            continue

        metadata = result.get('metadata')
        if not metadata or not isinstance(metadata, dict):
            continue

        page_id = metadata.get('page_id')

        # –ï—Å–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–µ—Ç –∏–ª–∏ –ª–∏–º–∏—Ç –Ω–µ –ø—Ä–µ–≤—ã—à–µ–Ω - –¥–æ–±–∞–≤–ª—è–µ–º
        if not page_id or page_counts.get(page_id, 0) < limit_per_page:
            filtered_results.append(result)
            if page_id:
                page_counts[page_id] = page_counts.get(page_id, 0) + 1

            # –î–æ—Å—Ç–∏–≥–ª–∏ –Ω—É–∂–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if len(filtered_results) >= limit:
                break

    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    if page_counts:
        logger.debug(f"Diversity filter: {len(filtered_results)} results from {len(page_counts)} unique pages (max {limit_per_page}/page)")
        for page_id, count in page_counts.items():
            if count > 1:
                logger.debug(f"  Page {page_id}: {count} chunks")

    return filtered_results

def enrich_result_with_context(result: dict) -> dict:
    """
    –û–±–æ–≥–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏–∑ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤.

    Args:
        result: –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞

    Returns:
        –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å breadcrumb
    """
    if not result or not isinstance(result, dict):
        return result

    metadata = result.get('metadata')
    if not metadata or not isinstance(metadata, dict):
        # –ï—Å–ª–∏ metadata –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å
        metadata = {}
        result['metadata'] = metadata

    # –§–æ—Ä–º–∏—Ä—É–µ–º breadcrumb (—Ö–ª–µ–±–Ω—ã–µ –∫—Ä–æ—à–∫–∏)
    # –ù–û–í–û–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º page_path –µ—Å–ª–∏ –µ—Å—Ç—å (–ø–æ–ª–Ω—ã–π –ø—É—Ç—å)
    page_path = metadata.get('page_path', '')
    if page_path:
        # page_path —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–ª–Ω—ã–π –ø—É—Ç—å: "Parent 1 > Parent 2 > Page"
        breadcrumb_parts = page_path.split(' > ')
    else:
        # Fallback: —Ñ–æ—Ä–º–∏—Ä—É–µ–º –∏–∑ parent_title (—Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥)
        breadcrumb_parts = []
        if metadata.get('parent_title'):
            breadcrumb_parts.append(metadata['parent_title'])
        if metadata.get('title') and metadata.get('title') != metadata.get('parent_title'):
            breadcrumb_parts.append(metadata['title'])

    # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å–µ–∫—Ü–∏–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
    if metadata.get('heading'):
        breadcrumb_parts.append(metadata['heading'])

    result['breadcrumb'] = ' ‚Üí '.join(breadcrumb_parts) if breadcrumb_parts else metadata.get('title', 'Unknown')

    return result

def classify_query_intent(query: str) -> dict:
    """
    –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –Ω–∞–º–µ—Ä–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–∏—Å–∫–∞.

    Returns:
        {
            'type': 'factual' | 'navigational' | 'howto' | 'exploratory',
            'boost_hierarchy': bool,  # –£—Å–∏–ª–∏—Ç—å –±—É—Å—Ç –∏–µ—Ä–∞—Ä—Ö–∏–∏
            'expand_context': bool,   # –†–∞—Å—à–∏—Ä–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
            'diversity': int          # –õ–∏–º–∏—Ç —á–∞–Ω–∫–æ–≤ —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        }
    """
    import re

    query_lower = query.lower()

    # 1. –ù–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã: "–≥–¥–µ", "–Ω–∞–π–¥–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—É", "–ø–æ–∫–∞–∂–∏"
    if re.search(r'\b(–≥–¥–µ|–Ω–∞–π–¥–∏|–ø–æ–∫–∞–∂–∏|—Å—Ç—Ä–∞–Ω–∏—Ü–∞|–¥–æ–∫—É–º–µ–Ω—Ç)\b', query_lower):
        return {
            'type': 'navigational',
            'boost_hierarchy': True,   # –í–∞–∂–Ω—ã –∫–æ—Ä–Ω–µ–≤—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            'expand_context': False,   # –ù–µ –Ω—É–∂–µ–Ω –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            'diversity': 1             # –ü–æ 1 —á–∞–Ω–∫—É —Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–ø–æ–∫–∞–∑–∞—Ç—å –±–æ–ª—å—à–µ —Å—Ç—Ä–∞–Ω–∏—Ü)
        }

    # 2. How-to –∑–∞–ø—Ä–æ—Å—ã: "–∫–∞–∫", "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è", "–Ω–∞—Å—Ç—Ä–æ–∏—Ç—å"
    if re.search(r'\b(–∫–∞–∫|–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è|–Ω–∞—Å—Ç—Ä–æ–∏—Ç—å|—É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å|–∑–∞–ø—É—Å—Ç–∏—Ç—å|—Å–¥–µ–ª–∞—Ç—å)\b', query_lower):
        return {
            'type': 'howto',
            'boost_hierarchy': False,  # –ù–µ –≤–∞–∂–Ω–∞ –∏–µ—Ä–∞—Ä—Ö–∏—è
            'expand_context': True,    # –ù—É–∂–µ–Ω –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            'diversity': 3             # –î–æ 3 —á–∞–Ω–∫–æ–≤ —Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–¥–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è)
        }

    # 3. –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã: "–∫–∞–∫–æ–π", "—á—Ç–æ", "–∫–æ–≥–¥–∞", "–∫—Ç–æ"
    if re.search(r'\b(–∫–∞–∫–æ–π|–∫–∞–∫–∞—è|–∫–∞–∫–∏–µ|—á—Ç–æ|–∫–æ–≥–¥–∞|–∫—Ç–æ|—Å–∫–æ–ª—å–∫–æ)\b', query_lower):
        return {
            'type': 'factual',
            'boost_hierarchy': False,  # –ù–µ –≤–∞–∂–Ω–∞ –∏–µ—Ä–∞—Ä—Ö–∏—è
            'expand_context': True,    # –ù—É–∂–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞
            'diversity': 3             # –î–æ 3 —á–∞–Ω–∫–æ–≤ (–º–æ–∂–µ—Ç –±—ã—Ç—å –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö)
        }

    # 4. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    return {
        'type': 'exploratory',
        'boost_hierarchy': False,
        'expand_context': True,
        'diversity': 2  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ª–∏–º–∏—Ç
    }

def init_rag() -> QdrantClient:
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã: Qdrant.

    Returns:
        QdrantClient

    Raises:
        Exception: –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    """
    try:
        logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Qdrant: {QDRANT_HOST}:{QDRANT_PORT}, collection={QDRANT_COLLECTION}")

        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ qdrant_storage
        from qdrant_storage import init_qdrant_client, init_qdrant_collection

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∏–µ–Ω—Ç
        client = init_qdrant_client()

        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å embeddings
        embedding_dim = get_embedding_dimension()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é (—Å–æ–∑–¥–∞–µ—Ç –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
        success = init_qdrant_collection(embedding_dim)
        if not success:
            raise ValueError(f"Failed to initialize Qdrant collection: {QDRANT_COLLECTION}")

        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        from qdrant_storage import get_qdrant_count
        doc_count = get_qdrant_count()

        logger.info(f"‚úÖ RAG —Å–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞. –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {doc_count}, –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {embedding_dim}D")
        return client

    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ RAG: {e}")
        sys.exit(1)

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è RAG –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
qdrant_client = None

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –º–æ–¥—É–ª—è (–¥–æ —Å–æ–∑–¥–∞–Ω–∏—è MCP —Å–µ—Ä–≤–µ—Ä–∞)
# –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ MCP —Å–µ—Ä–≤–µ—Ä–∞...")
qdrant_client = init_rag()
from qdrant_storage import get_qdrant_count
doc_count = get_qdrant_count()
logger.info(f"RAG —Å–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞. –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {doc_count}")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è BM25 retriever –¥–ª—è Hybrid Search (–ª–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è)
logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è BM25 retriever –¥–ª—è Hybrid Search...")
# BM25 —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–ø—Ä—è–º—É—é —Å Qdrant —á–µ—Ä–µ–∑ qdrant_storage
init_bm25_retriever(QDRANT_COLLECTION)

# –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ reranker –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ (—á—Ç–æ–±—ã –ø–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å –±—ã–ª –±—ã—Å—Ç—Ä–µ–µ)
logger.info("–ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ reranker –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ...")
try:
    init_reranker()
    logger.info("‚úÖ Reranker –º–æ–¥–µ–ª—å –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∏—Ç—å reranker –º–æ–¥–µ–ª—å: {e}. –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∑–∏—Ç—Å—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ.")

# Initialize SearchPipeline
from search_pipeline import SearchPipeline, SearchParams
search_pipeline = SearchPipeline(qdrant_client, QDRANT_COLLECTION, reranker)
logger.info("‚úÖ SearchPipeline initialized")

mcp = FastMCP("Confluence RAG")

def _extract_space_from_query(query: str, current_space: str) -> tuple[str, str]:
    """–ò–∑–≤–ª–µ—á—å –Ω–∞–∑–≤–∞–Ω–∏–µ space –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞."""
    if current_space:
        return query, current_space

    space_patterns = [
        r'\bspaces?\s+([A-Za-z0-9_-]+)\s*$',
        r'\bspaces?\s+([A-Za-z0-9_-]+)(?:\s|$)',
        r'\b–≤\s+–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ\s+([A-Za-z0-9_-]+)\s*$',
        r'\b–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ\s+([A-Za-z0-9_-]+)\s*$',
    ]

    for pattern in space_patterns:
        match = re.search(pattern, query, re.IGNORECASE)
        if match:
            space = match.group(1).strip()
            new_query = re.sub(pattern, '', query, flags=re.IGNORECASE).strip()
            logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω space –∏–∑ –∑–∞–ø—Ä–æ—Å–∞: '{space}'")
            return new_query, space

    return query, ""

def _validate_search_params(query: str, space: str, limit: int) -> tuple[bool, str]:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–∏—Å–∫–∞."""
    if not query or not isinstance(query, str):
        return False, "‚ùå –û—à–∏–±–∫–∞: –ü—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å"

    if len(query.strip()) < 2:
        return False, "‚ùå –û—à–∏–±–∫–∞: –ó–∞–ø—Ä–æ—Å —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π (–º–∏–Ω–∏–º—É–º 2 —Å–∏–º–≤–æ–ª–∞)"

    if space and not re.match(r'^[a-zA-Z0-9_-]+$', space.strip()):
        return False, "‚ùå –û—à–∏–±–∫–∞: –ü–∞—Ä–∞–º–µ—Ç—Ä space —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Å–∏–º–≤–æ–ª—ã"

    if qdrant_client is None:
        return False, "‚ùå –û—à–∏–±–∫–∞: RAG —Å–∏—Å—Ç–µ–º–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ —Å–µ—Ä–≤–µ—Ä–∞."

    return True, ""

@mcp.tool()
def confluence_semantic_search(query: str, limit: int = 5, space: str = "") -> str:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π Confluence.

    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5)
        space: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä –ø–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤—É (space key)

    Returns:
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ–∏—Å–∫–∞
    """
    try:
        # 1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ space
        query, space = _extract_space_from_query(query, space)
        query = query.strip()

        # 2. –í–∞–ª–∏–¥–∞—Ü–∏—è
        is_valid, error_msg = _validate_search_params(query, space, limit)
        if not is_valid:
            return error_msg

        if len(query) > 1000:
            logger.warning(f"–û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å ({len(query)} —Å–∏–º–≤–æ–ª–æ–≤), –æ–±—Ä–µ–∑–∞—é –¥–æ 1000")
            query = query[:1000]

        limit = min(max(limit, 1), 20)
        if space:
            space = space.strip()

        # 3. Structural Navigation Search
        structure = parse_query_structure(query)
        if structure['is_structural_query']:
            logger.info(f"üîç –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –∑–∞–ø—Ä–æ—Å –æ–±–Ω–∞—Ä—É–∂–µ–Ω: {structure['parts']}")
            structural_results = cached_structural_search(
                QDRANT_COLLECTION, structure, limit=limit * 10
            )
            if structural_results and len(structural_results) >= limit:
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª–µ–≥–∫–∏–π reranking
                for result in structural_results:
                    max_match = max([r['match_score'] for r in structural_results]) if structural_results else 1
                    result['rerank_score'] = (result['match_score'] / max_match) * 0.5 if max_match > 0 else 0.1
                    result['distance'] = 1.0 - result['rerank_score']

                metadata_analysis = analyze_query_with_metadata(query)
                structural_results = apply_metadata_boost(structural_results, metadata_analysis)
                structural_results.sort(key=lambda x: x.get('boosted_score', x.get('rerank_score', 0)), reverse=True)
                return format_search_results(structural_results[:limit], query, limit)

        # 4. Standard Semantic Search Pipeline
        expanded_queries = expand_query(query, space)
        params = SearchParams(
            query=query,
            space=space if space else None,
            limit=limit,
            use_reranking=True,
            expanded_queries=expanded_queries[1:] if len(expanded_queries) > 1 else []
        )

        results = search_pipeline.execute(params)

        if not results:
            return f"‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'"

        return format_search_results(results, query, limit)

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}", exc_info=True)
        return f"‚ùå –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {str(e)}"

@mcp.tool()
def confluence_list_spaces() -> str:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤ Confluence.

    Returns:
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤ —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    """

    try:
        if qdrant_client is None:
            return "‚ùå –û—à–∏–±–∫–∞: RAG —Å–∏—Å—Ç–µ–º–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞."

        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑—É–º–Ω—ã–π –ª–∏–º–∏—Ç –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è OOM
        MAX_SCAN_LIMIT = 10000
        from qdrant_storage import get_all_points
        all_points = get_all_points(limit=MAX_SCAN_LIMIT, include_payload=True)
        all_data = {
            'metadatas': [p.get('metadata', {}) for p in all_points.get('points', [])]
        }
        spaces = {}

        for metadata in all_data.get('metadatas', []):
            space_name = metadata.get('space', 'Unknown')
            if space_name:
                spaces[space_name] = spaces.get(space_name, 0) + 1

        if not spaces:
            return "‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ."

        result = "üìö –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ Confluence:\n\n"
        for space_name, count in sorted(spaces.items(), key=lambda x: x[1], reverse=True):
            result += f"  ‚Ä¢ **{space_name}**: {count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n"

        scanned_count = len(all_data.get('metadatas', []))
        if scanned_count >= MAX_SCAN_LIMIT:
            result += f"\n‚ö†Ô∏è –ü–æ–∫–∞–∑–∞–Ω—ã –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –∏–∑ –ø–µ—Ä–≤—ã—Ö {MAX_SCAN_LIMIT} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."

        return result
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤: {e}")
        return f"‚ùå –û—à–∏–±–∫–∞: {e}"

@mcp.tool()
def confluence_health() -> str:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ RAG —Å–∏—Å—Ç–µ–º—ã –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É.

    Returns:
        –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ —Å–∏—Å—Ç–µ–º—ã –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    """

    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ RAG –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ —Å–µ—Ä–≤–µ—Ä–∞
        if qdrant_client is None:
            return "‚ùå –û—à–∏–±–∫–∞: RAG —Å–∏—Å—Ç–µ–º–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ —Å–µ—Ä–≤–µ—Ä–∞."

        # –ü–æ–¥—Å—á—ë—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É–µ–º count() –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏)
        try:
            from qdrant_storage import get_qdrant_count
            total_docs = get_qdrant_count()
        except Exception:
            # Fallback: –æ—Ü–µ–Ω–∫–∞ —á–µ—Ä–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫—É
            from qdrant_storage import get_all_points
            all_points = get_all_points(limit=10, include_payload=True)
            data = {'points': all_points.get('points', [])}
            total_docs = len(data.get("ids", []))
            if total_docs == 10:
                total_docs = "10+"  # –ë–æ–ª—å—à–µ 10, —Ç–æ—á–Ω–æ–µ —á–∏—Å–ª–æ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ

        status = "‚úÖ –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç"
        if total_docs == 0:
            status = "‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å –ø—É—Å—Ç - –æ–∂–∏–¥–∞–Ω–∏–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏"

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Query Rewriting
        rewrite_stats = get_rewriter_stats()
        rewrite_info = (
            f"üìù Query Rewriting: {rewrite_stats['total_requests']} –∑–∞–ø—Ä–æ—Å–æ–≤, "
            f"–∫—ç—à: {rewrite_stats['cache_hit_rate']} ({rewrite_stats['cache_hits']}/{rewrite_stats['total_requests']})"
        )

        mode_str = 'Ollama' if USE_OLLAMA else 'HuggingFace'

        return (
            f"{status}\n"
            f"üìä –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ: {total_docs}\n"
            f"üîß –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {EMBED_MODEL}\n"
            f"üíæ Qdrant: {QDRANT_HOST}:{QDRANT_PORT} (Collection: {QDRANT_COLLECTION})\n"
            f"üîÑ –†–µ–∂–∏–º: {mode_str}\n"
            f"{rewrite_info}"
        )
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ health check: {e}", exc_info=True)
        return f"‚ùå –û—à–∏–±–∫–∞: {str(e)}"

if __name__ == "__main__":
    # Setup observability
    setup_observability("confluence-rag")

    # Initialize reranker in background
    init_reranker()

    # Start MCP server
    logger.info("MCP on 0.0.0.0:8012")
    try:
        mcp.run(transport="streamable-http", port=8012, host="0.0.0.0")
    except KeyboardInterrupt:
        pass
