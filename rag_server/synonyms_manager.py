#!/usr/bin/env python3
"""
–ú–µ–Ω–µ–¥–∂–µ—Ä —Å–∏–Ω–æ–Ω–∏–º–æ–≤ —Å —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ–º.

–ò—Å—Ç–æ—á–Ω–∏–∫–∏ —Å–∏–Ω–æ–Ω–∏–º–æ–≤:
1. –ë–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å (50 –æ–±—â–∏—Ö IT-—Ç–µ—Ä–º–∏–Ω–æ–≤)
2. –î–æ–º–µ–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑ Confluence)
3. –í—ã—É—á–µ–Ω–Ω—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã (Query Mining)
4. Ollama (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
"""

import json
import re
import logging
import time
from pathlib import Path

logger = logging.getLogger(__name__)

# –ë–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å (50 –æ–±—â–∏—Ö IT-—Ç–µ—Ä–º–∏–Ω–æ–≤)
# Blacklist: —Ç–µ—Ä–º–∏–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –ù–ï —Å–ª–µ–¥—É–µ—Ç –∑–∞–º–µ–Ω—è—Ç—å (—Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–º–µ–Ω–∞, –Ω–∞–∑–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤)
TERM_BLACKLIST = {
    'syntaxcheck', 'codesearch', 'docsearch', 'metadatasearch', 'templatesearch',
    'ollama', 'openrouter', 'litellm', 'confluence', 'jira', 'bitbucket',
    'github', 'gitlab', 'docker', 'kubernetes', 'postgres', 'mysql', 'redis',
    'mcp', 'rag', 'llm', 'gpt', 'claude', 'chatgpt',
    'rauii', 'map', 'md', 'mdo', 'mi'  # –ù–∞–∑–≤–∞–Ω–∏—è –≤–∞—à–∏—Ö spaces
}

BASE_SYNONYMS = {
    # === –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ ===
    '—Å—Ç–µ–∫': ['—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', '–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã', 'frameworks', 'tech stack', 'tools'],
    '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π': ['—Å—Ç–µ–∫', '–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤', 'tools', 'tech stack'],
    'framework': ['—Ñ—Ä–µ–π–º–≤–æ—Ä–∫', '–±–∏–±–ª–∏–æ—Ç–µ–∫–∞', 'library', '—Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏'],

    # === –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ ===
    '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞': ['development', 'dev', 'coding', '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ'],
    '–±–∞–≥': ['bug', '–æ—à–∏–±–∫–∞', 'error', '–¥–µ—Ñ–µ–∫—Ç', 'issue'],
    '—Ç–µ—Å—Ç': ['test', 'testing', '–ø—Ä–æ–≤–µ—Ä–∫–∞', '—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ'],

    # === –ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ ===
    '—Å–µ—Ä–≤–µ—Ä': ['server', 'backend', '–±—ç–∫–µ–Ω–¥', '—Ö–æ—Å—Ç', 'host'],
    '–±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö': ['–ë–î', 'database', 'DB', '—Ö—Ä–∞–Ω–∏–ª–∏—â–µ', 'storage'],
    '–±–¥': ['–±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö', 'database', 'DB', '—Ö—Ä–∞–Ω–∏–ª–∏—â–µ'],
    '–∫–æ–Ω—Ç–µ–π–Ω–µ—Ä': ['container', '–¥–æ–∫–µ—Ä'],

    # === API ===
    'api': ['–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å', 'endpoint', '–º–µ—Ç–æ–¥', '–≤–µ–±-—Å–µ—Ä–≤–∏—Å', 'rest'],
    'endpoint': ['api', '–º–µ—Ç–æ–¥', '—Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞', 'route', '—ç–Ω–¥–ø–æ–∏–Ω—Ç'],
    'rest': ['api', 'restful', '–≤–µ–±-—Å–µ—Ä–≤–∏—Å'],

    # === Confluence ===
    '—Å—Ç—Ä–∞–Ω–∏—Ü–∞': ['page', '–¥–æ–∫—É–º–µ–Ω—Ç', 'doc', '—Å—Ç—Ä–∞–Ω–∏—á–∫–∞'],
    '–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ': ['space', '—Å–ø–µ–π—Å', '–æ–±–ª–∞—Å—Ç—å'],
    '–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è': ['docs', 'documentation', '—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ', '–º–∞–Ω—É–∞–ª'],

    # === –û–±—â–∏–µ IT-—Ç–µ—Ä–º–∏–Ω—ã ===
    '–Ω–∞—Å—Ç—Ä–æ–π–∫–∞': ['–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è', 'config', 'configuration', 'setup'],
    '—É—Å—Ç–∞–Ω–æ–≤–∫–∞': ['–∏–Ω—Å—Ç–∞–ª–ª—è—Ü–∏—è', 'install', 'installation', 'setup'],
    '–∑–∞–ø—É—Å–∫': ['—Å—Ç–∞—Ä—Ç', 'start', 'run', 'launch'],
    '–ø—Ä–æ–±–ª–µ–º–∞': ['issue', '–±–∞–≥', '–æ—à–∏–±–∫–∞', 'problem'],
    '—Ä–µ—à–µ–Ω–∏–µ': ['solution', 'fix', '–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ'],
    '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è': ['—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ', 'guide', '–º–∞–Ω—É–∞–ª', 'howto'],
    '–∫–æ–º–∞–Ω–¥–∞': ['team', '–≥—Ä—É–ø–ø–∞', '–æ—Ç–¥–µ–ª'],
    '–ø—Ä–æ–µ–∫—Ç': ['project', '—Å–∏—Å—Ç–µ–º–∞', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '—Å–µ—Ä–≤–∏—Å'],
    '–≤–µ—Ä—Å–∏—è': ['version', '—Ä–µ–ª–∏–∑', 'release'],
    '–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ': ['update', '–∞–ø–¥–µ–π—Ç', 'upgrade'],
}


class QueryMiner:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ —Å–∏–Ω–æ–Ω–∏–º–æ–≤.
    """

    def __init__(self, data_dir: str = "./data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)

        self.query_log_file = self.data_dir / "query_log.json"
        self.learned_synonyms_file = self.data_dir / "learned_synonyms.json"

        self.query_log = self._load_query_log()
        self.co_occurrence = {}

        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≥—Ä–∞—Ñ –∏–∑ –ª–æ–≥–æ–≤
        self._rebuild_co_occurrence()

    def _load_query_log(self) -> list:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –∑–∞–ø—Ä–æ—Å–æ–≤."""
        if self.query_log_file.exists():
            try:
                with open(self.query_log_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å query_log: {e}")
        return []

    def _rebuild_co_occurrence(self):
        """–í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –≥—Ä–∞—Ñ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏ –∏–∑ –ª–æ–≥–æ–≤."""
        for entry in self.query_log:
            query = entry.get('query', '')
            result_pages = entry.get('result_pages', [])

            query_terms = self._extract_keywords(query)

            for term in query_terms:
                if term not in self.co_occurrence:
                    self.co_occurrence[term] = {'pages': set(), 'count': 0}

                self.co_occurrence[term]['pages'].update(result_pages)
                self.co_occurrence[term]['count'] += 1

    def _extract_keywords(self, text: str) -> list:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = text.lower()

        # –£–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {'–≤', '–Ω–∞', '–∏', '—Å', '–ø–æ', '–¥–ª—è', '–∫–∞–∫', '—á—Ç–æ', '—ç—Ç–æ', '–∏–ª–∏', '–∞', '–Ω–æ'}

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞ (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞ –∏ –ª–∞—Ç–∏–Ω–∏—Ü–∞)
        words = re.findall(r'[–∞-—è—ëa-z0-9]+', text)

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
        keywords = [w for w in words if w not in stop_words and len(w) > 2]

        return keywords

    def log_query(self, query: str, results: list):
        """
        –õ–æ–≥–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        """
        entry = {
            'query': query,
            'timestamp': time.time(),
            'result_pages': [r['metadata'].get('page_id') for r in results if 'metadata' in r]
        }

        self.query_log.append(entry)

        # –û–±–Ω–æ–≤–ª—è–µ–º –≥—Ä–∞—Ñ
        query_terms = self._extract_keywords(query)
        result_pages = set(entry['result_pages'])

        for term in query_terms:
            if term not in self.co_occurrence:
                self.co_occurrence[term] = {'pages': set(), 'count': 0}

            self.co_occurrence[term]['pages'].update(result_pages)
            self.co_occurrence[term]['count'] += 1

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–µ 10 –∑–∞–ø—Ä–æ—Å–æ–≤
        if len(self.query_log) % 10 == 0:
            self._save_query_log()

        # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤—ã—É—á–µ–Ω–Ω—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã –∫–∞–∂–¥—ã–µ 50 –∑–∞–ø—Ä–æ—Å–æ–≤
        if len(self.query_log) % 50 == 0:
            self.export_learned_synonyms()

    def _save_query_log(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –∑–∞–ø—Ä–æ—Å–æ–≤."""
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º sets –≤ lists –¥–ª—è JSON
            log_to_save = []
            for entry in self.query_log[-1000:]:  # –•—Ä–∞–Ω–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 1000 –∑–∞–ø—Ä–æ—Å–æ–≤
                log_to_save.append(entry)

            with open(self.query_log_file, 'w', encoding='utf-8') as f:
                json.dump(log_to_save, f, ensure_ascii=False, indent=2)

            logger.debug(f"Query log —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {len(log_to_save)} –∑–∞–ø–∏—Å–µ–π")
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å query_log: {e}")

    def find_synonyms(self, term: str, threshold: float = 0.5) -> list:
        """
        –ù–∞—Ö–æ–¥–∏—Ç —Å–∏–Ω–æ–Ω–∏–º—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏.

        –õ–æ–≥–∏–∫–∞: –ï—Å–ª–∏ –¥–≤–∞ —Ç–µ—Ä–º–∏–Ω–∞ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –æ–¥–Ω–∏–º –∏ —Ç–µ–º –∂–µ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º,
        –æ–Ω–∏, –≤–µ—Ä–æ—è—Ç–Ω–æ, —Å–∏–Ω–æ–Ω–∏–º—ã.

        Args:
            term: –¢–µ—Ä–º–∏–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–∏–Ω–æ–Ω–∏–º–æ–≤
            threshold: –ü–æ—Ä–æ–≥ Jaccard similarity (0.0-1.0)

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–∏–Ω–æ–Ω–∏–º–æ–≤
        """
        term = term.lower()

        if term not in self.co_occurrence:
            return []

        term_pages = self.co_occurrence[term]['pages']

        if len(term_pages) < 2:  # –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
            return []

        synonyms = []

        for other_term, data in self.co_occurrence.items():
            if other_term == term:
                continue

            other_pages = data['pages']

            if len(other_pages) < 2:  # –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
                continue

            # –í—ã—á–∏—Å–ª—è–µ–º Jaccard similarity
            intersection = len(term_pages & other_pages)
            union = len(term_pages | other_pages)

            if union > 0:
                similarity = intersection / union

                if similarity >= threshold:
                    synonyms.append((other_term, similarity))

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ similarity
        synonyms.sort(key=lambda x: x[1], reverse=True)

        return [syn for syn, _ in synonyms[:5]]

    def export_learned_synonyms(self) -> dict:
        """
        –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –≤—ã—É—á–µ–Ω–Ω—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã –≤ —Ñ–æ—Ä–º–∞—Ç —Å–ª–æ–≤–∞—Ä—è.

        Returns:
            –°–ª–æ–≤–∞—Ä—å –≤—ã—É—á–µ–Ω–Ω—ã—Ö —Å–∏–Ω–æ–Ω–∏–º–æ–≤
        """
        learned_synonyms = {}

        # –¢–æ–ª—å–∫–æ –¥–ª—è —Ç–µ—Ä–º–∏–Ω–æ–≤ —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö
        for term, data in self.co_occurrence.items():
            if data['count'] >= 3:  # –ú–∏–Ω–∏–º—É–º 3 –∑–∞–ø—Ä–æ—Å–∞
                synonyms = self.find_synonyms(term)
                if synonyms:
                    learned_synonyms[term] = synonyms

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
        try:
            with open(self.learned_synonyms_file, 'w', encoding='utf-8') as f:
                json.dump(learned_synonyms, f, ensure_ascii=False, indent=2)

            logger.info(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(learned_synonyms)} –≤—ã—É—á–µ–Ω–Ω—ã—Ö —Å–∏–Ω–æ–Ω–∏–º–æ–≤")
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å learned_synonyms: {e}")

        return learned_synonyms

    def get_learned_synonyms(self) -> dict:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—ã—É—á–µ–Ω–Ω—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã –∏–∑ —Ñ–∞–π–ª–∞."""
        if self.learned_synonyms_file.exists():
            try:
                with open(self.learned_synonyms_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å learned_synonyms: {e}")
        return {}


class SynonymsManager:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä —Å–∏–Ω–æ–Ω–∏–º–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.
    """

    def __init__(self, data_dir: str = "./data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)

        self.domain_terms_file = self.data_dir / "domain_terms.json"

        # Query Miner –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        self.query_miner = QueryMiner(data_dir)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–º–µ–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        self.domain_terms = self._load_domain_terms()

        logger.info(f"‚úÖ SynonymsManager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        logger.info(f"  - –ë–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å: {len(BASE_SYNONYMS)} —Ç–µ—Ä–º–∏–Ω–æ–≤")
        logger.info(f"  - –î–æ–º–µ–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã: {len(self.domain_terms)} —Ç–µ—Ä–º–∏–Ω–æ–≤")

    def _load_domain_terms(self) -> dict:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–æ–º–µ–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ —Ñ–∞–π–ª–∞."""
        if self.domain_terms_file.exists():
            try:
                with open(self.domain_terms_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å domain_terms: {e}")
        return {}

    def extract_domain_terms_from_confluence(self, collection) -> dict:
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ Confluence.

        –ò–∑–≤–ª–µ–∫–∞–µ—Ç:
        - –ù–∞–∑–≤–∞–Ω–∏—è spaces
        - –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã (–ó–ê–ì–õ–ê–í–ù–´–ï –°–õ–û–í–ê)
        - –ß–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è —Ç–µ—Ä–º–∏–Ω—ã

        Args:
            collection: ChromaDB collection

        Returns:
            –°–ª–æ–≤–∞—Ä—å –¥–æ–º–µ–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        """
        logger.info("üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é Confluence –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–æ–º–µ–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤...")

        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            all_docs = collection.get(limit=10000, include=['documents', 'metadatas'])

            if not all_docs or not all_docs.get('metadatas'):
                logger.warning("–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
                return {}

            domain_terms = {}

            # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è spaces
            spaces = set()
            for metadata in all_docs['metadatas']:
                if metadata and 'space' in metadata:
                    spaces.add(metadata['space'])

            logger.info(f"  –ù–∞–π–¥–µ–Ω–æ spaces: {list(spaces)}")

            # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã (–ó–ê–ì–õ–ê–í–ù–´–ï —Å–ª–æ–≤–∞ 2-6 –±—É–∫–≤)
            abbreviations = set()
            for doc in all_docs['documents']:
                if doc:
                    # –ö–∏—Ä–∏–ª–ª–∏—Ü–∞
                    abbrs_ru = re.findall(r'\b[–ê-–Ø–Å]{2,6}\b', doc)
                    abbreviations.update(abbrs_ru)

                    # –õ–∞—Ç–∏–Ω–∏—Ü–∞
                    abbrs_en = re.findall(r'\b[A-Z]{2,6}\b', doc)
                    abbreviations.update(abbrs_en)

            # –§–∏–ª—å—Ç—Ä—É–µ–º —á–∞—Å—Ç—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            stop_abbrs = {'HTTP', 'HTTPS', 'HTML', 'CSS', 'JSON', 'XML', 'URL', 'API', 'SQL'}
            abbreviations = abbreviations - stop_abbrs

            logger.info(f"  –ù–∞–π–¥–µ–Ω–æ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä: {len(abbreviations)} (–ø–æ–∫–∞–∑—ã–≤–∞—é –ø–µ—Ä–≤—ã–µ 20)")
            logger.info(f"  {list(abbreviations)[:20]}")

            # 3. –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ª–æ–≤–∞—Ä—å
            for space in spaces:
                domain_terms[space.lower()] = [space, space.upper(), space.lower()]

            for abbr in abbreviations:
                key = abbr.lower()
                if key not in domain_terms:
                    domain_terms[key] = [abbr, abbr.upper(), abbr.lower()]

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
            with open(self.domain_terms_file, 'w', encoding='utf-8') as f:
                json.dump(domain_terms, f, ensure_ascii=False, indent=2)

            logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(domain_terms)} –¥–æ–º–µ–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤")

            self.domain_terms = domain_terms
            return domain_terms

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –¥–æ–º–µ–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤: {e}")
            return {}

    def get_synonyms(self, word: str, max_synonyms: int = 5) -> list:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–∏–Ω–æ–Ω–∏–º—ã –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.

        –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:
        1. –ë–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å
        2. –î–æ–º–µ–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        3. –í—ã—É—á–µ–Ω–Ω—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã

        Args:
            word: –°–ª–æ–≤–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–∏–Ω–æ–Ω–∏–º–æ–≤
            max_synonyms: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–Ω–æ–Ω–∏–º–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–∏–Ω–æ–Ω–∏–º–æ–≤
        """
        word_lower = word.lower()
        synonyms = []

        # 1. –ë–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        if word_lower in BASE_SYNONYMS:
            synonyms.extend(BASE_SYNONYMS[word_lower])

        # 2. –î–æ–º–µ–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        if word_lower in self.domain_terms:
            synonyms.extend(self.domain_terms[word_lower])

        # 3. –í—ã—É—á–µ–Ω–Ω—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã
        learned = self.query_miner.get_learned_synonyms()
        if word_lower in learned:
            synonyms.extend(learned[word_lower])

        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        seen = set()
        unique_synonyms = []
        for syn in synonyms:
            syn_lower = syn.lower()
            if syn_lower not in seen and syn_lower != word_lower:
                seen.add(syn_lower)
                unique_synonyms.append(syn)

        return unique_synonyms[:max_synonyms]

    def log_query(self, query: str, results: list):
        """
        –õ–æ–≥–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Query Miner.

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        """
        self.query_miner.log_query(query, results)


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
_synonyms_manager = None

def get_synonyms_manager(data_dir: str = "./data") -> SynonymsManager:
    """–ü–æ–ª—É—á–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä SynonymsManager."""
    global _synonyms_manager
    if _synonyms_manager is None:
        _synonyms_manager = SynonymsManager(data_dir)
    return _synonyms_manager

