"""
–ï–¥–∏–Ω—ã–π –º–æ–¥—É–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å embeddings.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç HuggingFace, Ollama –∏ OpenAI-compatible API (LM Studio, Ollama –∏ –¥—Ä.).
Thread-safe –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π.
"""
import os
import logging
import threading
from typing import Any, List, Optional

logger = logging.getLogger(__name__)

# –û—Ç–∫–ª—é—á–∞–µ–º –∏–∑–±—ã—Ç–æ—á–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ HTTP –∑–∞–ø—Ä–æ—Å–æ–≤ –æ—Ç httpx/openai
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)
logging.getLogger("openai").setLevel(logging.WARNING)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ ENV
USE_OLLAMA = os.getenv("USE_OLLAMA", "false").lower() == "true"
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://ollama:11434")
EMBED_MODEL = os.getenv("EMBED_MODEL", "ai-forever/FRIDA")

# OpenAI-compatible API (–¥–ª—è Ollama, LM Studio –∏ –¥—Ä.)
OPENAI_API_BASE = os.getenv("OPENAI_API_BASE", "")  # –ù–∞–ø—Ä–∏–º–µ—Ä: http://localhost:11434/v1
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "ollama")  # –î–ª—è Ollama –æ–±—ã—á–Ω–æ –Ω–µ –Ω—É–∂–µ–Ω
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "")  # –ò–º—è –º–æ–¥–µ–ª–∏ –¥–ª—è API

# Thread-safe –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Lock
_model_lock = threading.Lock()
_embed_model = None
_embed_model_type = None  # 'openai', 'ollama', 'huggingface'

def get_embed_model():
    """
    –ü–æ–ª—É—á–∏—Ç—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å embeddings.
    
    –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏ (–≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞):
    1. OpenAI-compatible API (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω OPENAI_API_BASE) - –¥–ª—è Ollama, LM Studio –∏ –¥—Ä.
    2. LlamaIndex Ollama (–µ—Å–ª–∏ USE_OLLAMA=true)
    3. HuggingFace (–ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    
    –í–ê–ñ–ù–û: –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞ –º–æ–¥–µ–ª—å, –æ–Ω–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –¥–æ—Å—Ç—É–ø–Ω–∞. –ù–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ fallback.
    
    –ú–æ–¥–µ–ª—å –∫—ç—à–∏—Ä—É–µ—Ç—Å—è –≥–ª–æ–±–∞–ª—å–Ω–æ –∏ —Å–æ–∑–¥–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑.
    
    Returns:
        Embedding model instance
    
    Raises:
        RuntimeError: –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞
        ImportError: –ï—Å–ª–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–∞–∫–µ—Ç—ã
    """
    global _embed_model, _embed_model_type
    
    # ========================================
    # –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –í—ã–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–æ EMBEDDING_SOURCE
    # ========================================
    embedding_source = os.getenv('EMBEDDING_SOURCE', '').lower()
    
    if embedding_source:
        logger.info(f"üîÑ EMBEDDING_SOURCE={embedding_source} (explicit selection)")
        
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å –¥—Ä—É–≥–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º - –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å
        if _embed_model is not None and _embed_model_type != embedding_source:
            logger.warning(f"‚ö†Ô∏è Model already loaded with type {_embed_model_type}, reloading for {embedding_source}")
            _embed_model = None
            _embed_model_type = None
    else:
        logger.debug("‚ÑπÔ∏è EMBEDDING_SOURCE not specified, using legacy logic (priority order)")
    
    # Double-check locking pattern –¥–ª—è thread-safety
    if _embed_model is None:
        with _model_lock:
            # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤–Ω—É—Ç—Ä–∏ lock (–¥—Ä—É–≥–æ–π –ø–æ—Ç–æ–∫ –º–æ–≥ —É–∂–µ –∑–∞–≥—Ä—É–∑–∏—Ç—å)
            if _embed_model is None:
                import time
                start_time = time.time()
                
                # ========== –í–ê–†–ò–ê–ù–¢ 1: OpenRouter (–µ—Å–ª–∏ EMBEDDING_SOURCE=openrouter) ==========
                if embedding_source == 'openrouter':
                    if not OPENAI_API_BASE or not OPENAI_API_KEY or not OPENAI_MODEL:
                        raise ValueError(
                            "EMBEDDING_SOURCE=openrouter requires: "
                            "OPENAI_API_BASE, OPENAI_API_KEY, OPENAI_MODEL"
                        )
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ –¥–ª—è OpenAI-compatible API
                    try:
                        from openai import OpenAI
                        from llama_index.core.embeddings import BaseEmbedding
                        
                        api_base = OPENAI_API_BASE.rstrip('/')
                        if not api_base.endswith('/v1'):
                            api_base = f"{api_base}/v1"
                        
                        model_name = OPENAI_MODEL or EMBED_MODEL
                        
                        logger.info(f"üîå –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ OpenAI-compatible API: {api_base}")
                        logger.info(f"   –ú–æ–¥–µ–ª—å: {model_name}")
                        
                        client = OpenAI(base_url=api_base, api_key=OPENAI_API_KEY or None)
                        
                        # –¢–µ—Å—Ç–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
                        test_response = client.embeddings.create(
                            model=model_name,
                            input=["test"]
                        )
                        
                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
                        test_dim = len(test_response.data[0].embedding)
                        
                        # –°–æ–∑–¥–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π –∫–ª–∞—Å—Å, –Ω–∞—Å–ª–µ–¥—É—é—â–∏–π—Å—è –æ—Ç BaseEmbedding
                        class CustomOpenAIEmbedding(BaseEmbedding):
                            def __init__(self, client, model_name, dimension):
                                # BaseEmbedding –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Pydantic, –Ω—É–∂–Ω–æ –≤—ã–∑—ã–≤–∞—Ç—å super().__init__() –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                                super().__init__()
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞–∫ –ø—Ä–∏–≤–∞—Ç–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã (–Ω–µ Pydantic –ø–æ–ª—è)
                                self._client = client
                                self._model_name = model_name
                                self._dimension = dimension
                            
                            def _get_query_embedding(self, query: str) -> List[float]:
                                response = self._client.embeddings.create(
                                    model=self._model_name,
                                    input=[query]
                                )
                                return response.data[0].embedding
                            
                            async def _aget_query_embedding(self, query: str) -> List[float]:
                                # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç)
                                return self._get_query_embedding(query)
                            
                            def _get_text_embedding(self, text: str) -> List[float]:
                                return self._get_query_embedding(text)
                            
                            async def _aget_text_embedding(self, text: str) -> List[float]:
                                return self._get_text_embedding(text)
                            
                            def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:
                                response = self._client.embeddings.create(
                                    model=self._model_name,
                                    input=texts
                                )
                                return [item.embedding for item in response.data]
                            
                            async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:
                                return self._get_text_embeddings(texts)
                            
                            @property
                            def dimension(self) -> int:
                                return self._dimension
                            
                            def get_embedding_dimension(self) -> int:
                                """–ú–µ—Ç–æ–¥ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å get_embedding_dimension()."""
                                return self._dimension
                        
                        _embed_model = CustomOpenAIEmbedding(client, model_name, test_dim)
                        _embed_model_type = 'openai'
                        
                        elapsed = time.time() - start_time
                        logger.info(f"‚úÖ OpenAI-compatible API –ø–æ–¥–∫–ª—é—á–µ–Ω –∑–∞ {elapsed:.1f} —Å–µ–∫")
                        logger.info(f"   –ú–æ–¥–µ–ª—å: {model_name}, –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {test_dim}D")
                        return _embed_model
                        
                    except ImportError as import_err:
                        error_msg = (
                            f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –º–æ–¥—É–ª–∏\n"
                            f"   –û—à–∏–±–∫–∞: {import_err}\n"
                            f"   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install openai llama-index"
                        )
                        logger.error(error_msg)
                        raise RuntimeError(error_msg)
                    except Exception as api_error:
                        error_msg = (
                            f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ OpenAI-compatible API\n"
                            f"   URL: {OPENAI_API_BASE}\n"
                            f"   –ú–æ–¥–µ–ª—å: {model_name}\n"
                            f"   –û—à–∏–±–∫–∞: {api_error}\n\n"
                            f"   –†–ï–®–ï–ù–ò–ï:\n"
                            f"   1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ URL: {OPENAI_API_BASE}\n"
                            f"   2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ API –∫–ª—é—á: {OPENAI_API_KEY[:10] if OPENAI_API_KEY else '–Ω–µ —É–∫–∞–∑–∞–Ω'}...\n"
                            f"   3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏–º—è –º–æ–¥–µ–ª–∏: {model_name}"
                        )
                        logger.error(error_msg)
                        raise RuntimeError(error_msg)
                
                # ========== –í–ê–†–ò–ê–ù–¢ 2: Ollama (–µ—Å–ª–∏ EMBEDDING_SOURCE=ollama) ==========
                elif embedding_source == 'ollama':
                    ollama_url = os.getenv('OLLAMA_URL', 'http://localhost:11434')
                    ollama_model = os.getenv('OLLAMA_EMBEDDING_MODEL') or os.getenv('OLLAMA_MODEL') or EMBED_MODEL
                    
                    if not ollama_url:
                        raise ValueError("EMBEDDING_SOURCE=ollama requires: OLLAMA_URL")
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ –¥–ª—è Ollama
                    # ========== –ü–†–ò–û–†–ò–¢–ï–¢ 2: LlamaIndex Ollama ==========
                    try:
                        from llama_index.embeddings.ollama import OllamaEmbedding
                        logger.info(f"üîå –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {ollama_model} @ {ollama_url}")
                        _embed_model = OllamaEmbedding(model_name=ollama_model, base_url=ollama_url)
                        
                        # –¢–µ—Å—Ç–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
                        test_embedding = _embed_model.get_text_embedding("test")
                        _embed_model_type = 'ollama'
                        
                        elapsed = time.time() - start_time
                        logger.info(f"‚úÖ Ollama –ø–æ–¥–∫–ª—é—á–µ–Ω –∑–∞ {elapsed:.1f} —Å–µ–∫")
                        logger.info(f"   –ú–æ–¥–µ–ª—å: {ollama_model}, –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {len(test_embedding)}D")
                        return _embed_model
                        
                    except ImportError:
                        error_msg = (
                            f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: llama-index-embeddings-ollama –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\n"
                            f"   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install llama-index-embeddings-ollama"
                        )
                        logger.error(error_msg)
                        raise RuntimeError(error_msg)
                    except Exception as ollama_error:
                        error_msg = (
                            f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama\n"
                            f"   URL: {ollama_url}\n"
                            f"   –ú–æ–¥–µ–ª—å: {ollama_model}\n"
                            f"   –û—à–∏–±–∫–∞: {ollama_error}\n\n"
                            f"   –†–ï–®–ï–ù–ò–ï:\n"
                            f"   1. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω: ollama serve\n"
                            f"   2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ URL: {ollama_url}\n"
                            f"   3. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –º–æ–¥–µ–ª—å: ollama pull {ollama_model}"
                        )
                        logger.error(error_msg)
                        raise RuntimeError(error_msg)
                
                # ========== –í–ê–†–ò–ê–ù–¢ 3: HuggingFace (–µ—Å–ª–∏ EMBEDDING_SOURCE=huggingface) ==========
                elif embedding_source == 'huggingface':
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ –¥–ª—è HuggingFace
                    # ========== –ü–†–ò–û–†–ò–¢–ï–¢ 3: HuggingFace (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —É–∫–∞–∑–∞–Ω–æ) ==========
                    try:
                        from llama_index.embeddings.huggingface import HuggingFaceEmbedding
                        logger.info(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ HuggingFace embeddings: {EMBED_MODEL}")
                        logger.info("   (~1.5GB, –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 30-90 —Å–µ–∫)")
                        _embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL)
                        _embed_model_type = 'huggingface'
                        
                        elapsed = time.time() - start_time
                        logger.info(f"‚úÖ HuggingFace –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {elapsed:.1f} —Å–µ–∫")
                        logger.info(f"   –ú–æ–¥–µ–ª—å: {EMBED_MODEL}")
                        return _embed_model
                        
                    except ImportError as e:
                        error_msg = (
                            f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: llama-index-embeddings-huggingface –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\n"
                            f"   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install llama-index-embeddings-huggingface"
                        )
                        logger.error(error_msg)
                        raise RuntimeError(error_msg)
                    except Exception as e:
                        error_msg = (
                            f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å HuggingFace –º–æ–¥–µ–ª—å\n"
                            f"   –ú–æ–¥–µ–ª—å: {EMBED_MODEL}\n"
                            f"   –û—à–∏–±–∫–∞: {e}"
                        )
                        logger.error(error_msg)
                        import traceback
                        logger.error(f"Traceback: {traceback.format_exc()}")
                        raise RuntimeError(error_msg)
                
                # ========== LEGACY: –°—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞ (–µ—Å–ª–∏ EMBEDDING_SOURCE –Ω–µ —É–∫–∞–∑–∞–Ω) ==========
                elif embedding_source == '':
                    logger.info("‚ÑπÔ∏è EMBEDDING_SOURCE not specified, using legacy logic (priority order)")
                    
                    # ========== –ü–†–ò–û–†–ò–¢–ï–¢ 1: OpenAI-compatible API ==========
                    if OPENAI_API_BASE:
                        try:
                            from openai import OpenAI
                            from llama_index.core.embeddings import BaseEmbedding
                            
                            api_base = OPENAI_API_BASE.rstrip('/')
                            if not api_base.endswith('/v1'):
                                api_base = f"{api_base}/v1"
                            
                            model_name = OPENAI_MODEL or EMBED_MODEL
                            
                            logger.info(f"üîå –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ OpenAI-compatible API: {api_base}")
                            logger.info(f"   –ú–æ–¥–µ–ª—å: {model_name}")
                            
                            client = OpenAI(base_url=api_base, api_key=OPENAI_API_KEY or None)
                            
                            # –¢–µ—Å—Ç–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
                            test_response = client.embeddings.create(
                                model=model_name,
                                input=["test"]
                            )
                            
                            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
                            test_dim = len(test_response.data[0].embedding)
                            
                            # –°–æ–∑–¥–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π –∫–ª–∞—Å—Å, –Ω–∞—Å–ª–µ–¥—É—é—â–∏–π—Å—è –æ—Ç BaseEmbedding
                            class CustomOpenAIEmbedding(BaseEmbedding):
                                def __init__(self, client, model_name, dimension):
                                    super().__init__()
                                    self._client = client
                                    self._model_name = model_name
                                    self._dimension = dimension
                                
                                def _get_query_embedding(self, query: str) -> List[float]:
                                    response = self._client.embeddings.create(
                                        model=self._model_name,
                                        input=[query]
                                    )
                                    return response.data[0].embedding
                                
                                async def _aget_query_embedding(self, query: str) -> List[float]:
                                    return self._get_query_embedding(query)
                                
                                def _get_text_embedding(self, text: str) -> List[float]:
                                    return self._get_query_embedding(text)
                                
                                async def _aget_text_embedding(self, text: str) -> List[float]:
                                    return self._get_text_embedding(text)
                                
                                def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:
                                    response = self._client.embeddings.create(
                                        model=self._model_name,
                                        input=texts
                                    )
                                    return [item.embedding for item in response.data]
                                
                                async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:
                                    return self._get_text_embeddings(texts)
                                
                                @property
                                def dimension(self) -> int:
                                    return self._dimension
                                
                                def get_embedding_dimension(self) -> int:
                                    return self._dimension
                            
                            _embed_model = CustomOpenAIEmbedding(client, model_name, test_dim)
                            _embed_model_type = 'openai'
                            
                            elapsed = time.time() - start_time
                            logger.info(f"‚úÖ OpenAI-compatible API –ø–æ–¥–∫–ª—é—á–µ–Ω –∑–∞ {elapsed:.1f} —Å–µ–∫")
                            logger.info(f"   –ú–æ–¥–µ–ª—å: {model_name}, –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {test_dim}D")
                            return _embed_model
                            
                        except ImportError as import_err:
                            error_msg = (
                                f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –º–æ–¥—É–ª–∏\n"
                                f"   –û—à–∏–±–∫–∞: {import_err}\n"
                                f"   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install openai llama-index"
                            )
                            logger.error(error_msg)
                            raise RuntimeError(error_msg)
                        except Exception as api_error:
                            error_msg = (
                                f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ OpenAI-compatible API\n"
                                f"   URL: {OPENAI_API_BASE}\n"
                                f"   –ú–æ–¥–µ–ª—å: {model_name}\n"
                                f"   –û—à–∏–±–∫–∞: {api_error}\n\n"
                                f"   –†–ï–®–ï–ù–ò–ï:\n"
                                f"   1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ URL: {OPENAI_API_BASE}\n"
                                f"   2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ API –∫–ª—é—á: {OPENAI_API_KEY[:10] if OPENAI_API_KEY else '–Ω–µ —É–∫–∞–∑–∞–Ω'}...\n"
                                f"   3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏–º—è –º–æ–¥–µ–ª–∏: {model_name}"
                            )
                            logger.error(error_msg)
                            raise RuntimeError(error_msg)
                    
                    # ========== –ü–†–ò–û–†–ò–¢–ï–¢ 2: LlamaIndex Ollama ==========
                    if USE_OLLAMA:
                        try:
                            from llama_index.embeddings.ollama import OllamaEmbedding
                            logger.info(f"üîå –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {EMBED_MODEL} @ {OLLAMA_URL}")
                            _embed_model = OllamaEmbedding(model_name=EMBED_MODEL, base_url=OLLAMA_URL)
                            
                            # –¢–µ—Å—Ç–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
                            test_embedding = _embed_model.get_text_embedding("test")
                            _embed_model_type = 'ollama'
                            
                            elapsed = time.time() - start_time
                            logger.info(f"‚úÖ Ollama –ø–æ–¥–∫–ª—é—á–µ–Ω –∑–∞ {elapsed:.1f} —Å–µ–∫")
                            logger.info(f"   –ú–æ–¥–µ–ª—å: {EMBED_MODEL}, –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {len(test_embedding)}D")
                            return _embed_model
                            
                        except ImportError:
                            error_msg = (
                                f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: llama-index-embeddings-ollama –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\n"
                                f"   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install llama-index-embeddings-ollama"
                            )
                            logger.error(error_msg)
                            raise RuntimeError(error_msg)
                        except Exception as ollama_error:
                            error_msg = (
                                f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama\n"
                                f"   URL: {OLLAMA_URL}\n"
                                f"   –ú–æ–¥–µ–ª—å: {EMBED_MODEL}\n"
                                f"   –û—à–∏–±–∫–∞: {ollama_error}\n\n"
                                f"   –†–ï–®–ï–ù–ò–ï:\n"
                                f"   1. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω: ollama serve\n"
                                f"   2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ URL: {OLLAMA_URL}\n"
                                f"   3. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –º–æ–¥–µ–ª—å: ollama pull {EMBED_MODEL}"
                            )
                            logger.error(error_msg)
                            raise RuntimeError(error_msg)
                    
                    # ========== –ü–†–ò–û–†–ò–¢–ï–¢ 3: HuggingFace (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —É–∫–∞–∑–∞–Ω–æ) ==========
                    try:
                        from llama_index.embeddings.huggingface import HuggingFaceEmbedding
                        logger.info(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ HuggingFace embeddings: {EMBED_MODEL}")
                        logger.info("   (~1.5GB, –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 30-90 —Å–µ–∫)")
                        _embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL)
                        _embed_model_type = 'huggingface'
                        
                        elapsed = time.time() - start_time
                        logger.info(f"‚úÖ HuggingFace –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {elapsed:.1f} —Å–µ–∫")
                        logger.info(f"   –ú–æ–¥–µ–ª—å: {EMBED_MODEL}")
                        return _embed_model
                        
                    except ImportError as e:
                        error_msg = (
                            f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: llama-index-embeddings-huggingface –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\n"
                            f"   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install llama-index-embeddings-huggingface"
                        )
                        logger.error(error_msg)
                        raise RuntimeError(error_msg)
                    except Exception as e:
                        error_msg = (
                            f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å HuggingFace –º–æ–¥–µ–ª—å\n"
                            f"   –ú–æ–¥–µ–ª—å: {EMBED_MODEL}\n"
                            f"   –û—à–∏–±–∫–∞: {e}"
                        )
                        logger.error(error_msg)
                        import traceback
                        logger.error(f"Traceback: {traceback.format_exc()}")
                        raise RuntimeError(error_msg)
                
                else:
                    raise ValueError(
                        f"Unknown EMBEDDING_SOURCE: {embedding_source}. "
                        f"Use: 'openrouter', 'ollama', 'huggingface' or leave empty for legacy logic"
                    )
    
    return _embed_model

def get_embedding_dimension() -> int:
    """
    –ü–æ–ª—É—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å embeddings —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏.
    
    Returns:
        int: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞ embeddings
    """
    global _embed_model_type
    
    model = get_embed_model()
    
    if _embed_model_type == 'openai':
        # OpenAI-compatible API: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
        return model.get_embedding_dimension()
    elif _embed_model_type == 'ollama':
        # –î–ª—è Ollama –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        test_embedding = model.get_text_embedding("test")
        return len(test_embedding)
    else:
        # –î–ª—è HuggingFace –º–æ–∂–µ–º –ø–æ–ª—É—á–∏—Ç—å –∏–∑ _model
        try:
            return model._model.get_sentence_embedding_dimension()
        except AttributeError:
            # Fallback: —Ç–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            test_embedding = model.get_text_embedding("test")
            return len(test_embedding)

def generate_query_embedding(query: str) -> List[float]:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç embedding –¥–ª—è –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–∞–∫ HuggingFace, —Ç–∞–∫ –∏ Ollama.
    
    Args:
        query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        
    Returns:
        –°–ø–∏—Å–æ–∫ float –∑–Ω–∞—á–µ–Ω–∏–π (embedding –≤–µ–∫—Ç–æ—Ä)
    """
    model = get_embed_model()
    
    # LlamaIndex embeddings –∏–º–µ—é—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ get_query_embedding()
    embedding = model.get_query_embedding(query)
    
    return embedding

def generate_query_embeddings_batch(queries: List[str]) -> List[List[float]]:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç embeddings –¥–ª—è —Å–ø–∏—Å–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤.
    
    –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –¥–ª—è HuggingFace: 
    - Batch encoding –≤ 3-5 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ —á–µ—Ä–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π SentenceTransformer
    
    –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –¥–ª—è OpenAI-compatible API:
    - Batch encoding —á–µ—Ä–µ–∑ API (–µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è)
    
    –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –¥–ª—è Ollama:
    - –ò—Ç–µ—Ä–∞—Ü–∏—è –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º (Ollama API –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç batch)
    
    Args:
        queries: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤
        
    Returns:
        –°–ø–∏—Å–æ–∫ embedding –≤–µ–∫—Ç–æ—Ä–æ–≤
    """
    global _embed_model_type
    
    model = get_embed_model()
    
    if _embed_model_type == 'openai':
        # OpenAI-compatible API: –ø—Ä–æ–±—É–µ–º batch, fallback –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∏–≤–∞—Ç–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã –¥–ª—è CustomOpenAIEmbedding
            if hasattr(model, '_client'):
                # CustomOpenAIEmbedding –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–∏–≤–∞—Ç–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
                response = model._client.embeddings.create(
                    model=model._model_name,
                    input=queries
                )
                return [item.embedding for item in response.data]
            else:
                # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π OpenAIEmbedding –∏–∑ LlamaIndex
                response = model.client.embeddings.create(
                    model=model.model_name,
                    input=queries
                )
                return [item.embedding for item in response.data]
        except Exception as e:
            logger.debug(f"Batch –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã: {e}")
            return [model.get_query_embedding(q) for q in queries]
    elif _embed_model_type == 'huggingface':
        # HuggingFace: –∏—Å–ø–æ–ª—å–∑—É–µ–º batch encoding —á–µ—Ä–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π SentenceTransformer
        try:
            sentence_model = model._model
            embeddings = sentence_model.encode(queries, normalize_embeddings=False)
            return [emb.tolist() for emb in embeddings]
        except AttributeError:
            # Fallback –µ—Å–ª–∏ _model –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å batch encoding, fallback –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é")
            return [model.get_query_embedding(q) for q in queries]
    else:
        # Ollama: –ø–æ –æ–¥–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É (–Ω–µ—Ç batch API)
        return [model.get_query_embedding(q) for q in queries]

def validate_collection_dimension(collection, expected_dim: int = None) -> tuple[bool, int, int]:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ embeddings –≤ ChromaDB —Å —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª—å—é.
    
    Args:
        collection: ChromaDB collection
        expected_dim: –û–∂–∏–¥–∞–µ–º–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (–µ—Å–ª–∏ None - –±–µ—Ä–µ—Ç—Å—è –∏–∑ –º–æ–¥–µ–ª–∏)
        
    Returns:
        tuple: (is_valid, collection_dim, model_dim)
    """
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∏–∑ collection
        data = collection.get(limit=1, include=['embeddings'])
        embeddings = data.get('embeddings', [])
        
        if len(embeddings) == 0 or embeddings[0] is None:
            logger.warning("‚ö†Ô∏è ChromaDB –ø—É—Å—Ç–∞—è, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞")
            return (True, 0, 0)  # –ü—É—Å—Ç–∞—è –±–∞–∑–∞ - OK
        
        collection_dim = len(embeddings[0])
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
        if expected_dim is None:
            model_dim = get_embedding_dimension()
        else:
            model_dim = expected_dim
        
        is_valid = (collection_dim == model_dim)
        
        if not is_valid:
            logger.error(
                f"‚ùå –ù–ï–°–û–í–ü–ê–î–ï–ù–ò–ï –†–ê–ó–ú–ï–†–ù–û–°–¢–ò!\n"
                f"   ChromaDB: {collection_dim} dimensions\n"
                f"   –ú–æ–¥–µ–ª—å {EMBED_MODEL}: {model_dim} dimensions\n"
                f"   ‚Üí –ù–µ–æ–±—Ö–æ–¥–∏–º–∞ –ø–µ—Ä–µ—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –±–∞–∑—ã!"
            )
        else:
            logger.info(f"‚úÖ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å embeddings –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞: {model_dim}D")
        
        return (is_valid, collection_dim, model_dim)
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: {e}")
        return (False, 0, 0)

