import os
import json
import asyncio
from typing import List, Dict, Any
import pandas as pd
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_recall,
    context_precision
)
from rag_server.config import settings
from rag_server.mcp_rag_secure import confluence_semantic_search

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ OpenAI API Key –¥–ª—è Ragas
if settings.openai_api_key:
    os.environ["OPENAI_API_KEY"] = settings.openai_api_key

async def generate_rag_answers(questions: List[str]) -> List[Dict[str, Any]]:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã –∏—Å–ø–æ–ª—å–∑—É—è —Ç–µ–∫—É—â–∏–π RAG pipeline.
    """
    results = []
    for q in questions:
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –Ω–∞—à RAG
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º mcp tool –∫–∞–∫ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
        search_result_text = await confluence_semantic_search(q, limit=5)
        
        # –í —Ä–µ–∞–ª—å–Ω–æ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∑–¥–µ—Å—å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤—ã–∑–æ–≤ LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ search_result_text.
        # –î–ª—è –æ—Ü–µ–Ω–∫–∏ retrieval metrics (context_recall, context_precision) –Ω–∞–º –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
        # –î–ª—è –æ—Ü–µ–Ω–∫–∏ generation metrics (faithfulness, answer_relevancy) –Ω—É–∂–µ–Ω –æ—Ç–≤–µ—Ç LLM.
        
        # –≠–º—É–ª—è—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ LLM (–∏–ª–∏ —Ä–µ–∞–ª—å–Ω—ã–π –≤—ã–∑–æ–≤ –µ—Å–ª–∏ –µ—Å—Ç—å integration)
        # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π placeholder –∏–ª–∏ –º–æ–∂–Ω–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—ã–∑–æ–≤ LLM –µ—Å–ª–∏ –µ—Å—Ç—å.
        # –î–ª—è —Ü–µ–ª–µ–π –¥–∞–Ω–Ω–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞, –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º –º—ã –æ—Ü–µ–Ω–∏–≤–∞–µ–º retrieval –∫–∞—á–µ—Å—Ç–≤–æ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º.
        
        # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –æ–±—Ä–∞—Ç–Ω–æ (—ç—Ç–æ —Ç–µ–∫—Å—Ç) —á—Ç–æ–±—ã –¥–æ—Å—Ç–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã
        contexts = []
        if "‚úÖ –ù–∞–π–¥–µ–Ω–æ" in search_result_text:
            # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            lines = search_result_text.split('\n')
            current_context = ""
            for line in lines:
                if "üí¨" in line:
                    current_context = line.replace("üí¨", "").strip()
                    contexts.append(current_context)
        
        # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –æ—Ç–≤–µ—Ç–∞, —Ç–∞–∫ –∫–∞–∫ —É –Ω–∞—Å —Ç–æ–ª—å–∫–æ retrieval —á–∞—Å—Ç—å —Å–µ–π—á–∞—Å exposed —á–µ—Ä–µ–∑ mcp tool —è–≤–Ω–æ.
        # –í –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–º –ø–∞–π–ø–ª–∞–π–Ω–µ —Ç—É—Ç –±—ã–ª –±—ã –≤—ã–∑–æ–≤ generate_response(query, contexts)
        answer = "Generated answer based on retrieved contexts." 

        results.append({
            "question": q,
            "answer": answer,
            "contexts": contexts,
            # Ground truth –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
        })
    
    return results

def run_evaluation(golden_dataset_path: str = "data/golden_dataset.json"):
    """
    –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ RAG –ø–∞–π–ø–ª–∞–π–Ω–∞.
    """
    if not os.path.exists(golden_dataset_path):
        print(f"‚ö†Ô∏è Dataset not found at {golden_dataset_path}. Please create one first.")
        return

    with open(golden_dataset_path, 'r', encoding='utf-8') as f:
        golden_data = json.load(f)

    questions = [item['question'] for item in golden_data]
    ground_truths = [[item['ground_truth']] for item in golden_data] # Ragas –æ–∂–∏–¥–∞–µ—Ç list of lists

    print(f"üöÄ Starting evaluation for {len(questions)} questions...")

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ —Å–∏—Å—Ç–µ–º–æ–π
    rag_outputs = asyncio.run(generate_rag_answers(questions))
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Ragas
    data = {
        'question': questions,
        'answer': [item['answer'] for item in rag_outputs],
        'contexts': [item['contexts'] for item in rag_outputs],
        'ground_truth': ground_truths
    }
    
    dataset = Dataset.from_dict(data)

    # –û—Ü–µ–Ω–∫–∞
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ retrieval –º–µ—Ç—Ä–∏–∫–∏ –µ—Å–ª–∏ –Ω–µ—Ç —Ä–µ–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤
    metrics = [
        context_recall, 
        context_precision
    ]
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å OpenAI –∫–ª—é—á, –º–æ–∂–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é (faithfulness, answer_relevancy)
    if os.environ.get("OPENAI_API_KEY"):
        metrics.extend([faithfulness, answer_relevancy])
    else:
        print("‚ö†Ô∏è OpenAI API Key not found. Skipping generation metrics (faithfulness, answer_relevancy).")

    results = evaluate(
        dataset=dataset,
        metrics=metrics
    )

    print("\nüìä Evaluation Results:")
    print(results)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    df = results.to_pandas()
    output_path = "docs/analysis/rag_evaluation_results.csv"
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    df.to_csv(output_path, index=False)
    print(f"‚úÖ Detailed results saved to {output_path}")

if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä —Å–æ–∑–¥–∞–Ω–∏—è dummy –¥–∞—Ç–∞—Å–µ—Ç–∞ –µ—Å–ª–∏ –Ω–µ—Ç
    if not os.path.exists("data/golden_dataset.json"):
        os.makedirs("data", exist_ok=True)
        dummy_data = [
            {
                "question": "–ö–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å Qdrant?",
                "ground_truth": "–î–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Qdrant –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∑–∞–¥–∞—Ç—å host –∏ port –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."
            },
            {
                "question": "–ö–∞–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏?",
                "ground_truth": "–ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è faithfulness, answer_relevancy, context_recall –∏ context_precision."
            }
        ]
        with open("data/golden_dataset.json", 'w', encoding='utf-8') as f:
            json.dump(dummy_data, f, ensure_ascii=False, indent=2)
        print("Created dummy golden_dataset.json for testing.")

    run_evaluation()

