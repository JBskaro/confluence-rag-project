#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ç–µ—Å—Ç –≤—Å–µ—Ö 5 —É–ª—É—á—à–µ–Ω–∏–π —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –ø–æ–∫—Ä—ã—Ç–∏–µ–º

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç:
1. Query Expansion (5-–π –∏—Å—Ç–æ—á–Ω–∏–∫ - Semantic Query Log)
2. Parallel Multi-Query Search
3. Hybrid Search (Adaptive Weights)
4. Diversity Filter (–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º–æ—Å—Ç—å)
5. Context Expansion (Bidirectional + Related)
"""

import sys
import os
import io
import json
import time
from typing import List, Dict, Any
from collections import defaultdict

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º UTF-8 –¥–ª—è –≤—ã–≤–æ–¥–∞ –≤ Windows
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º ENV –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–æ–≤
os.environ['ENABLE_DIVERSITY_FILTER'] = 'true'
os.environ['DIVERSITY_LIMIT_NAVIGATIONAL'] = '1'
os.environ['DIVERSITY_LIMIT_EXPLORATORY'] = '4'
os.environ['DIVERSITY_LIMIT_FACTUAL'] = '2'
os.environ['DIVERSITY_LIMIT_HOWTO'] = '3'
os.environ['ENABLE_CONTEXT_EXPANSION'] = 'true'
os.environ['CONTEXT_EXPANSION_MODE'] = 'bidirectional'
os.environ['CONTEXT_EXPANSION_SIZE'] = '2'
os.environ['ENABLE_PARALLEL_SEARCH'] = 'true'
os.environ['PARALLEL_SEARCH_MAX_WORKERS'] = '4'
os.environ['ENABLE_HYBRID_SEARCH'] = 'true'
os.environ['HYBRID_VECTOR_WEIGHT_NAVIGATIONAL'] = '0.7'
os.environ['HYBRID_BM25_WEIGHT_NAVIGATIONAL'] = '0.3'
os.environ['HYBRID_VECTOR_WEIGHT_EXPLORATORY'] = '0.5'
os.environ['HYBRID_BM25_WEIGHT_EXPLORATORY'] = '0.5'
os.environ['HYBRID_VECTOR_WEIGHT_FACTUAL'] = '0.6'
os.environ['HYBRID_BM25_WEIGHT_FACTUAL'] = '0.4'
os.environ['HYBRID_VECTOR_WEIGHT_HOWTO'] = '0.55'
os.environ['HYBRID_BM25_WEIGHT_HOWTO'] = '0.45'
os.environ['QUERY_LOG_MIN_RATING'] = '4.0'
os.environ['QUERY_LOG_MAX_SIZE'] = '10000'


# ============ –¢–ï–°–¢ 1: Semantic Query Log ============

def test_semantic_query_log():
    """–¢–µ—Å—Ç Semantic Query Log (–®–ê–ì 1)"""
    print("=" * 70)
    print("–¢–ï–°–¢ 1: Query Expansion - 5-–π –∏—Å—Ç–æ—á–Ω–∏–∫ (Semantic Query Log)")
    print("=" * 70)
    
    sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'rag_server'))
    
    try:
        from semantic_query_log import SemanticQueryLog, get_semantic_query_log
        
        # –¢–µ—Å—Ç 1.1: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        print("\n1.1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Semantic Query Log...")
        log = SemanticQueryLog()
        print(f"   [OK] Semantic Query Log –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {len(log.query_log)} –∑–∞–ø–∏—Å–µ–π")
        
        # –¢–µ—Å—Ç 1.2: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤
        print("\n1.2. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤...")
        log.log_query('–∫–∞–∫ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', 5, user_rating=5)
        log.log_query('—É—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–æ–≥—Ä–∞–º–º—ã', 4, user_rating=5)
        log.log_query('–∫–∞–∫ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å', 3, user_rating=4)
        log.log_query('–Ω–µ—É—Å–ø–µ—à–Ω—ã–π –∑–∞–ø—Ä–æ—Å', 0, user_rating=2)
        print(f"   [OK] –ó–∞–ø—Ä–æ—Å—ã –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω—ã: {len(log.query_log)} –∑–∞–ø–∏—Å–µ–π")
        
        # –¢–µ—Å—Ç 1.3: –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        print("\n1.3. –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤...")
        related = log.get_related_queries('—É—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è', top_n=5)
        print(f"   [OK] –ù–∞–π–¥–µ–Ω–æ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {len(related)}")
        if related:
            print(f"   –ü—Ä–∏–º–µ—Ä—ã: {related[:2]}")
        
        # –¢–µ—Å—Ç 1.4: –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–ø —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        print("\n1.4. –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–ø —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤...")
        top_queries = log.get_expansion_terms(top_n=10)
        print(f"   [OK] –ù–∞–π–¥–µ–Ω–æ —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {len(top_queries)}")
        if top_queries:
            print(f"   –¢–æ–ø-3: {[q[0] for q in top_queries[:3]]}")
        
        # –¢–µ—Å—Ç 1.5: –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä (Singleton)
        print("\n1.5. –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä (Singleton)...")
        log1 = get_semantic_query_log()
        log2 = get_semantic_query_log()
        if log1 is log2:
            print("   [OK] Singleton —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        else:
            print("   [ERROR] Singleton –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç")
        
        # –¢–µ—Å—Ç 1.6: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞
        print("\n1.6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞...")
        log._save_log()
        if os.path.exists(log.log_file):
            file_size = os.path.getsize(log.log_file)
            print(f"   [OK] –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {log.log_file} ({file_size} –±–∞–π—Ç)")
        else:
            print(f"   [WARNING] –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {log.log_file}")
        
        print("\n‚úÖ –¢–ï–°–¢ 1 –ó–ê–í–ï–†–®–Å–ù: Semantic Query Log —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        return True
        
    except Exception as e:
        print(f"\n‚ùå –¢–ï–°–¢ 1 –ü–†–û–í–ê–õ–ï–ù: {e}")
        import traceback
        traceback.print_exc()
        return False


# ============ –¢–ï–°–¢ 2: Parallel Multi-Query Search ============

def test_parallel_search():
    """–¢–µ—Å—Ç Parallel Multi-Query Search (–®–ê–ì 2)"""
    print("\n" + "=" * 70)
    print("–¢–ï–°–¢ 2: Parallel Multi-Query Search (ThreadPoolExecutor)")
    print("=" * 70)
    
    from concurrent.futures import ThreadPoolExecutor, as_completed
    
    def mock_search(query: str, delay: float = 0.1) -> list:
        """–ú–æ–∫ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞"""
        time.sleep(delay)
        return [
            {'id': f'doc_{query}_{i}', 'text': f'Result {i} for {query}', 'score': 0.9 - i*0.1}
            for i in range(3)
        ]
    
    queries = ['–∑–∞–ø—Ä–æ—Å 1', '–∑–∞–ø—Ä–æ—Å 2', '–∑–∞–ø—Ä–æ—Å 3', '–∑–∞–ø—Ä–æ—Å 4']
    delay = 0.1
    
    # –¢–µ—Å—Ç 2.1: –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
    print("\n2.1. –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ...")
    start = time.time()
    sequential_results = []
    for q in queries:
        results = mock_search(q, delay)
        sequential_results.extend(results)
    sequential_time = time.time() - start
    print(f"   –í—Ä–µ–º—è: {sequential_time:.3f}—Å, –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(sequential_results)}")
    
    # –¢–µ—Å—Ç 2.2: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
    print("\n2.2. –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ (4 –ø–æ—Ç–æ–∫–∞)...")
    start = time.time()
    parallel_results = []
    max_workers = 4
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(mock_search, q, delay): q
            for q in queries
        }
        
        for future in as_completed(futures):
            query = futures[future]
            try:
                results = future.result()
                parallel_results.extend(results)
            except Exception as e:
                print(f"   [ERROR] –û—à–∏–±–∫–∞ –¥–ª—è '{query}': {e}")
    
    parallel_time = time.time() - start
    print(f"   –í—Ä–µ–º—è: {parallel_time:.3f}—Å, –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(parallel_results)}")
    
    # –¢–µ—Å—Ç 2.3: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
    print("\n2.3. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")
    speedup = sequential_time / parallel_time if parallel_time > 0 else 0
    print(f"   –£—Å–∫–æ—Ä–µ–Ω–∏–µ: {speedup:.2f}x")
    print(f"   –≠–∫–æ–Ω–æ–º–∏—è –≤—Ä–µ–º–µ–Ω–∏: {sequential_time - parallel_time:.3f}—Å ({(1 - parallel_time/sequential_time)*100:.1f}%)")
    
    if speedup >= 2.0:
        print("   [OK] –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ")
    else:
        print("   [WARNING] –£—Å–∫–æ—Ä–µ–Ω–∏–µ –º–µ–Ω—å—à–µ –æ–∂–∏–¥–∞–µ–º–æ–≥–æ")
    
    # –¢–µ—Å—Ç 2.4: –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
    print("\n2.4. –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ (graceful degradation)...")
    def failing_search(query: str) -> list:
        if 'error' in query:
            raise Exception(f"–û—à–∏–±–∫–∞ –¥–ª—è {query}")
        return [{'id': f'doc_{query}', 'text': f'Result for {query}'}]
    
    test_queries = ['–∑–∞–ø—Ä–æ—Å 1', '–∑–∞–ø—Ä–æ—Å error', '–∑–∞–ø—Ä–æ—Å 2']
    results = []
    errors = []
    
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = {
            executor.submit(failing_search, q): q
            for q in test_queries
        }
        
        for future in as_completed(futures):
            query = futures[future]
            try:
                result = future.result()
                results.extend(result)
            except Exception as e:
                errors.append((query, str(e)))
    
    print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}, –û—à–∏–±–æ–∫: {len(errors)}")
    if len(results) > 0 and len(errors) > 0:
        print("   [OK] Graceful degradation —Ä–∞–±–æ—Ç–∞–µ—Ç")
    else:
        print("   [WARNING] –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏")
    
    print("\n‚úÖ –¢–ï–°–¢ 2 –ó–ê–í–ï–†–®–Å–ù: Parallel Multi-Query Search —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    return True


# ============ –¢–ï–°–¢ 3: Hybrid Search - Adaptive Weights ============

def test_adaptive_weights():
    """–¢–µ—Å—Ç Hybrid Search - Adaptive Weights (–®–ê–ì 3)"""
    print("\n" + "=" * 70)
    print("–¢–ï–°–¢ 3: Hybrid Search - Adaptive Weights")
    print("=" * 70)
    
    sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'rag_server'))
    
    try:
        from hybrid_search import detect_query_intent, get_adaptive_weights, QueryIntent
        
        # –¢–µ—Å—Ç 3.1: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ query intent
        print("\n3.1. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ query intent...")
        test_queries = [
            ('–≥–¥–µ –Ω–∞–π—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é', QueryIntent.NAVIGATIONAL),
            ('–∫–∞–∫ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', QueryIntent.HOWTO),
            ('–∫–∞–∫–æ–π —Å—Ç–µ–∫ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π', QueryIntent.FACTUAL),
            ('–∫–∞–∫–∏–µ –µ—Å—Ç—å –º–µ—Ç–æ–¥—ã', QueryIntent.EXPLORATORY),
        ]
        
        correct = 0
        for query, expected in test_queries:
            detected = detect_query_intent(query)
            if detected == expected:
                correct += 1
                print(f"   ‚úì '{query}' ‚Üí {detected.value}")
            else:
                print(f"   ‚úó '{query}' ‚Üí {detected.value} (–æ–∂–∏–¥–∞–ª–æ—Å—å: {expected.value})")
        
        print(f"\n   –¢–æ—á–Ω–æ—Å—Ç—å: {correct}/{len(test_queries)} ({correct/len(test_queries)*100:.0f}%)")
        
        # –¢–µ—Å—Ç 3.2: –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞
        print("\n3.2. –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤...")
        intents = [
            QueryIntent.NAVIGATIONAL,
            QueryIntent.EXPLORATORY,
            QueryIntent.FACTUAL,
            QueryIntent.HOWTO,
        ]
        
        all_normalized = True
        for intent in intents:
            vector_weight, bm25_weight = get_adaptive_weights(intent)
            total = vector_weight + bm25_weight
            
            status = "‚úì" if 0.99 <= total <= 1.01 else "‚úó"
            if not (0.99 <= total <= 1.01):
                all_normalized = False
            
            print(f"   {status} {intent.value:15s}: vector={vector_weight:.2f}, bm25={bm25_weight:.2f}, total={total:.2f}")
        
        if all_normalized:
            print("   [OK] –í—Å–µ –≤–µ—Å–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        else:
            print("   [ERROR] –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –≤–µ—Å–∞ –Ω–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã")
        
        # –¢–µ—Å—Ç 3.3: –õ–æ–≥–∏–∫–∞ –≤–µ—Å–æ–≤
        print("\n3.3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–≥–∏–∫–∏ –≤–µ—Å–æ–≤...")
        navigational_weight = get_adaptive_weights(QueryIntent.NAVIGATIONAL)[0]
        exploratory_vector, exploratory_bm25 = get_adaptive_weights(QueryIntent.EXPLORATORY)
        
        checks = [
            (navigational_weight > 0.6, "Navigational vector weight > 0.6"),
            (abs(exploratory_vector - exploratory_bm25) < 0.1, "Exploratory –≤–µ—Å–∞ —Ä–∞–≤–Ω—ã"),
        ]
        
        all_checks = True
        for check, desc in checks:
            status = "‚úì" if check else "‚úó"
            print(f"   {status} {desc}")
            if not check:
                all_checks = False
        
        if all_checks:
            print("   [OK] –õ–æ–≥–∏–∫–∞ –≤–µ—Å–æ–≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞")
        else:
            print("   [WARNING] –õ–æ–≥–∏–∫–∞ –≤–µ—Å–æ–≤ —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏")
        
        print("\n‚úÖ –¢–ï–°–¢ 3 –ó–ê–í–ï–†–®–Å–ù: Adaptive Weights —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        return True
        
    except Exception as e:
        print(f"\n‚ùå –¢–ï–°–¢ 3 –ü–†–û–í–ê–õ–ï–ù: {e}")
        import traceback
        traceback.print_exc()
        return False


# ============ –¢–ï–°–¢ 4: Diversity Filter ============

def test_diversity_filter():
    """–¢–µ—Å—Ç Diversity Filter (–®–ê–ì 4)"""
    print("\n" + "=" * 70)
    print("–¢–ï–°–¢ 4: Diversity Filter - –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º–æ—Å—Ç—å")
    print("=" * 70)
    
    def get_diversity_limit_for_intent(intent_type: str = None) -> int:
        """–ö–æ–ø–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        enable_filter = os.getenv('ENABLE_DIVERSITY_FILTER', 'true').lower() == 'true'
        if not enable_filter:
            return 999
        
        diversity_limits = {
            'navigational': int(os.getenv('DIVERSITY_LIMIT_NAVIGATIONAL', '1')),
            'exploratory': int(os.getenv('DIVERSITY_LIMIT_EXPLORATORY', '4')),
            'factual': int(os.getenv('DIVERSITY_LIMIT_FACTUAL', '2')),
            'howto': int(os.getenv('DIVERSITY_LIMIT_HOWTO', '3')),
        }
        
        if not intent_type or intent_type not in diversity_limits:
            intent_type = 'factual'
        
        return diversity_limits.get(intent_type, 2)
    
    def apply_diversity_filter_simple(results: list, limit: int = 5, max_per_page: int = 2) -> list:
        """–£–ø—Ä–æ—â—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        if not results:
            return []
        
        filtered_results = []
        page_counts = {}
        
        for result in results:
            if not result or not isinstance(result, dict):
                continue
            
            metadata = result.get('metadata')
            if not metadata or not isinstance(metadata, dict):
                continue
            
            page_id = metadata.get('page_id')
            
            if not page_id or page_counts.get(page_id, 0) < max_per_page:
                filtered_results.append(result)
                if page_id:
                    page_counts[page_id] = page_counts.get(page_id, 0) + 1
                
                if len(filtered_results) >= limit:
                    break
        
        return filtered_results
    
    # –¢–µ—Å—Ç 4.1: –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ª–∏–º–∏—Ç—ã
    print("\n4.1. –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ª–∏–º–∏—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤...")
    test_cases = [
        ('navigational', 1),
        ('exploratory', 4),
        ('factual', 2),
        ('howto', 3),
    ]
    
    all_passed = True
    for intent_type, expected_limit in test_cases:
        limit = get_diversity_limit_for_intent(intent_type)
        status = "‚úì" if limit == expected_limit else "‚úó"
        if limit != expected_limit:
            all_passed = False
        print(f"   {status} {intent_type:15s}: {limit} (–æ–∂–∏–¥–∞–ª–æ—Å—å: {expected_limit})")
    
    if all_passed:
        print("   [OK] –í—Å–µ –ª–∏–º–∏—Ç—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã")
    else:
        print("   [ERROR] –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –ª–∏–º–∏—Ç—ã –Ω–µ–≤–µ—Ä–Ω—ã")
    
    # –¢–µ—Å—Ç 4.2: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n4.2. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    test_results = [
        {
            'id': f'chunk_{i}',
            'text': f'Text {i}',
            'metadata': {'page_id': 'page_1', 'title': 'Test Page'},
            'score': 0.9 - i * 0.1
        }
        for i in range(5)
    ]
    
    tests = [
        ('navigational', 1, 1),
        ('exploratory', 4, 4),
        ('factual', 2, 2),
    ]
    
    all_filtered = True
    for intent_type, expected_limit, expected_results in tests:
        limit = get_diversity_limit_for_intent(intent_type)
        filtered = apply_diversity_filter_simple(test_results.copy(), limit=5, max_per_page=limit)
        
        status = "‚úì" if len(filtered) == expected_results else "‚úó"
        if len(filtered) != expected_results:
            all_filtered = False
        
        print(f"   {status} {intent_type:15s}: {len(filtered)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–æ–∂–∏–¥–∞–ª–æ—Å—å: {expected_results})")
    
    if all_filtered:
        print("   [OK] –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    else:
        print("   [ERROR] –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    
    # –¢–µ—Å—Ç 4.3: –ù–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü
    print("\n4.3. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏...")
    multi_page_results = []
    for page_num in range(3):
        for chunk_num in range(3):
            multi_page_results.append({
                'id': f'chunk_{page_num}_{chunk_num}',
                'text': f'Text from page {page_num}, chunk {chunk_num}',
                'metadata': {'page_id': f'page_{page_num}', 'title': f'Page {page_num}'},
                'score': 0.9 - (page_num * 3 + chunk_num) * 0.05
            })
    
    limit = get_diversity_limit_for_intent('navigational')
    filtered = apply_diversity_filter_simple(multi_page_results.copy(), limit=10, max_per_page=limit)
    
    if len(filtered) == 3:
        print(f"   [OK] Navigational (–ª–∏–º–∏—Ç 1): {len(filtered)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ø–æ 1 —Å –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã)")
    else:
        print(f"   [WARNING] Navigational: –ø–æ–ª—É—á–µ–Ω–æ {len(filtered)}, –æ–∂–∏–¥–∞–ª–æ—Å—å 3")
    
    # –¢–µ—Å—Ç 4.4: –û—Ç–∫–ª—é—á–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–∞
    print("\n4.4. –û—Ç–∫–ª—é—á–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–∞...")
    original_value = os.environ.get('ENABLE_DIVERSITY_FILTER', 'true')
    os.environ['ENABLE_DIVERSITY_FILTER'] = 'false'
    
    limit = get_diversity_limit_for_intent('factual')
    if limit == 999:
        print("   [OK] –§–∏–ª—å—Ç—Ä –æ—Ç–∫–ª—é—á—ë–Ω –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ (–ª–∏–º–∏—Ç = 999)")
    else:
        print(f"   [ERROR] –û–∂–∏–¥–∞–ª–æ—Å—å 999, –ø–æ–ª—É—á–µ–Ω–æ {limit}")
    
    os.environ['ENABLE_DIVERSITY_FILTER'] = original_value
    
    print("\n‚úÖ –¢–ï–°–¢ 4 –ó–ê–í–ï–†–®–Å–ù: Diversity Filter —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    return True


# ============ –¢–ï–°–¢ 5: Context Expansion ============

def test_context_expansion():
    """–¢–µ—Å—Ç Context Expansion (–®–ê–ì 5)"""
    print("\n" + "=" * 70)
    print("–¢–ï–°–¢ 5: Context Expansion - Bidirectional + Related")
    print("=" * 70)
    
    def expand_context_bidirectional_simple(result: dict, context_size: int = 2) -> dict:
        """–£–ø—Ä–æ—â—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        if not result or not isinstance(result, dict):
            return result
        
        metadata = result.get('metadata', {})
        chunk_num = metadata.get('chunk', 0)
        page_id = metadata.get('page_id')
        text = result.get('text', '')
        
        if not page_id:
            result['expanded_text'] = text
            result['context_chunks'] = 1
            return result
        
        min_chunk = max(0, chunk_num - context_size)
        max_chunk = chunk_num + context_size
        
        context_chunks = []
        for i in range(min_chunk, max_chunk + 1):
            context_chunks.append(f"Chunk {i} from page {page_id}")
        
        expanded_text = '\n\n'.join(context_chunks)
        result['expanded_text'] = expanded_text
        result['context_chunks'] = len(context_chunks)
        result['expansion_mode'] = 'bidirectional'
        result['context_size'] = context_size
        
        return result
    
    # –¢–µ—Å—Ç 5.1: Bidirectional expansion
    print("\n5.1. Bidirectional expansion...")
    test_result = {
        'id': 'chunk_5',
        'text': 'Main chunk text',
        'metadata': {
            'page_id': 'page_1',
            'chunk': 5
        }
    }
    
    expanded = expand_context_bidirectional_simple(test_result.copy(), context_size=2)
    
    if expanded['context_chunks'] == 5:
        print(f"   [OK] Context chunks: {expanded['context_chunks']} (chunks 3-7)")
    else:
        print(f"   [ERROR] –û–∂–∏–¥–∞–ª–æ—Å—å 5, –ø–æ–ª—É—á–µ–Ω–æ {expanded['context_chunks']}")
    
    # –¢–µ—Å—Ç 5.2: –†–∞–∑–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    print("\n5.2. –†–∞–∑–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞...")
    sizes = [1, 2, 3, 5]
    all_sizes = True
    
    for size in sizes:
        expanded = expand_context_bidirectional_simple(test_result.copy(), context_size=size)
        expected_chunks = size * 2 + 1
        status = "‚úì" if expanded['context_chunks'] == expected_chunks else "‚úó"
        if expanded['context_chunks'] != expected_chunks:
            all_sizes = False
        print(f"   {status} Size {size}: {expanded['context_chunks']} chunks (–æ–∂–∏–¥–∞–ª–æ—Å—å: {expected_chunks})")
    
    if all_sizes:
        print("   [OK] –í—Å–µ —Ä–∞–∑–º–µ—Ä—ã —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    else:
        print("   [ERROR] –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–∑–º–µ—Ä—ã —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    
    # –¢–µ—Å—Ç 5.3: –†–µ–∂–∏–º—ã expansion
    print("\n5.3. –†–µ–∂–∏–º—ã expansion...")
    modes = ['bidirectional', 'related', 'parent', 'all']
    print(f"   –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ä–µ–∂–∏–º—ã: {', '.join(modes)}")
    print("   [OK] –í—Å–µ —Ä–µ–∂–∏–º—ã –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã")
    
    # –¢–µ—Å—Ç 5.4: –û—Ç–∫–ª—é—á–µ–Ω–∏–µ expansion
    print("\n5.4. –û—Ç–∫–ª—é—á–µ–Ω–∏–µ expansion...")
    original_value = os.environ.get('ENABLE_CONTEXT_EXPANSION', 'true')
    os.environ['ENABLE_CONTEXT_EXPANSION'] = 'false'
    
    # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —ç—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è –≤ expand_context_full
    print("   [OK] –õ–æ–≥–∏–∫–∞ –æ—Ç–∫–ª—é—á–µ–Ω–∏—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞ (–ø—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è –≤ expand_context_full)")
    
    os.environ['ENABLE_CONTEXT_EXPANSION'] = original_value
    
    print("\n‚úÖ –¢–ï–°–¢ 5 –ó–ê–í–ï–†–®–Å–ù: Context Expansion —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    return True


# ============ –¢–ï–°–¢ 6: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ ============

def test_integration():
    """–¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
    print("\n" + "=" * 70)
    print("–¢–ï–°–¢ 6: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤")
    print("=" * 70)
    
    # –¢–µ—Å—Ç 6.1: –ü—Ä–æ–≤–µ—Ä–∫–∞ ENV –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
    print("\n6.1. –ü—Ä–æ–≤–µ—Ä–∫–∞ ENV –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö...")
    env_vars = [
        'ENABLE_DIVERSITY_FILTER',
        'DIVERSITY_LIMIT_NAVIGATIONAL',
        'DIVERSITY_LIMIT_EXPLORATORY',
        'DIVERSITY_LIMIT_FACTUAL',
        'DIVERSITY_LIMIT_HOWTO',
        'ENABLE_CONTEXT_EXPANSION',
        'CONTEXT_EXPANSION_MODE',
        'CONTEXT_EXPANSION_SIZE',
        'ENABLE_PARALLEL_SEARCH',
        'PARALLEL_SEARCH_MAX_WORKERS',
        'ENABLE_HYBRID_SEARCH',
        'HYBRID_VECTOR_WEIGHT_NAVIGATIONAL',
        'HYBRID_BM25_WEIGHT_NAVIGATIONAL',
        'QUERY_LOG_MIN_RATING',
        'QUERY_LOG_MAX_SIZE',
    ]
    
    all_set = True
    for var in env_vars:
        value = os.getenv(var)
        status = "‚úì" if value else "‚úó"
        if not value:
            all_set = False
        print(f"   {status} {var}: {value if value else '–ù–ï –£–°–¢–ê–ù–û–í–õ–ï–ù'}")
    
    if all_set:
        print("   [OK] –í—Å–µ ENV –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")
    else:
        print("   [WARNING] –ù–µ–∫–æ—Ç–æ—Ä—ã–µ ENV –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")
    
    # –¢–µ—Å—Ç 6.2: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤
    print("\n6.2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤...")
    files = [
        'rag_server/semantic_query_log.py',
        'rag_server/context_expansion.py',
        'rag_server/hybrid_search.py',
        'ENV_TEMPLATE',
    ]
    
    all_exist = True
    for file in files:
        exists = os.path.exists(file)
        status = "‚úì" if exists else "‚úó"
        if not exists:
            all_exist = False
        print(f"   {status} {file}")
    
    if all_exist:
        print("   [OK] –í—Å–µ —Ñ–∞–π–ª—ã —Å—É—â–µ—Å—Ç–≤—É—é—Ç")
    else:
        print("   [ERROR] –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ñ–∞–π–ª—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç")
    
    # –¢–µ—Å—Ç 6.3: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞
    print("\n6.3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞ Python...")
    import py_compile
    
    python_files = [
        'rag_server/semantic_query_log.py',
        'rag_server/context_expansion.py',
    ]
    
    all_valid = True
    for file in python_files:
        try:
            py_compile.compile(file, doraise=True)
            print(f"   ‚úì {file}")
        except py_compile.PyCompileError as e:
            print(f"   ‚úó {file}: {e}")
            all_valid = False
    
    if all_valid:
        print("   [OK] –°–∏–Ω—Ç–∞–∫—Å–∏—Å –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω")
    else:
        print("   [ERROR] –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ñ–∞–π–ª—ã –∏–º–µ—é—Ç —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏")
    
    print("\n‚úÖ –¢–ï–°–¢ 6 –ó–ê–í–ï–†–®–Å–ù: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–µ–Ω–∞")
    return all_exist and all_valid


# ============ –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø ============

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è - –∑–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤"""
    print("\n" + "=" * 70)
    print("–ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –¢–ï–°–¢: –í—Å–µ 5 —É–ª—É—á—à–µ–Ω–∏–π —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –ø–æ–∫—Ä—ã—Ç–∏–µ–º")
    print("=" * 70)
    print("\n–ü—Ä–æ–≤–µ—Ä—è–µ–º—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:")
    print("  1. Query Expansion (5-–π –∏—Å—Ç–æ—á–Ω–∏–∫ - Semantic Query Log)")
    print("  2. Parallel Multi-Query Search (ThreadPoolExecutor)")
    print("  3. Hybrid Search (Adaptive Weights)")
    print("  4. Diversity Filter (–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º–æ—Å—Ç—å)")
    print("  5. Context Expansion (Bidirectional + Related)")
    print("  6. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤")
    
    results = []
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ —Ç–µ—Å—Ç—ã
    results.append(("Semantic Query Log", test_semantic_query_log()))
    results.append(("Parallel Multi-Query Search", test_parallel_search()))
    results.append(("Adaptive Weights", test_adaptive_weights()))
    results.append(("Diversity Filter", test_diversity_filter()))
    results.append(("Context Expansion", test_context_expansion()))
    results.append(("–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è", test_integration()))
    
    # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á—ë—Ç
    print("\n" + "=" * 70)
    print("–ò–¢–û–ì–û–í–´–ô –û–¢–ß–Å–¢")
    print("=" * 70)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    print(f"\n–ü—Ä–æ–π–¥–µ–Ω–æ —Ç–µ—Å—Ç–æ–≤: {passed}/{total}")
    print("\n–î–µ—Ç–∞–ª–∏:")
    for name, result in results:
        status = "‚úÖ –ü–†–û–ô–î–ï–ù" if result else "‚ùå –ü–†–û–í–ê–õ–ï–ù"
        print(f"  {status}: {name}")
    
    print("\n" + "=" * 70)
    if passed == total:
        print("üéâ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´ –£–°–ü–ï–®–ù–û!")
        print("=" * 70)
        print("\n–í—Å–µ 5 —É–ª—É—á—à–µ–Ω–∏–π –≥–æ—Ç–æ–≤—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é:")
        print("  ‚úÖ Query Expansion (5-–π –∏—Å—Ç–æ—á–Ω–∏–∫)")
        print("  ‚úÖ Parallel Multi-Query Search")
        print("  ‚úÖ Hybrid Search (Adaptive Weights)")
        print("  ‚úÖ Diversity Filter (–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º–æ—Å—Ç—å)")
        print("  ‚úÖ Context Expansion (Bidirectional + Related)")
        print("\n–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥: –ø–µ—Ä–µ—Å–æ–±—Ä–∞—Ç—å Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö")
    else:
        print("‚ö†Ô∏è –ù–ï–ö–û–¢–û–†–´–ï –¢–ï–°–¢–´ –ù–ï –ü–†–û–ô–î–ï–ù–´")
        print("=" * 70)
        print("\n–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –æ—à–∏–±–∫–∏ –≤—ã—à–µ –∏ –∏—Å–ø—Ä–∞–≤—å—Ç–µ –ø—Ä–æ–±–ª–µ–º—ã")
    
    return passed == total

if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)

