"""
MCP —Å–µ—Ä–≤–µ—Ä –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ Confluence.
–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è Open WebUI —á–µ—Ä–µ–∑ Model Context Protocol.
"""
from typing import Optional, Tuple, Any, List, Dict
import logging
import os
import re
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

from fastmcp import FastMCP
from qdrant_client import QdrantClient

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑ ENV
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
logging.basicConfig(
    level=getattr(logging, LOG_LEVEL, logging.INFO),
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# –û—Ç–∫–ª—é—á–∞–µ–º –∏–∑–±—ã—Ç–æ—á–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ HTTP –∑–∞–ø—Ä–æ—Å–æ–≤ –æ—Ç httpx/openai
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)
logging.getLogger("openai").setLevel(logging.WARNING)

# Qdrant configuration
QDRANT_HOST = os.getenv("QDRANT_HOST", "qdrant")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", "6333"))
QDRANT_COLLECTION = os.getenv("QDRANT_COLLECTION", "confluence")
# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–æ–¥—É–ª—å embeddings
from embeddings import (
    get_embed_model,
    generate_query_embedding,
    generate_query_embeddings_batch,
    get_embedding_dimension,
    EMBED_MODEL,
    USE_OLLAMA,
    OLLAMA_URL
)

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ –º–æ–¥—É–ª–∏ –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –ø–æ–∏—Å–∫–∞
from synonyms_manager import get_synonyms_manager
from semantic_cache import get_semantic_cache
from advanced_search import (
    pseudo_relevance_feedback,
    get_fallback_search,
    extract_keywords
)
from query_rewriter import cached_rewrite_query, get_rewriter_stats
from hybrid_search import hybrid_search, init_bm25_retriever
from context_expansion import expand_context_full

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è reranker (–ª–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è)
reranker = None

def init_reranker():
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è CrossEncoder –¥–ª—è reranking.
    
    –ú–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ –∏ –∫—ç—à–∏—Ä—É–µ—Ç—Å—è –≥–ª–æ–±–∞–ª—å–Ω–æ.
    –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏ (‚âà30-40 —Å–µ–∫) –ø—Ä–∏ –∫–∞–∂–¥–æ–º –∑–∞–ø—Ä–æ—Å–µ.
    
    –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Russian MS-MARCO –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å —Ä—É—Å—Å–∫–∏–º —è–∑—ã–∫–æ–º.
    –ú–æ–¥–µ–ª—å –º–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è RE_RANKER_MODEL.
    """
    global reranker
    if reranker is None:
        try:
            start_time = time.time()
            from sentence_transformers import CrossEncoder
            
            # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ ENV –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç (Russian MS-MARCO)
            model_name = os.getenv(
                'RE_RANKER_MODEL',
                'DiTy/cross-encoder-russian-msmarco'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é Russian MS-MARCO
            )
            
            logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è CrossEncoder –¥–ª—è reranking...")
            logger.info(f"  –ú–æ–¥–µ–ª—å: {model_name}")
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö
            model_info = {
                'BAAI/bge-reranker-v2-m3': {
                    'name': 'BAAI bge-reranker-v2-m3',
                    'language': '100+ —è–∑—ã–∫–æ–≤ (–≤–∫–ª—é—á–∞—è —Ä—É—Å—Å–∫–∏–π)',
                    'quality': '95%+ –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤'
                },
                'DiTy/cross-encoder-russian-msmarco': {
                    'name': 'Russian MS-MARCO',
                    'language': '–†—É—Å—Å–∫–∏–π',
                    'quality': '92% –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ'
                },
                'cross-encoder/ms-marco-MiniLM-L-6-v2': {
                    'name': 'MS-MARCO MiniLM',
                    'language': '–ê–Ω–≥–ª–∏–π—Å–∫–∏–π (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è)',
                    'quality': '85% –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ'
                },
                'Qwen/Qwen3-Reranker-8B': {
                    'name': 'Qwen3 Reranker',
                    'language': '100+ —è–∑—ã–∫–æ–≤',
                    'quality': '95% (—Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ —Ä–µ—Å—É—Ä—Å–æ–≤)'
                }
            }
            
            info = model_info.get(model_name, {})
            if info:
                logger.info(f"  –ù–∞–∑–≤–∞–Ω–∏–µ: {info.get('name', model_name)}")
                logger.info(f"  –Ø–∑—ã–∫: {info.get('language', 'N/A')}")
                logger.info(f"  –ö–∞—á–µ—Å—Ç–≤–æ: {info.get('quality', 'N/A')}")
            
            reranker = CrossEncoder(model_name)
            elapsed = time.time() - start_time
            logger.info(f"‚úÖ CrossEncoder –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∑–∞ {elapsed:.1f}—Å. –ú–æ–¥–µ–ª—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∞.")
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å reranker: {e}")
            reranker = None
    else:
        logger.debug("–ü–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ CrossEncoder")
    return reranker

def extract_keywords(query: str) -> list[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ (—É–¥–∞–ª—è–µ—Ç —Å—Ç–æ–ø-—Å–ª–æ–≤–∞).
    
    Args:
        query: –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å
    
    Returns:
        –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    """
    # –†—É—Å—Å–∫–∏–µ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
    stop_words = {
        '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è', '–∫–∞–∫–∏–µ', '–≥–¥–µ', '–∫–∞–∫', '—á—Ç–æ', '—ç—Ç–æ', '–≤', '–Ω–∞', '–ø–æ',
        '–¥–ª—è', '—Å', '–∫', '–∏–∑', '–æ', '–æ–±', '–∏', '–∞', '–Ω–æ', '–∏–ª–∏', '–∂–µ',
        'the', 'is', 'at', 'which', 'on', 'in', 'a', 'an', 'and', 'or', 'but'
    }
    
    words = query.lower().split()
    keywords = [w for w in words if w not in stop_words and len(w) > 2]
    
    return keywords

def expand_query(query: str, space: str = "") -> list[str]:
    """
    –£–º–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å–∏–Ω–æ–Ω–∏–º–æ–≤.
    
    –ò—Å—Ç–æ—á–Ω–∏–∫–∏ —Å–∏–Ω–æ–Ω–∏–º–æ–≤ (–≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞):
    1. –ù–û–í–û–ï: Semantic Query Log (—É—Å–ø–µ—à–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π) ‚Üê –í–´–°–®–ò–ô –ü–†–ò–û–†–ò–¢–ï–¢!
    2. –ë–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å (50 –æ–±—â–∏—Ö IT-—Ç–µ—Ä–º–∏–Ω–æ–≤)
    3. –î–æ–º–µ–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑ Confluence)
    4. –í—ã—É—á–µ–Ω–Ω—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã (Query Mining)
    5. Ollama (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
    
    –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–ª–∏–Ω—ã –∑–∞–ø—Ä–æ—Å–∞.
    
    Args:
        query: –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        space: –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
    Returns:
        –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ (–æ—Ä–∏–≥–∏–Ω–∞–ª + –≤–∞—Ä–∏–∞–Ω—Ç—ã)
    """
    queries = [query]
    query_lower = query.lower().strip()
    
    # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –ø–æ –¥–ª–∏–Ω–µ –∑–∞–ø—Ä–æ—Å–∞
    query_length = len(query.split())
    if query_length <= 2:
        max_variants = 5  # –ö–æ—Ä–æ—Ç–∫–∏–π –∑–∞–ø—Ä–æ—Å ‚Üí –±–æ–ª—å—à–µ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –¥–ª—è –ø–æ–∫—Ä—ã—Ç–∏—è
    elif query_length <= 4:
        max_variants = 3  # –°—Ä–µ–¥–Ω–∏–π –∑–∞–ø—Ä–æ—Å ‚Üí —É–º–µ—Ä–µ–Ω–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
    else:
        max_variants = 2  # –î–ª–∏–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å ‚Üí –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ (—É–∂–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ–Ω)
    
    # === –ò–°–¢–û–ß–ù–ò–ö 1 (–í–´–°–®–ò–ô –ü–†–ò–û–†–ò–¢–ï–¢): Semantic Query Log (—É—Å–ø–µ—à–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π) ===
    semantic_log = None  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ Query Rewriting
    try:
        from semantic_query_log import get_semantic_query_log
        
        semantic_log = get_semantic_query_log()
        related_queries = semantic_log.get_related_queries(query, top_n=3)
        
        for related_query in related_queries:
            if related_query not in queries:
                queries.append(related_query)
                logger.debug(f"Semantic Query Log: –¥–æ–±–∞–≤–ª–µ–Ω –ø–æ—Ö–æ–∂–∏–π –∑–∞–ø—Ä–æ—Å '{related_query}'")
                
                if len(queries) >= max_variants:
                    break
        
        if related_queries:
            logger.debug(f"Semantic Query Log: –Ω–∞–π–¥–µ–Ω–æ {len(related_queries)} –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
    except Exception as e:
        logger.debug(f"Semantic Query Log –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
    
    # === –ò–°–¢–û–ß–ù–ò–ö 2-4: SynonymsManager (–±–∞–∑–æ–≤—ã–π + –¥–æ–º–µ–Ω–Ω—ã–µ + –≤—ã—É—á–µ–Ω–Ω—ã–µ) ===
    try:
        synonyms_manager = get_synonyms_manager()
        from synonyms_manager import TERM_BLACKLIST
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        keywords = extract_keywords(query)
        
        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞ –ø–æ–ª—É—á–∞–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã
        for keyword in keywords[:3]:  # –ú–∞–∫—Å–∏–º—É–º 3 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞
            keyword_lower = keyword.lower()
            
            # –ó–ê–©–ò–¢–ê: –ù–µ –∑–∞–º–µ–Ω—è–µ–º blacklist —Ç–µ—Ä–º–∏–Ω—ã (—Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–º–µ–Ω–∞, –Ω–∞–∑–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤)
            if keyword_lower in TERM_BLACKLIST:
                logger.debug(f"–ü—Ä–æ–ø—É—Å–∫–∞—é blacklist —Ç–µ—Ä–º–∏–Ω: {keyword}")
                continue
            
            synonyms = synonyms_manager.get_synonyms(keyword, max_synonyms=2)
            
            if synonyms:
                # –ó–∞–º–µ–Ω—è–µ–º –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –Ω–∞ —Å–∏–Ω–æ–Ω–∏–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º word boundaries
                for synonym in synonyms:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ–π –∑–∞–º–µ–Ω—ã —Ü–µ–ª–æ–≥–æ —Å–ª–æ–≤–∞
                    import re
                    pattern = r'\b' + re.escape(keyword_lower) + r'\b'
                    expanded = re.sub(pattern, synonym.lower(), query_lower, flags=re.IGNORECASE)
                    
                    if expanded != query_lower and expanded not in queries:
                        queries.append(expanded)
                        
                        if len(queries) >= max_variants:
                            break
            
            if len(queries) >= max_variants:
                break
                
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ SynonymsManager: {e}")
    
    # === –ò–°–¢–û–ß–ù–ò–ö 5: Query Rewriting (Ollama ‚Üí OpenRouter) ===
    try:
        rewrite_variants = cached_rewrite_query(query, semantic_log=semantic_log)
        for variant in rewrite_variants[1:]:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—ã–π (–æ—Ä–∏–≥–∏–Ω–∞–ª)
            if variant not in queries and len(queries) < max_variants:
                queries.append(variant)
                logger.debug(f"Query rewriting variant: {variant}")
                
                if len(queries) >= max_variants:
                    break
    except Exception as e:
        logger.warning(f"Query rewriting failed: {e}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –±–µ–∑ —Å—Ç–æ–ø-—Å–ª–æ–≤
    keywords = extract_keywords(query)
    if len(keywords) >= 2:
        clean_query = ' '.join(keywords)
        if clean_query not in queries:
            queries.append(clean_query)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
    if space and len(query_lower.split()) <= 5:  # –¢–æ–ª—å–∫–æ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        queries.append(f"{query} {space}")
    
    # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è 1–°/—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π —Ç–µ—Ä–º–∏–Ω—ã
    if any(term in query_lower for term in ['1—Å', '1c', '–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è']):
        normalized = query.replace('1–°', '1C').replace('1—Å', '1c')
        if normalized != query and normalized not in queries:
            queries.append(normalized)
    
    # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ü—Ä–∏–º–µ–Ω—è–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ª–∏–º–∏—Ç
    result = list(dict.fromkeys(queries))[:max_variants]
    
    if len(result) < len(queries):
        logger.debug(f"Query expansion –æ–≥—Ä–∞–Ω–∏—á–µ–Ω: {len(queries)} ‚Üí {len(result)} –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ (query_length={query_length})")
    
    return result

def calculate_optimal_candidate_limit(query: str, limit: int) -> int:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è reranking.
    
    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        limit: –ñ–µ–ª–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
    Returns:
        –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
    """
    query_words = len(query.split())
    
    if query_words <= 2:
        multiplier = 5  # –ö–æ—Ä–æ—Ç–∫–∏–π –∑–∞–ø—Ä–æ—Å ‚Üí –±–æ–ª—å—à–µ —à—É–º–∞, –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
    elif query_words <= 4:
        multiplier = 3  # –°—Ä–µ–¥–Ω–∏–π –∑–∞–ø—Ä–æ—Å
    else:
        multiplier = 2  # –î–ª–∏–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å ‚Üí —É–∂–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π
    
    return min(limit * multiplier, 50)  # –ú–∞–∫—Å–∏–º—É–º 50 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤

def detect_content_type(text: str) -> str:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ —Ç–µ–∫—Å—Ç–µ.
    
    Returns:
        'table' | 'list' | 'code' | 'plain'
    """
    import re
    
    # –¢–∞–±–ª–∏—Ü—ã: | col1 | col2 | –∏–ª–∏ —Å—Ç—Ä–æ–∫–∏ —Å —Ç–∞–±—É–ª—è—Ü–∏–µ–π
    if re.search(r'\|.*\|.*\|', text) or text.count('\t') > 5:
        return 'table'
    
    # –°–ø–∏—Å–∫–∏: 3+ —Å—Ç—Ä–æ–∫ –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö—Å—è —Å *, -, ‚Ä¢, —Ü–∏—Ñ—Ä
    list_lines = re.findall(r'^\s*[\*\-‚Ä¢][\s\)]|^\s*\d+[\.\)]', text, re.MULTILINE)
    if len(list_lines) >= 3:
        return 'list'
    
    # –ö–æ–¥: ```code``` –∏–ª–∏ 5+ —Å—Ç—Ä–æ–∫ —Å –æ—Ç—Å—Ç—É–ø–∞–º–∏
    if '```' in text or len(re.findall(r'^\s{4,}', text, re.MULTILINE)) >= 5:
        return 'code'
    
    return 'plain'


def format_search_results(results: List[Dict[str, Any]], query: str, limit: int) -> str:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –≤ —á–∏—Ç–∞–µ–º—ã–π —Ç–µ–∫—Å—Ç.
    
    Args:
        results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    
    Returns:
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    """
    if not results:
        return f"‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'"
    
    response = [f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:\n"]
    
    for i, r in enumerate(results[:limit], 1):
        if not r or not isinstance(r, dict):
            continue
        
        m = r.get('metadata', {})
        if not isinstance(m, dict):
            m = {}
        
        page_space = m.get('space', 'Unknown')
        page_url = m.get('url', '')
        # Breadcrumb –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
        breadcrumb = (r.get('breadcrumb') or 
                     m.get('page_path') or 
                     m.get('title') or 
                     '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
        chunk_num = m.get('chunk', 0)
        
        # –¢–µ–∫—Å—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        text = r.get('expanded_text', r.get('text', "[–¢–µ–∫—Å—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω]"))
        text_preview = extract_relevant_snippet(text, query, max_length=800)
        
        # Score –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        final_score = r.get('boosted_score', r.get('rerank_score', r.get('final_score', 0)))
        score_emoji = "üî•" if final_score > 0.5 else "‚≠ê" if final_score > 0.3 else "‚úì" if final_score > 0.1 else "¬∑"
        score_str = f"{score_emoji} {final_score:.3f}"
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        extra_info = []
        labels = m.get('labels', '')
        if labels:
            extra_info.append(f"üè∑Ô∏è {labels}")
        created_by = m.get('created_by', '')
        if created_by:
            extra_info.append(f"üë§ {created_by}")
        
        extra_str = " | ".join(extra_info)
        if extra_str:
            extra_str = f" | {extra_str}"
        
        # –¢–∏–ø –ø–æ–∏—Å–∫–∞
        search_type = r.get('search_type', 'semantic')
        search_type_str = "üîç structural" if search_type == 'structural' else "üîé semantic"
        
        response.append(
            f"[{i}] {search_type_str} üìç {breadcrumb}\n"
            f"    üìÅ {page_space} | Chunk #{chunk_num} | {score_str}{extra_str}\n"
            f"    üîó {page_url}\n"
            f"    üí¨ {text_preview}\n"
        )
    
    return "\n".join(response)

def extract_relevant_snippet(text: str, query: str, max_length: int = 400) -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∑–∞–ø—Ä–æ—Å–∞.
    –£–º–µ–µ—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å —Å–ø–∏—Å–∫–∏, —Ç–∞–±–ª–∏—Ü—ã –∏ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –∏—Ö –ø–æ–ª–Ω–æ—Å—Ç—å—é.
    
    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞
        
    Returns:
        –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞
    """
    if len(text) <= max_length:
        return text
    
    import re
    
    # –ù–û–í–û–ï: –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    content_type = detect_content_type(text)
    
    # –¢–∞–±–ª–∏—Ü—ã –∏ —Å–ø–∏—Å–∫–∏ –ù–ï –æ–±—Ä–µ–∑–∞–µ–º - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é (–¥–æ —Ä–∞–∑—É–º–Ω–æ–≥–æ –ª–∏–º–∏—Ç–∞)
    if content_type in ['table', 'list']:
        limit = max_length * 6  # –î–æ 2400 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        if len(text) <= limit:
            return text
        else:
            # –û–±—Ä–µ–∑–∞–µ–º, –Ω–æ –ø—ã—Ç–∞–µ–º—Å—è —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            return text[:limit] + "\n... (–æ–±—Ä–µ–∑–∞–Ω–æ)"
    
    # –î–ª—è –∫–æ–¥–∞ - —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π –ª–∏–º–∏—Ç
    if content_type == 'code':
        limit = max_length * 3  # –î–æ 1200 —Å–∏–º–≤–æ–ª–æ–≤
        if len(text) <= limit:
            return text
        else:
            return text[:limit] + "\n... (–æ–±—Ä–µ–∑–∞–Ω–æ)"
    
    # –î–ª—è –æ–±—ã—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ª–æ–≥–∏–∫–∞
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–ø–æ —Ç–æ—á–∫–∞–º, –≤–æ–ø—Ä–æ—Å–∞–º, –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏—è–º)
    sentences = re.split(r'[.!?]+', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    
    if not sentences:
        return text[:max_length] + "..."
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
    query_words = set(extract_keywords(query))
    
    # –ù–∞—Ö–æ–¥–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º overlap
    best_idx = 0
    best_score = 0
    
    for idx, sent in enumerate(sentences):
        sent_words = set(extract_keywords(sent))
        overlap = len(query_words & sent_words)
        
        if overlap > best_score:
            best_score = overlap
            best_idx = idx
    
    # –ë–µ—Ä–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ + –∫–æ–Ω—Ç–µ–∫—Å—Ç (1 –¥–æ, 1 –ø–æ—Å–ª–µ)
    start = max(0, best_idx - 1)
    end = min(len(sentences), best_idx + 2)
    snippet = '. '.join(sentences[start:end]).strip()
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
    if len(snippet) > max_length:
        snippet = snippet[:max_length] + "..."
    
    return snippet

def deduplicate_results(results: list) -> list:
    """
    –£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞.
    
    Args:
        results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    
    Returns:
        –°–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    if len(results) <= 1:
        return results
    
    # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ hash –ø–µ—Ä–≤—ã—Ö 200 —Å–∏–º–≤–æ–ª–æ–≤
    seen_signatures = set()
    unique_results = []
    
    for r in results:
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∫ —Å–∏–≥–Ω–∞—Ç—É—Ä—É
        text = r.get('text', '')
        signature = text[:200].strip()
        text_hash = hash(signature)
        
        if text_hash not in seen_signatures:
            seen_signatures.add(text_hash)
            unique_results.append(r)
        else:
            logger.debug(f"–£–¥–∞–ª–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç: {r['metadata'].get('title', 'Unknown')}")
    
    return unique_results

def expand_context_window(result: dict, window_size: int = 1) -> dict:
    """
    –†–∞—Å—à–∏—Ä—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —á–∞–Ω–∫–∞ —Å–æ—Å–µ–¥–Ω–∏–º–∏ —á–∞–Ω–∫–∞–º–∏.
    
    Context Window Retrieval - –ø–æ–ø—É–ª—è—Ä–Ω–∞—è —Ç–µ—Ö–Ω–∏–∫–∞ –∏–∑ LangChain –∏ LlamaIndex.
    –ù–∞—Ö–æ–¥–∏–º –º–∞–ª–µ–Ω—å–∫–∏–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —á–∞–Ω–∫, –Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–æ–ª—å—à–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.
    
    Args:
        result: –ù–∞–π–¥–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        window_size: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –¥–æ/–ø–æ—Å–ª–µ (1 = ¬±1 —á–∞–Ω–∫)
    
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
    """
    global qdrant_client
    
    if qdrant_client is None:
        return result
    
    try:
        if not result or not isinstance(result, dict):
            return result
        
        metadata = result.get('metadata')
        if not metadata or not isinstance(metadata, dict):
            return result
        
        chunk_num = metadata.get('chunk', 0)
        page_id = metadata.get('page_id')
        
        if not page_id:
            return result
        
        # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Å–æ—Å–µ–¥–Ω–∏–µ —á–∞–Ω–∫–∏ –∏–∑ —Ç–æ–π –∂–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        min_chunk = max(0, chunk_num - window_size)
        max_chunk = chunk_num + window_size
        
        # –ü–æ–ª—É—á–∞–µ–º —á–∞–Ω–∫–∏ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ
        from qdrant_storage import get_points_by_filter
        neighbors_raw = get_points_by_filter(
            where_filter={
                '$and': [
                    {'page_id': page_id},
                    {'chunk': {'$gte': min_chunk}},
                    {'chunk': {'$lte': max_chunk}}
                ]
            },
            limit=100,
            collection=QDRANT_COLLECTION
        )
        neighbors = {
            'documents': [r.get('text', '') for r in neighbors_raw],
            'metadatas': [r.get('metadata', {}) for r in neighbors_raw]
        }
        
        # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: neighbors –º–æ–∂–µ—Ç –±—ã—Ç—å None –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç—å None –ø–æ–ª—è
        if (neighbors and 
            neighbors.get('documents') and 
            neighbors.get('metadatas') and
            len(neighbors['documents']) == len(neighbors['metadatas'])):
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ chunk_num
            chunk_data = []
            for i, doc in enumerate(neighbors['documents']):
                if i < len(neighbors['metadatas']):
                    chunk_meta = neighbors['metadatas'][i]
                    if chunk_meta and isinstance(chunk_meta, dict):
                        chunk_data.append({
                            'chunk_num': chunk_meta.get('chunk', 0),
                            'text': doc if doc else ''
                        })
            
            chunk_data.sort(key=lambda x: x['chunk_num'])
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç—ã
            expanded_text = '\n\n'.join([c['text'] for c in chunk_data])
            result['expanded_text'] = expanded_text
            result['context_chunks'] = len(chunk_data)
            
            logger.debug(f"Context expanded: chunk {chunk_num} + {len(chunk_data)-1} neighbors")
        else:
            result['expanded_text'] = result.get('text', '')
            result['context_chunks'] = 1
            
    except Exception as e:
        logger.warning(f"Context expansion failed: {e}")
        result['expanded_text'] = result.get('text', '')
        result['context_chunks'] = 1
    
    return result

def calculate_hierarchy_boost(metadata: dict) -> float:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –±—É—Å—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–∑–∏—Ü–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –∏–µ—Ä–∞—Ä—Ö–∏–∏ Confluence.
    
    –£–õ–£–ß–®–ï–ù–ò–ï: –î–æ–±–∞–≤–ª–µ–Ω Metadata Boosting –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü.
    
    Hierarchy Boost - —Ç–µ—Ö–Ω–∏–∫–∞ –∏–∑ Elasticsearch –∏ Pinecone –¥–ª—è —É—á–µ—Ç–∞
    –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö –ø–æ–ª–æ–∂–µ–Ω–∏—è –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ.
    
    Args:
        metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        
    Returns:
        –ë—É—Å—Ç –æ—Ç -0.2 –¥–æ +0.8
    """
    boost = 0.0
    
    # 1. –ö–æ—Ä–Ω–µ–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –≤ space (–Ω–µ—Ç —Ä–æ–¥–∏—Ç–µ–ª—è) - —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ
    if not metadata.get('parent_title'):
        boost += 0.5
        logger.debug(f"Root page boost: +0.5 for {metadata.get('title')}")
    
    # 2. –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    title = metadata.get('title', '').lower()
    important_keywords = {
        '–æ–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è': 0.3,
        '–≥–ª–∞–≤–Ω–∞—è': 0.3,
        'readme': 0.3,
        'getting started': 0.3,
        '–Ω–∞—á–∞–ª–æ —Ä–∞–±–æ—Ç—ã': 0.3,
        '–æ–±–∑–æ—Ä': 0.2,
        '–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è': 0.2,
        '—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ': 0.2,
    }
    
    for keyword, value in important_keywords.items():
        if keyword in title:
            boost += value
            logger.debug(f"Title keyword boost: +{value} for '{keyword}'")
            break  # –¢–æ–ª—å–∫–æ –æ–¥–∏–Ω –±—É—Å—Ç –∑–∞ title
    
    # 3. –£—Ä–æ–≤–µ–Ω—å –∑–∞–≥–æ–ª–æ–≤–∫–∞ (h1 –≤–∞–∂–Ω–µ–µ h4)
    heading_level = metadata.get('heading_level', 0)
    if heading_level == 1:
        boost += 0.2
    elif heading_level == 2:
        boost += 0.1
    
    # 4. –ù–∞–ª–∏—á–∏–µ –º–µ—Ç–æ–∫ (labeled pages –æ–±—ã—á–Ω–æ –≤–∞–∂–Ω–µ–µ)
    labels_str = metadata.get('labels', '').lower()
    if labels_str:
        # –£–õ–£–ß–®–ï–ù–ò–ï: Metadata Boosting - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –±—É—Å—Ç –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç–æ–∫
        technical_labels = ['api', 'technical', '–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞', 'development', 
                           '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', '–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è', 'configuration', '–Ω–∞—Å—Ç—Ä–æ–π–∫–∞']
        
        has_technical_label = any(label in labels_str for label in technical_labels)
        if has_technical_label:
            boost += 0.3  # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π –±—É—Å—Ç –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
            logger.debug(f"Technical label boost: +0.3 for labels '{labels_str}'")
        else:
            boost += 0.05  # –ë–∞–∑–æ–≤—ã–π –±—É—Å—Ç –∑–∞ –Ω–∞–ª–∏—á–∏–µ –º–µ—Ç–æ–∫
    
    return min(boost, 0.8)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º—É–º

def calculate_breadcrumb_match_score(query: str, breadcrumb: str) -> float:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å breadcrumb (–ø—É—Ç–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã).
    
    Path Matching - —Ç–µ—Ö–Ω–∏–∫–∞ –∏–∑ semantic search –¥–ª—è —É—á–µ—Ç–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
    –∑–∞–ø—Ä–æ—Å–∞ —Å –∏–µ—Ä–∞—Ä—Ö–∏–µ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞.
    
    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        breadcrumb: –ü—É—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã (Space ‚Üí Parent ‚Üí Page ‚Üí Section)
        
    Returns:
        Score –æ—Ç 0.0 –¥–æ 1.0
    """
    if not breadcrumb:
        return 0.0
    
    query_words = set(extract_keywords(query))
    breadcrumb_words = set(extract_keywords(breadcrumb))
    
    if not query_words or not breadcrumb_words:
        return 0.0
    
    # Jaccard similarity
    intersection = len(query_words & breadcrumb_words)
    union = len(query_words | breadcrumb_words)
    
    score = intersection / union if union > 0 else 0.0
    
    if score > 0:
        logger.debug(f"Breadcrumb match: {score:.2f} ({intersection}/{union} words)")
    
    return score

# ========================================
# –°–¢–†–£–ö–¢–£–†–ù–´–ô –ü–û–ò–°–ö (Structural Navigation Search)
# ========================================

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
_metadata_cache = {}
_cache_timestamp = 0
_cache_ttl = 3600  # 1 —á–∞—Å

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
_structural_cache = {}
_structural_cache_timestamp = {}

def get_all_metadata_cached(collection: Any, ttl_seconds: int = 3600) -> Dict[str, Any]:
    """
    –ö—ç—à–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–ø—Ä–æ—Å–æ–≤.
    
    Args:
        collection: ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏—è
        ttl_seconds: –í—Ä–µ–º—è –∂–∏–∑–Ω–∏ –∫—ç—à–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    """
    global _metadata_cache, _cache_timestamp, _cache_ttl
    
    current_time = time.time()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
    if (_metadata_cache and 
        current_time - _cache_timestamp < ttl_seconds):
        logger.debug(f"‚úÖ Metadata cache hit: {len(_metadata_cache.get('ids', []))} items")
        return _metadata_cache
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à
    logger.info("üìä –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ metadata cache...")
    try:
        from qdrant_storage import get_all_points
        all_points = get_all_points(limit=10000, include_payload=True, collection=QDRANT_COLLECTION)
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ ChromaDB —Ñ–æ—Ä–º–∞—Ç
        all_data = {
            'ids': [p.get('id', '') for p in all_points.get('points', [])],
            'metadatas': [p.get('metadata', {}) for p in all_points.get('points', [])]
        }
        _metadata_cache = all_data
        _cache_timestamp = current_time
        _cache_ttl = ttl_seconds
        
        logger.info(f"‚úÖ Metadata cache updated: {len(all_data.get('ids', []))} items")
        return all_data
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è metadata cache: {e}")
        return _metadata_cache if _metadata_cache else {}

def parse_query_structure(query: str) -> Dict[str, Any]:
    """
    –ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∑–∞–ø—Ä–æ—Å–∞.
    
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–º (—Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏ >, ‚Üí)
    –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —á–∞—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞.
    
    –ü—Ä–∏–º–µ—Ä—ã:
    - "–°–∫–ª–∞–¥ > –£—á–µ—Ç –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã" ‚Üí structural
    - "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Å—Ç–µ–∫ RAUII" ‚Üí –æ–±—ã—á–Ω—ã–π
    - "–û–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ > –°–∫–ª–∞–¥ > –£—á–µ—Ç –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã" ‚Üí structural
    
    Args:
        query: –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –∑–∞–ø—Ä–æ—Å–∞
    """
    query_lower = query.lower().strip()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π –∏–µ—Ä–∞—Ä—Ö–∏–∏
    structural_separators = ['>', '‚Üí', '‚Üí', ' / ', ' | ']
    has_separator = any(sep in query for sep in structural_separators)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    # –£–õ–£–ß–®–ï–ù–û: –î–æ–±–∞–≤–ª–µ–Ω—ã –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è "–ø–æ –±–ª–æ–∫—É X, –∞ —Ç–æ—á–Ω–µ–µ Y"
    structural_patterns = [
        (r'–ø–æ\s+–±–ª–æ–∫—É\s+(\w+)(?:\s*,\s*–∞\s+—Ç–æ—á–Ω–µ–µ\s+)?([^\.]+)?', True),  # "–ø–æ –±–ª–æ–∫—É –°–∫–ª–∞–¥, –∞ —Ç–æ—á–Ω–µ–µ –£—á–µ—Ç –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã"
        (r'(\w+)\s*,\s*–∞\s+—Ç–æ—á–Ω–µ–µ\s+([^\.]+)', True),  # "–°–∫–ª–∞–¥, –∞ —Ç–æ—á–Ω–µ–µ –£—á–µ—Ç –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã"
        (r'–ø–æ\s+–±–ª–æ–∫—É\s+(\w+)', False),  # "–ø–æ –±–ª–æ–∫—É –°–∫–ª–∞–¥"
        (r'–≤\s+—Ä–∞–∑–¥–µ–ª–µ\s+(\w+)', False),  # "–≤ —Ä–∞–∑–¥–µ–ª–µ –£—á–µ—Ç"
        (r'–Ω–∞\s+—Å—Ç—Ä–∞–Ω–∏—Ü–µ\s+(\w+)', False),  # "–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –°–∫–ª–∞–¥"
    ]
    
    is_structural = has_separator
    parts = []
    
    if has_separator:
        # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º
        for sep in structural_separators:
            if sep in query:
                parts = [p.strip() for p in query.split(sep) if p.strip()]
                break
    else:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–≤–∫–ª—é—á–∞—è –Ω–æ–≤—ã–µ –¥–ª—è "–∞ —Ç–æ—á–Ω–µ–µ")
        for pattern, extract_parts in structural_patterns:
            match = re.search(pattern, query_lower)
            if match:
                is_structural = True
                if extract_parts:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –≥—Ä—É–ø–ø—ã –∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
                    groups = match.groups()
                    extracted_parts = [g.strip() for g in groups if g and g.strip()]
                    if len(extracted_parts) >= 2:
                        # –ù–∞—à–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω "X, –∞ —Ç–æ—á–Ω–µ–µ Y" - –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±–µ —á–∞—Å—Ç–∏
                        parts = extracted_parts
                    elif len(extracted_parts) == 1:
                        # –ù–∞—à–ª–∏ –ø–µ—Ä–≤—É—é —á–∞—Å—Ç—å, –∏—â–µ–º –≤—Ç–æ—Ä—É—é –ø–æ—Å–ª–µ "–∞ —Ç–æ—á–Ω–µ–µ"
                        after_match = re.search(r'–∞\s+—Ç–æ—á–Ω–µ–µ\s+([^\.]+)', query_lower)
                        if after_match:
                            parts = [extracted_parts[0], after_match.group(1).strip()]
                        else:
                            parts = extracted_parts
                    break
                else:
                    # –°—Ç–∞—Ä—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω - –∏–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—É—é —á–∞—Å—Ç—å
                    groups = match.groups()
                    if groups:
                        parts = [g.strip() for g in groups if g and g.strip()]
                    else:
                        parts = [query]
                    break
    
    result = {
        'is_structural_query': is_structural,
        'parts': parts if parts else [query],
        'original_query': query,
        'query_lower': query_lower
    }
    
    logger.debug(f"üîç Query structure: is_structural={is_structural}, parts={result['parts']}")
    
    return result

def structural_metadata_search(
    collection: Any,
    structure: Dict[str, Any],
    limit: int = 100
) -> List[Dict[str, Any]]:
    """
    –ü–æ–∏—Å–∫ –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∑–∞–ø—Ä–æ—Å–∞.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç in-memory —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é (ChromaDB –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç $contains –Ω–∞–ø—Ä—è–º—É—é).
    –ò—â–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤:
    - page_path (–ø–æ–ª–Ω—ã–π –ø—É—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã)
    - title (–∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã)
    - heading_path (–ø—É—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤)
    - heading (–∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ä–∞–∑–¥–µ–ª–∞)
    - parent_path (–ø—É—Ç—å —Ä–æ–¥–∏—Ç–µ–ª—è)
    
    Args:
        collection: ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏—è
        structure: –†–µ–∑—É–ª—å—Ç–∞—Ç parse_query_structure()
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    
    Returns:
        –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å match_score
    """
    if not structure['is_structural_query']:
        return []
    
    parts = structure['parts']
    
    # ============ –£–õ–£–ß–®–ï–ù–ò–ï: –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ ============
    search_start = time.time()
    logger.info(f"üîç –ù–∞—á–∞–ª–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: parts={parts}, limit={limit}")
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º –í–°–ï —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (—Å —Ä–∞–∑—É–º–Ω—ã–º –ª–∏–º–∏—Ç–æ–º)
        max_scan = min(limit * 10, 10000)  # –ù–µ –±–æ–ª–µ–µ 10K –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        
        fetch_start = time.time()
        from qdrant_storage import get_all_points
        all_points = get_all_points(limit=max_scan, include_payload=True, collection=QDRANT_COLLECTION)
        all_data = {
            'ids': [p.get('id', '') for p in all_points.get('points', [])],
            'documents': [p.get('text', '') for p in all_points.get('points', [])],
            'metadatas': [p.get('metadata', {}) for p in all_points.get('points', [])]
        }
        fetch_time = time.time() - fetch_start
        
        logger.debug(f"üìä –ü–æ–ª—É—á–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {len(all_data.get('ids', []))} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∑–∞ {fetch_time:.3f}—Å")
        
        if not all_data or not all_data.get('ids'):
            logger.debug("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞")
            return []
        
        formatted_results = []
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –≤ –ø–∞–º—è—Ç–∏
        filter_start = time.time()
        checked_count = 0
        matched_count = 0
        
        for idx, doc_id in enumerate(all_data['ids']):
            checked_count += 1
            metadata = all_data['metadatas'][idx] if all_data.get('metadatas') else {}
            
            if not metadata:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å —á–∞—Å—Ç—è–º–∏ –∑–∞–ø—Ä–æ—Å–∞
            match_score = 0
            matches = []
            
            for part_idx, part in enumerate(parts):
                part_lower = part.lower().strip()
                if not part_lower or len(part_lower) < 2:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–æ–ª—è
                page_path = (metadata.get('page_path', '') or '').lower()
                page_title = (metadata.get('title', '') or '').lower()
                heading_path = (metadata.get('heading_path', '') or '').lower()
                heading = (metadata.get('heading', '') or '').lower()
                parent_path = (metadata.get('parent_path', '') or '').lower()
                
                # –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (—Ç–æ—á–Ω–æ–µ –∏–ª–∏ —á–∞—Å—Ç–∏—á–Ω–æ–µ)
                matched_field = None
                if part_lower in page_path:
                    matched_field = 'page_path'
                elif part_lower in page_title:
                    matched_field = 'title'
                elif part_lower in heading_path:
                    matched_field = 'heading_path'
                elif part_lower in heading:
                    matched_field = 'heading'
                elif part_lower in parent_path:
                    matched_field = 'parent_path'
                
                if matched_field:
                    # –ë–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π –≤–µ—Å –¥–ª—è —Ä–∞–Ω–Ω–∏—Ö —á–∞—Å—Ç–µ–π (space > page > section)
                    weight = len(parts) - part_idx
                    match_score += weight
                    matches.append({
                        'part': part,
                        'field': matched_field,
                        'weight': weight
                    })
            
            if match_score > 0:
                matched_count += 1
                formatted_results.append({
                    'id': doc_id,
                    'text': all_data['documents'][idx] if all_data.get('documents') else '',
                    'metadata': metadata,
                    'distance': 0.0,
                    'search_type': 'structural',
                    'match_score': match_score,
                    'matches': matches  # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏
                })
        
        filter_time = time.time() - filter_start
        total_time = time.time() - search_start
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ match_score
        formatted_results.sort(key=lambda x: x['match_score'], reverse=True)
        
        # ============ –î–ï–¢–ê–õ–¨–ù–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï ============
        logger.info(
            f"‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω: "
            f"–Ω–∞–π–¥–µ–Ω–æ {len(formatted_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ "
            f"(–ø—Ä–æ–≤–µ—Ä–µ–Ω–æ {checked_count}, —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π {matched_count}) "
            f"–∑–∞ {total_time:.3f}—Å "
            f"(fetch: {fetch_time:.3f}—Å, filter: {filter_time:.3f}—Å)"
        )
        
        if formatted_results and logger.isEnabledFor(logging.DEBUG):
            # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ø-3 –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            for i, r in enumerate(formatted_results[:3], 1):
                logger.debug(
                    f"  [{i}] match_score={r['match_score']}, "
                    f"page_id={r['metadata'].get('page_id')}, "
                    f"title={r['metadata'].get('title', '')[:50]}, "
                    f"matches={len(r.get('matches', []))}"
                )
        
        return formatted_results[:limit]
        
    except Exception as e:
        total_time = time.time() - search_start
        logger.error(f"–û—à–∏–±–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (–∑–∞ {total_time:.3f}—Å): {e}", exc_info=True)
        return []

def cached_structural_search(
    collection: Any,
    structure: Dict[str, Any],
    limit: int = 100,
    ttl_seconds: int = 300  # 5 –º–∏–Ω—É—Ç
) -> List[Dict[str, Any]]:
    """
    –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞.
    
    Args:
        collection: ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏—è
        structure: –†–µ–∑—É–ª—å—Ç–∞—Ç parse_query_structure()
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        ttl_seconds: –í—Ä–µ–º—è –∂–∏–∑–Ω–∏ –∫—ç—à–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
    
    Returns:
        –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
    """
    cache_key = tuple(sorted(structure['parts']))
    current_time = time.time()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
    if (cache_key in _structural_cache and
        cache_key in _structural_cache_timestamp and
        current_time - _structural_cache_timestamp[cache_key] < ttl_seconds):
        
        cache_age = current_time - _structural_cache_timestamp[cache_key]
        logger.debug(
            f"‚úÖ Structural cache hit: {cache_key} "
            f"(age: {cache_age:.1f}—Å, results: {len(_structural_cache[cache_key])})"
        )
        return _structural_cache[cache_key]
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
    search_start = time.time()
    results = structural_metadata_search(collection, structure, limit)
    search_time = time.time() - search_start
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
    _structural_cache[cache_key] = results
    _structural_cache_timestamp[cache_key] = current_time
    
    logger.info(
        f"üìù Structural cache updated: {cache_key} ‚Üí {len(results)} results "
        f"(search time: {search_time:.3f}—Å)"
    )
    
    return results

def analyze_query_with_metadata(
    query: str,
    collection: Any
) -> Dict[str, Any]:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –∏ –Ω–∞—Ö–æ–¥–∏—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
    
    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        collection: ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏—è
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º–∏ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    """
    query_lower = query.lower()
    keywords = extract_keywords(query)
    
    # –ü–æ–ª—É—á–∞–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    all_data = get_all_metadata_cached(collection)
    
    if not all_data or not all_data.get('metadatas'):
        return {'page_title_matches': [], 'heading_path_matches': [], 'page_path_matches': []}
    
    page_title_matches = []
    heading_path_matches = []
    page_path_matches = []
    
    seen_pages = set()
    
    for idx, metadata in enumerate(all_data['metadatas']):
        if not metadata:
            continue
        
        page_id = metadata.get('page_id')
        if not page_id or page_id in seen_pages:
            continue
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ page_title
        page_title = (metadata.get('title', '') or '').lower()
        if page_title:
            for keyword in keywords:
                if len(keyword) > 3 and keyword in page_title:
                    page_title_matches.append({
                        'page_id': page_id,
                        'page_title': metadata.get('title', ''),
                        'page_path': metadata.get('page_path', ''),
                        'match_keyword': keyword,
                        'match_score': len(keyword) / len(page_title) if page_title else 0
                    })
                    seen_pages.add(page_id)
                    break
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ page_path
        page_path = (metadata.get('page_path', '') or '').lower()
        if page_path:
            for keyword in keywords:
                if len(keyword) > 3 and keyword in page_path:
                    page_path_matches.append({
                        'page_id': page_id,
                        'page_path': metadata.get('page_path', ''),
                        'match_keyword': keyword,
                        'match_score': len(keyword) / len(page_path) if page_path else 0
                    })
                    break
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ heading_path
        heading_path = (metadata.get('heading_path', '') or '').lower()
        if heading_path:
            for keyword in keywords:
                if len(keyword) > 3 and keyword in heading_path:
                    heading_path_matches.append({
                        'page_id': page_id,
                        'heading_path': metadata.get('heading_path', ''),
                        'match_keyword': keyword,
                        'match_score': len(keyword) / len(heading_path) if heading_path else 0
                    })
                    break
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ match_score
    page_title_matches.sort(key=lambda x: x['match_score'], reverse=True)
    heading_path_matches.sort(key=lambda x: x['match_score'], reverse=True)
    page_path_matches.sort(key=lambda x: x['match_score'], reverse=True)
    
    return {
        'page_title_matches': page_title_matches[:10],
        'heading_path_matches': heading_path_matches[:10],
        'page_path_matches': page_path_matches[:10]
    }

def apply_metadata_boost(
    results: List[Dict[str, Any]],
    metadata_analysis: Dict[str, Any]
) -> List[Dict[str, Any]]:
    """
    –ü—Ä–∏–º–µ–Ω–∏—Ç—å boost —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π (–Ω–µ –±–æ–ª–µ–µ 30% –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ score).
    
    Args:
        results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        metadata_analysis: –†–µ–∑—É–ª—å—Ç–∞—Ç analyze_query_with_metadata()
    
    Returns:
        –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã–º boost
    """
    if not results:
        return results
    
    for result in results:
        result['metadata_boost'] = 0.0
        page_id = result.get('metadata', {}).get('page_id')
        
        if not page_id:
            continue
        
        # Boost –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ page_title
        for match in metadata_analysis.get('page_title_matches', [])[:3]:
            if match['page_id'] == page_id:
                # Boost –Ω–µ –±–æ–ª–µ–µ 30% –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ score
                current_score = result.get('rerank_score', 0)
                boost = current_score * 0.3 * match['match_score']
                result['metadata_boost'] += boost
                logger.debug(f"Page title boost: +{boost:.3f} for {page_id}")
                break
        
        # Boost –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ heading_path
        heading_path = result.get('metadata', {}).get('heading_path', '')
        if heading_path:
            for match in metadata_analysis.get('heading_path_matches', [])[:3]:
                if (match['page_id'] == page_id and 
                    match['heading_path'].lower() in heading_path.lower()):
                    current_score = result.get('rerank_score', 0)
                    boost = current_score * 0.2 * match['match_score']
                    result['metadata_boost'] += boost
                    logger.debug(f"Heading path boost: +{boost:.3f} for {page_id}")
                    break
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π score
        result['boosted_score'] = result.get('rerank_score', 0) + result['metadata_boost']
    
    return results

def get_diversity_limit_for_intent(intent_type: str = None) -> int:
    """
    –ü–æ–ª—É—á–∏—Ç—å –ª–∏–º–∏—Ç diversity filter –¥–ª—è —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞.
    
    Args:
        intent_type: –¢–∏–ø –∑–∞–ø—Ä–æ—Å–∞ ('navigational', 'exploratory', 'factual', 'howto')
        
    Returns:
        –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ chunks —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∫–ª—é—á—ë–Ω –ª–∏ —Ñ–∏–ª—å—Ç—Ä
    enable_filter = os.getenv('ENABLE_DIVERSITY_FILTER', 'true').lower() == 'true'
    if not enable_filter:
        return 999  # –û—á–µ–Ω—å –±–æ–ª—å—à–æ–π –ª–∏–º–∏—Ç (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ—Ç —Ñ–∏–ª—å—Ç—Ä)
    
    # –ü–æ–ª—É—á–∞–µ–º –ª–∏–º–∏—Ç—ã –∏–∑ ENV –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç—ã
    diversity_limits = {
        'navigational': int(os.getenv('DIVERSITY_LIMIT_NAVIGATIONAL', '1')),
        'exploratory': int(os.getenv('DIVERSITY_LIMIT_EXPLORATORY', '4')),
        'factual': int(os.getenv('DIVERSITY_LIMIT_FACTUAL', '2')),
        'howto': int(os.getenv('DIVERSITY_LIMIT_HOWTO', '3')),
    }
    
    # –ï—Å–ª–∏ —Ç–∏–ø –Ω–µ —É–∫–∞–∑–∞–Ω –∏–ª–∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç –¥–ª—è factual
    if not intent_type or intent_type not in diversity_limits:
        intent_type = 'factual'
    
    return diversity_limits.get(intent_type, 2)


def apply_diversity_filter(results: list, limit: int = 5, max_per_page: int = None, query: str = None, intent: dict = None) -> list:
    """
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç diversity filter: –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ chunks —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
    
    –≠—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ (Google, Perplexity): –º–∞–∫—Å–∏–º—É–º 2-3 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ 
    —Å –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –≤ —Ç–æ–ø-5, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –º–µ—Å—Ç–∞ - –¥—Ä—É–≥–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏.
    
    –ù–û–í–û–ï: –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ª–∏–º–∏—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ query intent —á–µ—Ä–µ–∑ ENV –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ.
    
    Args:
        results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ score
        limit: –°–∫–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–µ—Ä–Ω—É—Ç—å
        max_per_page: –ú–∞–∫—Å–∏–º—É–º chunks —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–µ—Å–ª–∏ None, –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –ø–æ intent)
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è intent, –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        intent: –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ç–∏–ø–µ –∑–∞–ø—Ä–æ—Å–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
    Returns:
        –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    """
    if not results:
        return []
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º max_per_page –Ω–∞ –æ—Å–Ω–æ–≤–µ intent –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω
    if max_per_page is None:
        intent_type = None
        if intent and isinstance(intent, dict):
            intent_type = intent.get('type')
        elif query:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º intent –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω
            intent_dict = classify_query_intent(query)
            intent_type = intent_dict.get('type') if intent_dict else None
        
        max_per_page = get_diversity_limit_for_intent(intent_type)
        logger.debug(f"Diversity filter: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ª–∏–º–∏—Ç {max_per_page} –¥–ª—è intent={intent_type}")
    
    filtered_results = []
    page_counts = {}
    
    for result in results:
        if not result or not isinstance(result, dict):
            continue
        
        metadata = result.get('metadata')
        if not metadata or not isinstance(metadata, dict):
            continue
        
        page_id = metadata.get('page_id')
        
        # –ï—Å–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–µ—Ç –∏–ª–∏ –ª–∏–º–∏—Ç –Ω–µ –ø—Ä–µ–≤—ã—à–µ–Ω - –¥–æ–±–∞–≤–ª—è–µ–º
        if not page_id or page_counts.get(page_id, 0) < max_per_page:
            filtered_results.append(result)
            if page_id:
                page_counts[page_id] = page_counts.get(page_id, 0) + 1
            
            # –î–æ—Å—Ç–∏–≥–ª–∏ –Ω—É–∂–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if len(filtered_results) >= limit:
                break
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    if page_counts:
        logger.debug(f"Diversity filter: {len(filtered_results)} results from {len(page_counts)} unique pages (max {max_per_page}/page)")
        for page_id, count in page_counts.items():
            if count > 1:
                logger.debug(f"  Page {page_id}: {count} chunks")
    
    return filtered_results

def enrich_result_with_context(result: dict) -> dict:
    """
    –û–±–æ–≥–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏–∑ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤.
    
    Args:
        result: –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞
        
    Returns:
        –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å breadcrumb
    """
    if not result or not isinstance(result, dict):
        return result
    
    metadata = result.get('metadata')
    if not metadata or not isinstance(metadata, dict):
        # –ï—Å–ª–∏ metadata –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å
        metadata = {}
        result['metadata'] = metadata
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º breadcrumb (—Ö–ª–µ–±–Ω—ã–µ –∫—Ä–æ—à–∫–∏)
    # –ù–û–í–û–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º page_path –µ—Å–ª–∏ –µ—Å—Ç—å (–ø–æ–ª–Ω—ã–π –ø—É—Ç—å)
    page_path = metadata.get('page_path', '')
    if page_path:
        # page_path —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–ª–Ω—ã–π –ø—É—Ç—å: "Parent 1 > Parent 2 > Page"
        breadcrumb_parts = page_path.split(' > ')
    else:
        # Fallback: —Ñ–æ—Ä–º–∏—Ä—É–µ–º –∏–∑ parent_title (—Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥)
        breadcrumb_parts = []
        if metadata.get('parent_title'):
            breadcrumb_parts.append(metadata['parent_title'])
        if metadata.get('title') and metadata.get('title') != metadata.get('parent_title'):
            breadcrumb_parts.append(metadata['title'])
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å–µ–∫—Ü–∏–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
    if metadata.get('heading'):
        breadcrumb_parts.append(metadata['heading'])
    
    result['breadcrumb'] = ' ‚Üí '.join(breadcrumb_parts) if breadcrumb_parts else metadata.get('title', 'Unknown')
    
    return result

def classify_query_intent(query: str) -> dict:
    """
    –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –Ω–∞–º–µ—Ä–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–∏—Å–∫–∞.
    
    Returns:
        {
            'type': 'factual' | 'navigational' | 'howto' | 'exploratory',
            'boost_hierarchy': bool,  # –£—Å–∏–ª–∏—Ç—å –±—É—Å—Ç –∏–µ—Ä–∞—Ä—Ö–∏–∏
            'expand_context': bool,   # –†–∞—Å—à–∏—Ä–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
            'diversity': int          # –õ–∏–º–∏—Ç —á–∞–Ω–∫–æ–≤ —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        }
    """
    import re
    
    query_lower = query.lower()
    
    # 1. –ù–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã: "–≥–¥–µ", "–Ω–∞–π–¥–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—É", "–ø–æ–∫–∞–∂–∏"
    if re.search(r'\b(–≥–¥–µ|–Ω–∞–π–¥–∏|–ø–æ–∫–∞–∂–∏|—Å—Ç—Ä–∞–Ω–∏—Ü–∞|–¥–æ–∫—É–º–µ–Ω—Ç)\b', query_lower):
        return {
            'type': 'navigational',
            'boost_hierarchy': True,   # –í–∞–∂–Ω—ã –∫–æ—Ä–Ω–µ–≤—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            'expand_context': False,   # –ù–µ –Ω—É–∂–µ–Ω –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            'diversity': 1             # –ü–æ 1 —á–∞–Ω–∫—É —Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–ø–æ–∫–∞–∑–∞—Ç—å –±–æ–ª—å—à–µ —Å—Ç—Ä–∞–Ω–∏—Ü)
        }
    
    # 2. How-to –∑–∞–ø—Ä–æ—Å—ã: "–∫–∞–∫", "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è", "–Ω–∞—Å—Ç—Ä–æ–∏—Ç—å"
    if re.search(r'\b(–∫–∞–∫|–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è|–Ω–∞—Å—Ç—Ä–æ–∏—Ç—å|—É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å|–∑–∞–ø—É—Å—Ç–∏—Ç—å|—Å–¥–µ–ª–∞—Ç—å)\b', query_lower):
        return {
            'type': 'howto',
            'boost_hierarchy': False,  # –ù–µ –≤–∞–∂–Ω–∞ –∏–µ—Ä–∞—Ä—Ö–∏—è
            'expand_context': True,    # –ù—É–∂–µ–Ω –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            'diversity': 3             # –î–æ 3 —á–∞–Ω–∫–æ–≤ —Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–¥–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è)
        }
    
    # 3. –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã: "–∫–∞–∫–æ–π", "—á—Ç–æ", "–∫–æ–≥–¥–∞", "–∫—Ç–æ"
    if re.search(r'\b(–∫–∞–∫–æ–π|–∫–∞–∫–∞—è|–∫–∞–∫–∏–µ|—á—Ç–æ|–∫–æ–≥–¥–∞|–∫—Ç–æ|—Å–∫–æ–ª—å–∫–æ)\b', query_lower):
        return {
            'type': 'factual',
            'boost_hierarchy': False,  # –ù–µ –≤–∞–∂–Ω–∞ –∏–µ—Ä–∞—Ä—Ö–∏—è
            'expand_context': True,    # –ù—É–∂–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞
            'diversity': 3             # –î–æ 3 —á–∞–Ω–∫–æ–≤ (–º–æ–∂–µ—Ç –±—ã—Ç—å –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö)
        }
    
    # 4. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    return {
        'type': 'exploratory',
        'boost_hierarchy': False,
        'expand_context': True,
        'diversity': 2  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ª–∏–º–∏—Ç
    }

def init_rag() -> QdrantClient:
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã: Qdrant.
    
    Returns:
        QdrantClient
    
    Raises:
        Exception: –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    """
    try:
        logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Qdrant: {QDRANT_HOST}:{QDRANT_PORT}, collection={QDRANT_COLLECTION}")
        
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ qdrant_storage
        from qdrant_storage import init_qdrant_client, init_qdrant_collection
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∏–µ–Ω—Ç
        client = init_qdrant_client()
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å embeddings
        embedding_dim = get_embedding_dimension()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é (—Å–æ–∑–¥–∞–µ—Ç –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
        success = init_qdrant_collection(embedding_dim)
        if not success:
            raise ValueError(f"Failed to initialize Qdrant collection: {QDRANT_COLLECTION}")
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        from qdrant_storage import get_qdrant_count
        doc_count = get_qdrant_count()
        
        logger.info(f"‚úÖ RAG —Å–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞. –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {doc_count}, –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {embedding_dim}D")
        return client
        
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ RAG: {e}")
        sys.exit(1)

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è RAG –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
qdrant_client = None

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –º–æ–¥—É–ª—è (–¥–æ —Å–æ–∑–¥–∞–Ω–∏—è MCP —Å–µ—Ä–≤–µ—Ä–∞)
# –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ MCP —Å–µ—Ä–≤–µ—Ä–∞...")
qdrant_client = init_rag()
from qdrant_storage import get_qdrant_count
doc_count = get_qdrant_count()
logger.info(f"RAG —Å–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞. –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {doc_count}")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è BM25 retriever –¥–ª—è Hybrid Search (–ª–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è)
logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è BM25 retriever –¥–ª—è Hybrid Search...")
# BM25 —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–ø—Ä—è–º—É—é —Å Qdrant —á–µ—Ä–µ–∑ qdrant_storage
init_bm25_retriever(qdrant_client)

# –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ reranker –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ (—á—Ç–æ–±—ã –ø–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å –±—ã–ª –±—ã—Å—Ç—Ä–µ–µ)
logger.info("–ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ reranker –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ...")
try:
    init_reranker()
    logger.info("‚úÖ Reranker –º–æ–¥–µ–ª—å –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∏—Ç—å reranker –º–æ–¥–µ–ª—å: {e}. –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∑–∏—Ç—Å—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ.")

mcp = FastMCP("Confluence RAG")

def execute_single_query_search(
    query_embedding: List[float],
    query_text: str,
    search_limit: int,
    where_filter: Optional[Dict],
    collection
) -> List[Dict]:
    """
    –í—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–∏—Å–∫ –¥–ª—è –æ–¥–Ω–æ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞.
    
    Args:
        query_embedding: Embedding –∑–∞–ø—Ä–æ—Å–∞
        query_text: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ (–¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è)
        search_limit: –õ–∏–º–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        where_filter: –§–∏–ª—å—Ç—Ä –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º
        collection: ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏—è
        
    Returns:
        –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
    """
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º search_in_qdrant –Ω–∞–ø—Ä—è–º—É—é
        from qdrant_storage import search_in_qdrant
        
        results_raw = search_in_qdrant(
            query_embedding=query_embedding,
            limit=search_limit,
            where_filter=where_filter,
            collection=QDRANT_COLLECTION
        )
        
        results = []
        for result in results_raw:
            doc_id = result.get('id', '')
            if doc_id:
                results.append({
                    'id': doc_id,
                    'text': result.get('text', ''),
                    'metadata': result.get('metadata', {}),
                    'distance': 1.0 - result.get('score', 0.0),
                    'query_variant': query_text  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞—Ä–∏–∞–Ω—Ç –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                })
        
        return results
        
    except Exception as e:
        error_msg = str(e)
        logger.error(f"–û—à–∏–±–∫–∞ Qdrant –ø—Ä–∏ –ø–æ–∏—Å–∫–µ '{query_text}': {error_msg}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –≤–º–µ—Å—Ç–æ –∏—Å–∫–ª—é—á–µ–Ω–∏—è –¥–ª—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏
        return []


def parallel_multi_query_search(
    expanded_queries: List[str],
    query_embeddings: List[List[float]],
    search_limit: int,
    where_filter: Optional[Dict],
    collection,
    max_workers: int = None
) -> List[Dict]:
    """
    –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º –≤–∞—Ä–∏–∞–Ω—Ç–∞–º –∑–∞–ø—Ä–æ—Å–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ThreadPoolExecutor.
    
    Args:
        expanded_queries: –°–ø–∏—Å–æ–∫ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–∞
        query_embeddings: –°–ø–∏—Å–æ–∫ embeddings –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞
        search_limit: –õ–∏–º–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        where_filter: –§–∏–ª—å—Ç—Ä –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º
        collection: ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏—è
        max_workers: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ ENV –∏–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤)
        
    Returns:
        –°–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–¥–µ–¥—É–ø–ª–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ id)
    """
    if not expanded_queries or not query_embeddings:
        return []
    
    if len(expanded_queries) != len(query_embeddings):
        logger.error(f"–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ embeddings: {len(expanded_queries)} != {len(query_embeddings)}")
        return []
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤
    if max_workers is None:
        max_workers = int(os.getenv('PARALLEL_SEARCH_MAX_WORKERS', '4'))
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∑–∞–ø—Ä–æ—Å–æ–≤
    max_workers = min(max_workers, len(expanded_queries))
    
    # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –∏–ª–∏ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º –æ—Ç–∫–ª—é—á–µ–Ω, –≤—ã–ø–æ–ª–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
    enable_parallel = os.getenv('ENABLE_PARALLEL_SEARCH', 'true').lower() == 'true'
    if not enable_parallel or len(expanded_queries) == 1:
        logger.debug("–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –æ—Ç–∫–ª—é—á–µ–Ω –∏–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å, –≤—ã–ø–æ–ª–Ω—è—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ")
        all_results = []
        seen_ids = set()
        for i, q in enumerate(expanded_queries):
            results = execute_single_query_search(
                query_embeddings[i], q, search_limit, where_filter, collection
            )
            for result in results:
                if result['id'] not in seen_ids:
                    seen_ids.add(result['id'])
                    all_results.append(result)
        return all_results
    
    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
    logger.info(f"–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫: {len(expanded_queries)} –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ {max_workers} –ø–æ—Ç–æ–∫–∞—Ö")
    start_time = time.time()
    
    all_results = []
    seen_ids = set()
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # –°–æ–∑–¥–∞—ë–º –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        futures = {
            executor.submit(
                execute_single_query_search,
                query_embeddings[i],
                expanded_queries[i],
                search_limit,
                where_filter,
                collection
            ): expanded_queries[i]
            for i in range(len(expanded_queries))
        }
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –∏—Ö –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
        for future in as_completed(futures):
            query_variant = futures[future]
            try:
                results = future.result()
                for result in results:
                    if result['id'] not in seen_ids:
                        seen_ids.add(result['id'])
                        all_results.append(result)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º –ø–æ–∏—Å–∫–µ –¥–ª—è '{query_variant}': {e}")
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –¥—Ä—É–≥–∏–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ (graceful degradation)
    
    elapsed = time.time() - start_time
    logger.info(f"–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à—ë–Ω –∑–∞ {elapsed:.3f}—Å: {len(all_results)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    
    return all_results


def get_text_for_reranking(text: str, query: str, max_len: int = 1200) -> str:
    """
    –£–º–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è reranking.
    –ú–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–µ.
    
    Args:
        text: –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        max_len: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1200 —Å–∏–º–≤–æ–ª–æ–≤)
    
    Returns:
        –§—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    """
    if not text or not query:
        return text[:max_len] if text else ""
    
    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –∫–æ—Ä–æ—Ç–∫–∏–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Å—å
    if len(text) <= max_len:
        return text
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ (—É–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞)
    stop_words = {'–≤', '–Ω–∞', '–ø–æ', '–¥–ª—è', '—Å', '–∫', '–∏–∑', '–æ', '–æ–±', '–∏', '–∞', '–Ω–æ', '–∏–ª–∏', '–∂–µ', '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è', '–∫–∞–∫–∏–µ', '–≥–¥–µ', '–∫–∞–∫', '—á—Ç–æ', '—ç—Ç–æ'}
    query_keywords = {kw.lower() for kw in query.split() if kw.lower() not in stop_words and len(kw) > 2}
    
    if not query_keywords:
        # –ï—Å–ª–∏ –Ω–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–µ max_len —Å–∏–º–≤–æ–ª–æ–≤
        return text[:max_len]
    
    # –ù–∞—á–∞–ª—å–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç (–ø–µ—Ä–≤—ã–µ max_len —Å–∏–º–≤–æ–ª–æ–≤)
    best_snippet = text[:max_len]
    best_score = sum(1 for kw in query_keywords if kw in best_snippet.lower())
    
    # –ò—â–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    # –®–∞–≥ 50 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    for start in range(0, len(text) - max_len, 50):
        snippet = text[start:start + max_len]
        score = sum(1 for kw in query_keywords if kw in snippet.lower())
        
        if score > best_score:
            best_score = score
            best_snippet = snippet
    
    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ª—É—á—à–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç, –ø—Ä–æ–±—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ max_len —Å–∏–º–≤–æ–ª–æ–≤
    if best_score == 0:
        last_snippet = text[-max_len:] if len(text) > max_len else text
        last_score = sum(1 for kw in query_keywords if kw in last_snippet.lower())
        if last_score > best_score:
            best_snippet = last_snippet
    
    return best_snippet


@mcp.tool()
def confluence_semantic_search(query: str, limit: int = 5, space: str = "") -> str:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π Confluence —Å –≥–∏–±—Ä–∏–¥–Ω—ã–º –ø–æ–∏—Å–∫–æ–º.

    –í–ê–ñ–ù–û: –í–°–ï–ì–î–ê —É—Ç–æ—á–Ω–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ (space) –ø–µ—Ä–µ–¥ –ø–æ–∏—Å–∫–æ–º. –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏!

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç:
    - Hybrid Search (Vector 60% + BM25 40%) - –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ RRF
    - Query expansion (—Å–∏–Ω–æ–Ω–∏–º—ã, –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞, Semantic Query Log)
    - Metadata pre-filtering (–ø–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤—É, –¥–∞—Ç–µ, —Ç–∏–ø—É)
    - CrossEncoder reranking (DiTy/russian-msmarco –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏)
    - Context expansion (—Å–æ—Å–µ–¥–Ω–∏–µ —Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–ª–Ω–æ—Ç—ã)
    - Hierarchy boost (–∏–µ—Ä–∞—Ä—Ö–∏—è –¥–ª—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏)

    Args:
        query (str): –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)
        limit (int): –ú–∞–∫—Å–∏–º—É–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (1-20, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5). 
                     –î–ª—è completeness –∏—Å–ø–æ–ª—å–∑—É–π 10+
        space (str): –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ Confluence –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.
                     –ü—Ä–∏–º–µ—Ä—ã: "RAUII" (–ø—Ä–æ–µ–∫—Ç), "Surveys" (–æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è), "DEVOPS" (–¥–µ–≤–æ–ø—Å)
                     –ö–†–ò–¢–ò–ß–ù–û: –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω ‚Üí —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ –≤—Å–µ—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤ (–º–µ–Ω–µ–µ —Ç–æ—á–Ω–æ)

    Returns:
        str: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –∏–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ

    Examples:
        - –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç "–∫–∞–∫–æ–π —Å—Ç–µ–∫ –≤ RAUII" ‚Üí space="RAUII"
        - –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç "–≤–æ–ø—Ä–æ—Å—ã –ø—Ä–æ –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ" ‚Üí space="Surveys"
        - –ï—Å–ª–∏ space –Ω–µ —è—Å–µ–Ω ‚Üí —Å–ø—Ä–æ—Å–∏: "–í –∫–∞–∫–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –∏—Å–∫–∞—Ç—å: RAUII –∏–ª–∏ Surveys?"

    Note:
        - –ò—Å–ø–æ–ª—å–∑—É–π completeness search (limit=10 –∏–ª–∏ –±–æ–ª—å—à–µ) –¥–ª—è –ø–æ–ª–Ω–æ—Ç—ã
        - –£–∫–∞–∑—ã–≤–∞–π –∏—Å—Ç–æ—á–Ω–∏–∫ –≤ –æ—Ç–≤–µ—Ç–µ (–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ, —Å—Ç—Ä–∞–Ω–∏—Ü–∞, ID)
        - –ù–µ —Å–æ—á–∏–Ω—è–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤–Ω–µ Confluence
        - –ï—Å–ª–∏ scores –Ω–∏–∑–∫–∏–µ (<0.0001) ‚Üí retry —Å limit=20
    """
    try:
        # ============ –ë–ï–ó–û–ü–ê–°–ù–û–°–¢–¨: –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ============
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ query
        if not query or not isinstance(query, str):
            return "‚ùå –û—à–∏–±–∫–∞: –ü—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å"
        
        query = query.strip()
        if len(query) < 2:
            return "‚ùå –û—à–∏–±–∫–∞: –ó–∞–ø—Ä–æ—Å —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π (–º–∏–Ω–∏–º—É–º 2 —Å–∏–º–≤–æ–ª–∞)"
        
        if len(query) > 1000:
            logger.warning(f"–û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å ({len(query)} —Å–∏–º–≤–æ–ª–æ–≤), –æ–±—Ä–µ–∑–∞—é –¥–æ 1000")
            query = query[:1000]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ boost (–¥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è space)
        original_query = query
        
        # ============ –ù–û–í–û–ï: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ space –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞ ============
        # –ï—Å–ª–∏ space –Ω–µ —É–∫–∞–∑–∞–Ω –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä, –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–∑ —Ç–µ–∫—Å—Ç–∞
        if not space:
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã: "spaces RAUII", "space RAUII", "–≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ RAUII"
            space_patterns = [
                r'\bspaces?\s+([A-Za-z0-9_-]+)\s*$',  # "spaces RAUII" –≤ –∫–æ–Ω—Ü–µ
                r'\bspaces?\s+([A-Za-z0-9_-]+)(?:\s|$)',  # "spaces RAUII" –≤ –ª—é–±–æ–º –º–µ—Å—Ç–µ
                r'\b–≤\s+–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ\s+([A-Za-z0-9_-]+)\s*$',  # "–≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ RAUII"
                r'\b–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ\s+([A-Za-z0-9_-]+)\s*$',  # "–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ RAUII"
            ]
            
            for pattern in space_patterns:
                match = re.search(pattern, query, re.IGNORECASE)
                if match:
                    extracted_space = match.group(1).strip()
                    # –£–¥–∞–ª—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
                    query = re.sub(pattern, '', query, flags=re.IGNORECASE).strip()
                    space = extracted_space
                    logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω space –∏–∑ –∑–∞–ø—Ä–æ—Å–∞: '{space}', –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{query}'")
                    break
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ space
        if space and not isinstance(space, str):
            return "‚ùå –û—à–∏–±–∫–∞: –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä space"
        
        if space:
            space = space.strip()
            # –ó–∞—â–∏—Ç–∞ –æ—Ç injection: —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã, –¥–µ—Ñ–∏—Å, –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏–µ
            if not re.match(r'^[a-zA-Z0-9_-]+$', space):
                logger.warning(f"–ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–π space –ø–∞—Ä–∞–º–µ—Ç—Ä: {space}")
                return "‚ùå –û—à–∏–±–∫–∞: –ü–∞—Ä–∞–º–µ—Ç—Ä space —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Å–∏–º–≤–æ–ª—ã"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ RAG –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ —Å–µ—Ä–≤–µ—Ä–∞
        if qdrant_client is None:
            return "‚ùå –û—à–∏–±–∫–∞: RAG —Å–∏—Å—Ç–µ–º–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ —Å–µ—Ä–≤–µ—Ä–∞."
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ª–∏–º–∏—Ç–∞
        limit = min(max(limit, 1), 20)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        from qdrant_storage import get_qdrant_count
        doc_count = get_qdrant_count()
        if doc_count == 0:
            logger.warning("–ü–æ–ø—ã—Ç–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –ø—É—Å—Ç–æ–º—É –∏–Ω–¥–µ–∫—Å—É")
            return "‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å –ø—É—Å—Ç. –î–æ–∂–¥–∏—Ç–µ—Å—å –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–µ—Ä–≤–æ–π —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏."
        
        # ============ –ù–û–í–û–ï: Structural Navigation Search ============
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–º (—Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏ >, ‚Üí)
        structure = parse_query_structure(query)
        logger.debug(f"üîç Query structure analysis: is_structural={structure['is_structural_query']}, parts={structure['parts']}")
        
        # –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è)
        structural_results = None
        
        if structure['is_structural_query']:
            logger.info(f"üîç –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –∑–∞–ø—Ä–æ—Å –æ–±–Ω–∞—Ä—É–∂–µ–Ω: {structure['parts']}")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –ø–æ–∏—Å–∫ (—Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º)
            structural_search_start = time.time()
            structural_results = cached_structural_search(
                collection,
                structure,
                limit=limit * 10  # –ü–æ–ª—É—á–∞–µ–º –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ reranking
            )
            structural_search_time = time.time() - structural_search_start
            
            if structural_results:
                logger.info(
                    f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(structural_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ "
                    f"(–∑–∞ {structural_search_time:.3f}—Å)"
                )
                
                # –ï—Å–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ - –ø—Ä–∏–º–µ–Ω—è–µ–º –ª–µ–≥–∫–∏–π reranking –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º
                if len(structural_results) >= limit:
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª–µ–≥–∫–∏–π reranking –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                    # (–∏—Å–ø–æ–ª—å–∑—É–µ–º match_score –∫–∞–∫ –±–∞–∑–æ–≤—ã–π score)
                    for result in structural_results:
                        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º match_score –≤ –¥–∏–∞–ø–∞–∑–æ–Ω 0-1
                        max_match = max([r['match_score'] for r in structural_results]) if structural_results else 1
                        result['rerank_score'] = (result['match_score'] / max_match) * 0.5 if max_match > 0 else 0.1
                        result['distance'] = 1.0 - result['rerank_score']  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                    
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è boost
                    metadata_analysis = analyze_query_with_metadata(query, collection)
                    structural_results = apply_metadata_boost(structural_results, metadata_analysis)
                    
                    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ boosted_score
                    structural_results.sort(key=lambda x: x.get('boosted_score', x.get('rerank_score', 0)), reverse=True)
                    
                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    format_start = time.time()
                    formatted = format_search_results(structural_results[:limit], query, limit)
                    format_time = time.time() - format_start
                    total_structural_time = time.time() - structural_search_start
                    
                    logger.info(
                        f"‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –ø–æ–∏—Å–∫: –≤–æ–∑–≤—Ä–∞—â–µ–Ω–æ {limit} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ "
                        f"(total: {total_structural_time:.3f}—Å, format: {format_time:.3f}—Å)"
                    )
                    return formatted
                else:
                    logger.info(f"‚ö†Ô∏è –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∞—à–µ–ª —Ç–æ–ª—å–∫–æ {len(structural_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –æ–±—ä–µ–¥–∏–Ω—è—é —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º")
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –ø–æ–∏—Å–∫–æ–º (–ø—Ä–æ–¥–æ–ª–∂–∞–µ–º pipeline)
                    # structural_results –±—É–¥—É—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã –ø–æ–∑–∂–µ
            else:
                logger.info(f"‚ö†Ô∏è –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, fallback –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π")
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—ã—á–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
        
        # ============ –ù–û–í–û–ï: Semantic Caching ============
        cache = get_semantic_cache()
        cached_results = cache.get(query, space, limit)
        
        if cached_results:
            logger.info(f"‚úÖ Cache HIT: '{query[:50]}...'")
            return cached_results
        
        logger.debug(f"Cache MISS: '{query[:50]}...', –≤—ã–ø–æ–ª–Ω—è—é –ø–æ–∏—Å–∫...")
        
        # ============ –ù–û–í–û–ï: Query Intent Classification ============
        intent = classify_query_intent(query)
        if not intent or not isinstance(intent, dict):
            # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            intent = {
                'type': 'exploratory',
                'boost_hierarchy': False,
                'expand_context': True,
                'diversity': 2
            }
        logger.info(f"Query intent: {intent.get('type', 'unknown')} (diversity={intent.get('diversity', 2)}, expand_context={intent.get('expand_context', True)})")
        
        # ============ –£–õ–£–ß–®–ï–ù–ò–ï 1: Query Expansion (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è) ============
        expanded_queries = expand_query(query, space)
        if len(expanded_queries) > 1:
            logger.info(f"Query expansion: {query} ‚Üí {expanded_queries}")
        
        # ============ –£–õ–£–ß–®–ï–ù–ò–ï 2: –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ª–∏–º–∏—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ ============
        search_limit = calculate_optimal_candidate_limit(query, limit)
        logger.debug(f"–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {search_limit}")
        
        # ============ –£–õ–£–ß–®–ï–ù–ò–ï 3: Metadata Pre-filtering ============
        # –ü–æ–∏—Å–∫ –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ ChromaDB —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –î–û –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        
        where_filter = None
        if space:
            where_filter = {"space": space}
            logger.info(f"Pre-filtering by space: {space}")
        
        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Å–µ embeddings batch'–æ–º (–≤ 3-5 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ)
        embedding_start = time.time()
        query_embeddings = generate_query_embeddings_batch(expanded_queries)
        embedding_elapsed = time.time() - embedding_start
        logger.debug(f"Batch embeddings generated –∑–∞ {embedding_elapsed:.3f}—Å –¥–ª—è {len(expanded_queries)} –∑–∞–ø—Ä–æ—Å–æ–≤")
        
        # ============ –ù–û–í–û–ï: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π Multi-Query Search ============
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º –≤–∞—Ä–∏–∞–Ω—Ç–∞–º –∑–∞–ø—Ä–æ—Å–∞ –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–û
        try:
            all_results = parallel_multi_query_search(
                expanded_queries=expanded_queries,
                query_embeddings=query_embeddings,
                search_limit=search_limit,
                where_filter=where_filter,
                collection=QDRANT_COLLECTION
            )
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º –ø–æ–∏—Å–∫–µ: {e}, fallback –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∂–∏–º")
            # Fallback –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∂–∏–º
            all_results = []
            seen_ids = set()
            for i, q in enumerate(expanded_queries):
                results = execute_single_query_search(
                    query_embeddings[i], q, search_limit, where_filter, collection
                )
                for result in results:
                    if result['id'] not in seen_ids:
                        seen_ids.add(result['id'])
                        all_results.append(result)
        
        # ============ –£–õ–£–ß–®–ï–ù–ò–ï 4: Fallback —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ (3 —É—Ä–æ–≤–Ω—è) ============
        fallback_search = get_fallback_search(min_results=3)
        fallback_message = ""
        original_space = space
        
        # Fallback #1: –£–±–∏—Ä–∞–µ–º space —Ñ–∏–ª—å—Ç—Ä
        if fallback_search.should_apply_fallback(all_results, level=1) and space:
            logger.info(f"üîÑ Fallback #1: –£–±–∏—Ä–∞—é space —Ñ–∏–ª—å—Ç—Ä '{space}'...")
            # –ü–æ–≤—Ç–æ—Ä—è–µ–º –ø–æ–∏—Å–∫ –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ space (—Ç–æ–∂–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ)
            where_filter = None
            # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ü–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ embeddings
            try:
                fallback_results = parallel_multi_query_search(
                    expanded_queries=expanded_queries,
                    query_embeddings=query_embeddings,
                    search_limit=search_limit,
                    where_filter=where_filter,
                    collection=QDRANT_COLLECTION
                )
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                seen_ids = {r['id'] for r in all_results}
                for result in fallback_results:
                    if result['id'] not in seen_ids:
                        all_results.append(result)
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º fallback –ø–æ–∏—Å–∫–µ: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∂–∏–º")
                # Fallback –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∂–∏–º
                seen_ids = {r['id'] for r in all_results}
                for i, q in enumerate(expanded_queries):
                    results = execute_single_query_search(
                        query_embeddings[i], q, search_limit, where_filter, QDRANT_COLLECTION
                    )
                    for result in results:
                        if result['id'] not in seen_ids:
                            seen_ids.add(result['id'])
                            all_results.append(result)
            
            if all_results:
                fallback_message = fallback_search.get_fallback_message(1, original_space)
        
        # Fallback #2: PRF (Pseudo-Relevance Feedback)
        if fallback_search.should_apply_fallback(all_results, level=2):
            logger.info(f"üîÑ Fallback #2: –ü—Ä–∏–º–µ–Ω—è—é PRF (Pseudo-Relevance Feedback)...")
            
            try:
                # –ü—Ä–∏–º–µ–Ω—è–µ–º PRF –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É
                expanded_query_prf = pseudo_relevance_feedback(query, all_results, top_k=3, max_terms=5)
                
                if expanded_query_prf != query:
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
                    prf_embedding = generate_query_embedding(expanded_query_prf)
                    
                    # –ü–æ–≤—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Qdrant
                    from qdrant_storage import search_in_qdrant
                    qdrant_results = search_in_qdrant(
                        query_embedding=prf_embedding,
                        limit=search_limit,
                        where_filter=None,  # –ë–µ–∑ —Ñ–∏–ª—å—Ç—Ä–æ–≤
                        collection=QDRANT_COLLECTION
                    )
                    
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    for result in qdrant_results:
                        doc_id = result.get('id', '')
                        if doc_id and doc_id not in seen_ids:
                            seen_ids.add(doc_id)
                            doc_text = result.get('text', '')
                            doc_metadata = result.get('metadata', {})
                            
                            if doc_text:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–µ–∫—Å—Ç –Ω–µ –ø—É—Å—Ç–æ–π
                                all_results.append({
                                    'id': doc_id,
                                    'text': doc_text,
                                    'metadata': doc_metadata,
                                    'distance': 1.0 - result.get('score', 0.0)
                                })
                    
                    if all_results:
                        fallback_message = fallback_search.get_fallback_message(2)
                        
            except Exception as e:
                logger.warning(f"PRF Fallback failed: {e}")
        
        # Fallback 2: –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –º–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –ø—Ä–æ–±—É–µ–º —Ç–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        if len(all_results) < 3:
            keywords = extract_keywords(query)
            if len(keywords) >= 2:
                keyword_query = ' '.join(keywords)
                logger.info(f"–ú–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –ø—Ä–æ–±—É—é –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: {keyword_query}")
                
                # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding —Ç–æ–ª—å–∫–æ –¥–ª—è keyword_query
                keyword_embedding = generate_query_embedding(keyword_query)
                
                # –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Qdrant
                from qdrant_storage import search_in_qdrant
                qdrant_results = search_in_qdrant(
                    query_embedding=keyword_embedding,
                    limit=search_limit,
                    where_filter=None,  # –ë–µ–∑ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –¥–ª—è keyword search
                    collection=QDRANT_COLLECTION
                )
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                for result in qdrant_results:
                    doc_id = result.get('id', '')
                    if doc_id and doc_id not in seen_ids:
                        seen_ids.add(doc_id)
                        doc_text = result.get('text', '')
                        doc_metadata = result.get('metadata', {})
                        
                        if doc_text:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–µ–∫—Å—Ç –Ω–µ –ø—É—Å—Ç–æ–π
                            all_results.append({
                                'id': doc_id,
                                'text': doc_text,
                                'metadata': doc_metadata,
                                'distance': 1.0 - result.get('score', 0.0)
                            })
        
        if not all_results:
            return f"‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'"
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(all_results)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        
        # ============ –ù–û–í–û–ï: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º–∏ ============
        # –ï—Å–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∞—à–µ–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –Ω–æ –∏—Ö –º–∞–ª–æ - –æ–±—ä–µ–¥–∏–Ω—è–µ–º
        if structural_results and len(structural_results) > 0:
            merge_start = time.time()
            logger.info(f"üîó –û–±—ä–µ–¥–∏–Ω—è—é {len(structural_results)} —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å {len(all_results)} —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º–∏")
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –Ω–∞—á–∞–ª–æ —Å–ø–∏—Å–∫–∞ (–æ–Ω–∏ –±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã)
            seen_ids = {r.get('id') for r in all_results}
            for struct_result in structural_results:
                if struct_result.get('id') not in seen_ids:
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º match_score –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                    max_match = max([r['match_score'] for r in structural_results]) if structural_results else 1
                    struct_result['rerank_score'] = (struct_result['match_score'] / max_match) * 0.5 if max_match > 0 else 0.1
                    struct_result['distance'] = 1.0 - struct_result['rerank_score']
                    all_results.insert(0, struct_result)  # –í—Å—Ç–∞–≤–ª—è–µ–º –≤ –Ω–∞—á–∞–ª–æ
                    seen_ids.add(struct_result.get('id'))
            
            merge_time = time.time() - merge_start
            logger.info(
                f"‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ: {len(all_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (—Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ + —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ) "
                f"–∑–∞ {merge_time:.3f}—Å"
            )
        
        # ============ –ù–û–í–û–ï: Hybrid Search (Vector + BM25) ============
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å BM25 —á–µ—Ä–µ–∑ RRF
        try:
            all_results = hybrid_search(
                query=query,
                collection=QDRANT_COLLECTION,
                vector_results=all_results,
                space_filter=space if space else None,
                limit=search_limit * 2  # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
            )
            logger.info(f"‚úÖ Hybrid Search: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–æ {len(all_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (Vector + BM25)")
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ Hybrid Search: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
        
        # ============ –£–õ–£–ß–®–ï–ù–ò–ï 5: –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è ============
        all_results = deduplicate_results(all_results)
        logger.info(f"–ü–æ—Å–ª–µ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {len(all_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        
        # ============ –£–õ–£–ß–®–ï–ù–ò–ï 6: Adaptive Reranking (—É–º–Ω—ã–π –≤—ã–±–æ—Ä –ª–∏–º–∏—Ç–∞) ============
        # ADAPTIVE: –í—ã–±–∏—Ä–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è reranking –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–ø—Ä–æ—Å–∞
        
        def get_adaptive_rerank_limit(query: str, candidate_count: int, has_space_filter: bool) -> int:
            """
            –£–º–Ω—ã–π –≤—ã–±–æ—Ä –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è reranking.
            
            –§–∞–∫—Ç–æ—Ä—ã:
            1. –î–ª–∏–Ω–∞ –∑–∞–ø—Ä–æ—Å–∞ (–∫–æ—Ä–æ—Ç–∫–∏–µ = –º–µ–Ω—å—à–µ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –Ω—É–∂–Ω–æ)
            2. –ù–∞–ª–∏—á–∏–µ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ space (—É–∂–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ = –º–µ–Ω—å—à–µ –Ω—É–∂–Ω–æ)
            3. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            
            Returns:
                –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è reranking (8-20)
            """
            query_words = len(query.split())
            
            # –ë–∞–∑–æ–≤—ã–π –ª–∏–º–∏—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–ª–∏–Ω—ã –∑–∞–ø—Ä–æ—Å–∞
            if query_words <= 3:
                # –ö–æ—Ä–æ—Ç–∫–∏–π –∑–∞–ø—Ä–æ—Å: "—Å—Ç–µ–∫ RAUII", "–∫–æ–Ω—Ç–∞–∫—Ç—ã –∫–æ–º–∞–Ω–¥—ã"
                base_limit = 8
            elif query_words <= 8:
                # –°—Ä–µ–¥–Ω–∏–π –∑–∞–ø—Ä–æ—Å: "–∫–∞–∫–æ–π —Å—Ç–µ–∫ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è"
                base_limit = 12
            else:
                # –î–ª–∏–Ω–Ω—ã–π/—Å–ª–æ–∂–Ω—ã–π –∑–∞–ø—Ä–æ—Å
                base_limit = 20
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å —Ñ–∏–ª—å—Ç—Ä –ø–æ space - —É–º–µ–Ω—å—à–∞–µ–º –ª–∏–º–∏—Ç (—É–∂–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ)
            if has_space_filter:
                base_limit = max(8, int(base_limit * 0.8))
            
            # –ù–µ –±–æ–ª—å—à–µ —á–µ–º –µ—Å—Ç—å –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            return min(base_limit, candidate_count)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ª–∏–º–∏—Ç
        RERANK_LIMIT = get_adaptive_rerank_limit(query, len(all_results), bool(space))
        logger.info(f"Adaptive rerank limit: {RERANK_LIMIT} (query_words: {len(query.split())}, has_filter: {bool(space)})")
        
        if len(all_results) > RERANK_LIMIT:
            logger.info(f"–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ reranking: {len(all_results)} ‚Üí {RERANK_LIMIT} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            all_results = all_results[:RERANK_LIMIT]
        
        ranker = init_reranker()
        
        if ranker and len(all_results) > 1:
            try:
                start_time = time.time()
                
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä (query, document) –¥–ª—è reranking
                # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –£–º–µ–Ω—å—à–∞–µ–º –¥–ª–∏–Ω—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
                # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ text –Ω–µ None
                # –í–∞–∂–Ω–æ: —Å–æ–∑–¥–∞–µ–º –ø–∞—Ä—ã —Ç–æ–ª—å–∫–æ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å —Ç–µ–∫—Å—Ç–æ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å—ã
                valid_indices = []
                pairs = []
                for i, r in enumerate(all_results):
                    text = r.get('text')
                    if text and isinstance(text, str) and len(text.strip()) > 0:
                        valid_indices.append(i)
                        pairs.append((query, get_text_for_reranking(text, query, max_len=1200)))
                
                if not pairs:
                    logger.warning("–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –ø–∞—Ä –¥–ª—è reranking (–≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–º–µ—é—Ç –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç)")
                else:
                    # –ü–æ–ª—É—á–∞–µ–º scores –æ—Ç CrossEncoder
                    # CrossEncoder –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –±–∞—Ç—á–∏—Ç –∑–∞–ø—Ä–æ—Å—ã (batch_size=32 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
                    scores = ranker.predict(pairs)
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º scores —Ç–æ–ª—å–∫–æ –∫ –≤–∞–ª–∏–¥–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
                    for pair_idx, result_idx in enumerate(valid_indices):
                        if pair_idx < len(scores):
                            all_results[result_idx]['rerank_score'] = float(scores[pair_idx])
                        else:
                            all_results[result_idx]['rerank_score'] = 0.0
                    
                    # –î–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–∑ —Ç–µ–∫—Å—Ç–∞ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º score = 0
                    for i, result in enumerate(all_results):
                        if i not in valid_indices:
                            result['rerank_score'] = 0.0
                
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ rerank score (descending)
                all_results.sort(key=lambda x: x['rerank_score'], reverse=True)
                
                # ============ –ù–û–í–û–ï: Metadata Boost ============
                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º score –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ –≤ title/breadcrumb/page_path
                query_keywords = set(query.lower().split())
                stop_words = {'–≤', '–Ω–∞', '–ø–æ', '–¥–ª—è', '—Å', '–∫', '–∏–∑', '–æ', '–æ–±', '–∏', '–∞', '–Ω–æ', '–∏–ª–∏', '–∂–µ', '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è', '–∫–∞–∫–∏–µ', '–≥–¥–µ', '–∫–∞–∫', '—á—Ç–æ', '—ç—Ç–æ', '–ø—Ä–æ–µ–∫—Ç–∞', '–ø—Ä–æ–µ–∫—Ç'}
                query_keywords = {kw for kw in query_keywords if kw.lower() not in stop_words and len(kw) > 2}
                
                boosted_count = 0
                for r in all_results:
                    if not r or not isinstance(r, dict):
                        continue
                    
                    metadata = r.get('metadata', {})
                    if not isinstance(metadata, dict):
                        continue
                    
                    title = metadata.get('title', '').lower()
                    breadcrumb = r.get('breadcrumb', '').lower()
                    page_path = metadata.get('page_path', '').lower()
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                    title_match = any(kw in title for kw in query_keywords)
                    breadcrumb_match = any(kw in breadcrumb for kw in query_keywords)
                    path_match = any(kw in page_path for kw in query_keywords)
                    
                    if title_match or breadcrumb_match or path_match:
                        current_score = r.get('rerank_score', 0)
                        if current_score >= 0:  # Boost –¥–∞–∂–µ –¥–ª—è score 0
                            # –°–∏–ª—å–Ω—ã–π boost –µ—Å–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–∞–π–¥–µ–Ω—ã
                            r['rerank_score'] = max(current_score * 2.0, 0.1)  # –ú–∏–Ω–∏–º—É–º 0.1
                            r['metadata_boost'] = True
                            boosted_count += 1
                            logger.debug(
                                f"Metadata boost –¥–ª—è '{metadata.get('title', 'Unknown')}': "
                                f"{current_score:.3f} ‚Üí {r['rerank_score']:.3f}"
                            )
                    
                    # ============ –ù–û–í–û–ï: Exact Phrase Boost ============
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π boost –¥–ª—è —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤–∞–∂–Ω—ã—Ö —Ñ—Ä–∞–∑
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å (–¥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è space)
                    original_query_lower = original_query.lower()
                    important_phrases = [
                        '—É—á–µ—Ç –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã',
                        '–Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞',
                        '—Å–∫–ª–∞–¥',
                        '–æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ',
                        '–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è',
                    ]
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Ñ—Ä–∞–∑ (–±–æ–ª–µ–µ —Å–∏–ª—å–Ω—ã–π boost)
                    for phrase in important_phrases:
                        if phrase in original_query_lower:
                            combined_metadata = f"{title} {breadcrumb} {page_path}"
                            if phrase in combined_metadata:
                                # –û—á–µ–Ω—å —Å–∏–ª—å–Ω—ã–π boost –¥–ª—è —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤–∞–∂–Ω—ã—Ö —Ñ—Ä–∞–∑
                                current_score = r.get('rerank_score', 0)
                                r['rerank_score'] = max(current_score * 2.5, 0.2)
                                r['exact_phrase_boost'] = True
                                boosted_count += 1
                                logger.debug(
                                    f"Exact phrase boost –¥–ª—è '{phrase}' –≤ '{metadata.get('title', 'Unknown')}': "
                                    f"{current_score:.3f} ‚Üí {r['rerank_score']:.3f}"
                                )
                                break
                
                if boosted_count > 0:
                    logger.info(f"Metadata boost –ø—Ä–∏–º–µ–Ω–µ–Ω –∫ {boosted_count} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º")
                
                # –ü–µ—Ä–µ—Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ—Å–ª–µ boost
                all_results.sort(key=lambda x: x['rerank_score'], reverse=True)
                
                elapsed = time.time() - start_time
                if all_results:
                    top_score = max((r.get('rerank_score', 0) for r in all_results), default=0)
                    logger.info(f"Reranking completed –∑–∞ {elapsed:.2f}—Å. Top score: {top_score:.3f}")
                else:
                    logger.warning("Reranking completed, –Ω–æ –Ω–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            except Exception as e:
                logger.warning(f"Reranking failed: {e}, using original order")
        
        # ============ –ù–û–í–û–ï: Score Threshold Filtering (–∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π) ============
        # –£–õ–£–ß–®–ï–ù–ò–ï: –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã —Ç—Ä–µ–±—É—é—Ç –±–æ–ª–µ–µ –º—è–≥–∫–æ–≥–æ –ø–æ—Ä–æ–≥–∞
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –≤ –∑–∞–ø—Ä–æ—Å–µ
        technical_terms = ['api', 'http', 'rest', 'json', 'xml', 'sql', 'docker', 
                          'git', '1—Å', '1c', 'endpoint', 'webhook', 'oauth', 
                          'deployment', 'ssl', '—Ç–µ—Å—Ç', '–±–∞–≥', '–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è']
        
        is_technical_query = any(term in query.lower() for term in technical_terms)
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ —á–µ—Ä–µ–∑ ENV –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç
        RERANK_THRESHOLD_TECHNICAL = float(os.getenv('RERANK_THRESHOLD_TECHNICAL', '1.5'))
        RERANK_THRESHOLD_GENERAL = float(os.getenv('RERANK_THRESHOLD_GENERAL', '2.0'))
        
        if is_technical_query:
            MIN_RERANK_SCORE = RERANK_THRESHOLD_TECHNICAL
            logger.debug(f"–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å –æ–±–Ω–∞—Ä—É–∂–µ–Ω, –ø–æ—Ä–æ–≥: {MIN_RERANK_SCORE}")
        else:
            MIN_RERANK_SCORE = RERANK_THRESHOLD_GENERAL
            logger.debug(f"–û–±—â–∏–π –∑–∞–ø—Ä–æ—Å, –ø–æ—Ä–æ–≥: {MIN_RERANK_SCORE}")
        
        original_count = len(all_results)
        all_results = [r for r in all_results if r.get('rerank_score', 0) >= MIN_RERANK_SCORE]
        
        if len(all_results) < original_count:
            filtered_count = original_count - len(all_results)
            logger.info(f"Score threshold filtering: —É–±—Ä–∞–Ω–æ {filtered_count} –Ω–∏–∑–∫–æ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (score < {MIN_RERANK_SCORE})")
        
        if not all_results:
            logger.warning(f"–í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã –ø–æ score threshold ({MIN_RERANK_SCORE})")
            return f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–º–µ—é—Ç –Ω–∏–∑–∫—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (score < {MIN_RERANK_SCORE}). –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å."
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –ø—É—Å—Ç—ã–º —Ç–µ–∫—Å—Ç–æ–º (–¥–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏)
        all_results = [r for r in all_results if r['text'] and len(str(r['text']).strip()) > 0]
        
        if not all_results:
            return f"‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}' (–≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–º–µ–ª–∏ –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç)"
        
        # ============ –£–õ–£–ß–®–ï–ù–ò–ï 7: Context Enrichment ============
        # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ None
        enriched_results = []
        for r in all_results:
            if r and isinstance(r, dict):
                enriched = enrich_result_with_context(r)
                if enriched:
                    enriched_results.append(enriched)
        all_results = enriched_results
        
        if not all_results:
            return f"‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}' (–æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)"
        
        # ============ –£–õ–£–ß–®–ï–ù–ò–ï 8: Context Expansion (Bidirectional + Related) ============
        # –ê–î–ê–ü–¢–ò–í–ù–û: –†–∞—Å—à–∏—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ intent —Ç—Ä–µ–±—É–µ—Ç
        if intent and intent.get('expand_context', True):
            expansion_mode = os.getenv('CONTEXT_EXPANSION_MODE', 'bidirectional').lower()
            context_size = int(os.getenv('CONTEXT_EXPANSION_SIZE', '2'))
            logger.info(f"–†–∞—Å—à–∏—Ä—è—é –∫–æ–Ω—Ç–µ–∫—Å—Ç (mode={expansion_mode}, size={context_size}, intent: expand_context=True)...")
            
            # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª—å embeddings –¥–ª—è related expansion
            embeddings_model = None
            if expansion_mode in ['related', 'all']:
                try:
                    embeddings_model = get_embed_model()
                except Exception as e:
                    logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å embeddings model –¥–ª—è related expansion: {e}")
            
            # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ None
            expanded_results = []
            for r in all_results:
                if r and isinstance(r, dict):
                    expanded = expand_context_full(
                        r,
                        collection=QDRANT_COLLECTION,
                        embeddings_model=embeddings_model,
                        expansion_mode=expansion_mode,
                        context_size=context_size
                    )
                    if expanded:
                        expanded_results.append(expanded)
            all_results = expanded_results
            
            if not all_results:
                logger.warning("–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ—Å–ª–µ context expansion")
        else:
            logger.info("–ü—Ä–æ–ø—É—Å–∫–∞—é context expansion (intent: expand_context=False)")
            # –ó–∞–ø–æ–ª–Ω—è–µ–º expanded_text –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º
            for r in all_results:
                r['expanded_text'] = r.get('text', '')
                r['context_chunks'] = 1
                r['expansion_mode'] = 'disabled'
        
        # ============ –£–õ–£–ß–®–ï–ù–ò–ï 9: Hierarchy & Path Boost ============
        logger.info("–í—ã—á–∏—Å–ª—è—é –±—É—Å—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–µ—Ä–∞—Ä—Ö–∏–∏ –∏ –ø—É—Ç–∏...")
        for r in all_results:
            if not r or not isinstance(r, dict):
                continue
            
            # –ë–∞–∑–æ–≤—ã–π score –æ—Ç reranker
            base_score = r.get('rerank_score', 0.0)
            
            # Hierarchy boost (–≤–∞–∂–Ω–æ—Å—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ)
            # –ê–î–ê–ü–¢–ò–í–ù–û: –£—Å–∏–ª–∏–≤–∞–µ–º –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            metadata = r.get('metadata', {})
            if not isinstance(metadata, dict):
                metadata = {}
                r['metadata'] = metadata
            
            hierarchy_boost = calculate_hierarchy_boost(metadata)
            if intent and intent.get('boost_hierarchy', False):
                hierarchy_boost *= 1.5  # –£—Å–∏–ª–∏–≤–∞–µ–º –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
                logger.debug(f"Hierarchy boost —É—Å–∏–ª–µ–Ω –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {hierarchy_boost:.2f}")
            
            # Breadcrumb match (—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø—É—Ç–∏ —Å –∑–∞–ø—Ä–æ—Å–æ–º)
            breadcrumb_boost = calculate_breadcrumb_match_score(query, r.get('breadcrumb', ''))
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π score
            # –í–µ—Å–∞: rerank (–æ—Å–Ω–æ–≤–Ω–æ–π) + hierarchy (30%) + breadcrumb (20%)
            final_score = (
                base_score * 1.0 +           # –û—Å–Ω–æ–≤–Ω–æ–π score –æ—Ç CrossEncoder
                hierarchy_boost * 0.3 +      # –ë—É—Å—Ç –∑–∞ –≤–∞–∂–Ω–æ—Å—Ç—å –≤ –∏–µ—Ä–∞—Ä—Ö–∏–∏
                breadcrumb_boost * 0.2       # –ë—É—Å—Ç –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø—É—Ç–∏
            )
            
            r['final_score'] = final_score
            r['hierarchy_boost'] = hierarchy_boost
            r['breadcrumb_boost'] = breadcrumb_boost
            
            logger.debug(
                f"Scores for {r['metadata'].get('title', 'Unknown')}: "
                f"rerank={base_score:.2f}, hierarchy=+{hierarchy_boost:.2f}, "
                f"breadcrumb=+{breadcrumb_boost:.2f}, final={final_score:.2f}"
            )
        
        # –ü–µ—Ä–µ—Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º—É score
        all_results.sort(key=lambda x: x.get('final_score', 0), reverse=True)
        if all_results:
            logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–µ—Ä–µ—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã. Top score: {all_results[0].get('final_score', 0):.2f}")
        else:
            logger.warning("–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ—Å–ª–µ –ø–µ—Ä–µ—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏")
        
        # ============ –ù–û–í–û–ï: Diversity Filter (–∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Å ENV) ============
        # –ê–î–ê–ü–¢–ò–í–ù–û: –õ–∏–º–∏—Ç —á–∞–Ω–∫–æ–≤/—Å—Ç—Ä–∞–Ω–∏—Ü—É –∑–∞–≤–∏—Å–∏—Ç –æ—Ç intent –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ ENV
        intent_type = intent.get('type', 'unknown') if intent else 'unknown'
        diversity_limit = get_diversity_limit_for_intent(intent_type)
        logger.info(f"–ü—Ä–∏–º–µ–Ω—è—é diversity filter (max {diversity_limit} chunks/page, intent={intent_type})...")
        filtered_results = apply_diversity_filter(
            all_results, 
            limit=limit, 
            max_per_page=diversity_limit,
            query=query,
            intent=intent
        )
        logger.info(f"–ü–æ—Å–ª–µ diversity filter: {len(filtered_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        
        results = filtered_results
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
        response = [f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (intent={intent_type}, diversity: max {diversity_limit}/page)"
                   f" | pipeline: expansion ‚Üí filter ‚Üí reranking ‚Üí hierarchy ‚Üí diversity:\n"]
        for i, r in enumerate(results, 1):
            if not r or not isinstance(r, dict):
                continue
            
            m = r.get('metadata')
            if not m or not isinstance(m, dict):
                m = {}
                r['metadata'] = m
            page_space = m.get('space', 'Unknown')
            page_url = m.get('url', '')
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º breadcrumb –≤–º–µ—Å—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
            breadcrumb = r.get('breadcrumb', m.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'))
            
            # –ù–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            labels = m.get('labels', '')
            created_by = m.get('created_by', '')
            attachments = m.get('attachments', '')
            chunk_num = m.get('chunk', 0)
            
            # ============ Snippet Extraction –∏–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ ============
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º expanded_text (—Å context expansion)
            text = r.get('expanded_text', r.get('text', "[–¢–µ–∫—Å—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω]"))
            text_preview = extract_relevant_snippet(text, query, max_length=800)
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π score —Å –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–µ–π
            final_score = r.get('final_score', 0)
            rerank_score = r.get('rerank_score', 0)
            hierarchy_boost = r.get('hierarchy_boost', 0)
            breadcrumb_boost = r.get('breadcrumb_boost', 0)
            context_chunks = r.get('context_chunks', 1)
            
            # –≠–º–æ–¥–∑–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç score
            if final_score > 7.0:
                score_emoji = "üî•"
            elif final_score > 5.0:
                score_emoji = "‚≠ê"
            elif final_score > 3.0:
                score_emoji = "‚úì"
            else:
                score_emoji = "¬∑"
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É —Å–æ score
            score_parts = [f"{score_emoji} {final_score:.2f}"]
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—é –µ—Å–ª–∏ –µ—Å—Ç—å –±—É—Å—Ç—ã
            if hierarchy_boost > 0 or breadcrumb_boost > 0:
                score_details = []
                if rerank_score > 0:
                    score_details.append(f"base:{rerank_score:.1f}")
                if hierarchy_boost > 0:
                    score_details.append(f"+hier:{hierarchy_boost:.1f}")
                if breadcrumb_boost > 0:
                    score_details.append(f"+path:{breadcrumb_boost:.1f}")
                score_parts.append(f"({', '.join(score_details)})")
            
            score_str = " | ".join(score_parts)
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
            context_str = ""
            if context_chunks > 1:
                context_str = f" | üìö {context_chunks} chunks"
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            extra_info = []
            if labels:
                extra_info.append(f"üè∑Ô∏è {labels}")
            if created_by:
                extra_info.append(f"üë§ {created_by}")
            if attachments:
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ 3 –ø–µ—Ä–≤—ã—Ö –≤–ª–æ–∂–µ–Ω–∏–π
                att_list = attachments.split(',')[:3]
                att_preview = ', '.join(att_list)
                if len(attachments.split(',')) > 3:
                    att_preview += f" (+{len(attachments.split(',')) - 3})"
                extra_info.append(f"üìé {att_preview}")
            
            extra_str = " | ".join(extra_info)
            if extra_str:
                extra_str = f" | {extra_str}"
            
            response.append(
                f"[{i}] üìç {breadcrumb}\n"
                f"    üìÅ {page_space} | Chunk #{chunk_num} | {score_str}{context_str}{extra_str}\n"
                f"    üîó {page_url}\n"
                f"    üí¨ {text_preview}\n"
            )
        
        # –î–æ–±–∞–≤–ª—è–µ–º fallback_message –µ—Å–ª–∏ –µ—Å—Ç—å
        if fallback_message:
            response.insert(1, f"\n{fallback_message}\n")
        
        final_response = "\n".join(response)
        
        # ============ –ù–û–í–û–ï: –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à ============
        try:
            cache.set(query, final_response, space, limit)
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –∫—ç—à: {e}")
        
        # ============ –ù–û–í–û–ï: –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ============
        try:
            synonyms_manager = get_synonyms_manager()
            synonyms_manager.log_query(query, results)
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å: {e}")
        
        # ============ –ù–û–í–û–ï: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è Semantic Query Log (5-–π –∏—Å—Ç–æ—á–Ω–∏–∫) ============
        expansion_source = 'other'
        try:
            from semantic_query_log import get_semantic_query_log
            
            semantic_log = get_semantic_query_log()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –ª–∏ Semantic Query Log –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
            related_queries = semantic_log.get_related_queries(query, top_n=1)
            if related_queries:
                expansion_source = 'semantic_query_log'
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
            semantic_log.log_query(query, len(results))
            logger.debug(f"Semantic Query Log: –∑–∞–ø—Ä–æ—Å '{query}' –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω ({len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –∏—Å—Ç–æ—á–Ω–∏–∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è: {expansion_source})")
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ Semantic Query Log: {e}")
        
        return final_response
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}", exc_info=True)
        return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –ø–æ–∏—Å–∫–∞: {str(e)}"

@mcp.tool()
def confluence_search_by_label(label: str, limit: int = 5) -> str:
    """
    –ü–æ–∏—Å–∫ —Å—Ç—Ä–∞–Ω–∏—Ü Confluence –ø–æ –º–µ—Ç–∫–µ (label/tag).
    
    Args:
        label: –ù–∞–∑–≤–∞–Ω–∏–µ –º–µ—Ç–∫–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (1-20)
    
    Returns:
        –°–ø–∏—Å–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü —Å –¥–∞–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π
    """
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ RAG –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ —Å–µ—Ä–≤–µ—Ä–∞
        if qdrant_client is None:
            return "‚ùå –û—à–∏–±–∫–∞: RAG —Å–∏—Å—Ç–µ–º–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ —Å–µ—Ä–≤–µ—Ä–∞."
        
        limit = min(max(limit, 1), 20)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        from qdrant_storage import get_qdrant_count, get_all_points
        doc_count = get_qdrant_count(QDRANT_COLLECTION)
        if doc_count == 0:
            return "‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å –ø—É—Å—Ç. –î–æ–∂–¥–∏—Ç–µ—Å—å –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–µ—Ä–≤–æ–π —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏."
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å —Ä–∞–∑—É–º–Ω—ã–º –ª–∏–º–∏—Ç–æ–º –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è OOM
        MAX_SCAN_LIMIT = 10000  # –ú–∞–∫—Å–∏–º—É–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        all_points = get_all_points(limit=MAX_SCAN_LIMIT, include_payload=True, collection=QDRANT_COLLECTION)
        all_data = {
            'documents': [p.get('text', '') for p in all_points.get('points', [])],
            'metadatas': [p.get('metadata', {}) for p in all_points.get('points', [])]
        }
        scanned_count = len(all_data.get('metadatas', []))
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –º–µ—Ç–∫–µ
        matching_results = []
        seen_pages = set()  # –î–ª—è —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ page_id
        
        for idx, metadata in enumerate(all_data.get('metadatas', [])):
            labels_str = metadata.get('labels', '')
            page_id = metadata.get('page_id', '')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–µ—Ç–∫–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –µ—â–µ –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω–∞
            if label.lower() in labels_str.lower() and page_id not in seen_pages:
                matching_results.append({
                    'title': metadata.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'),
                    'space': metadata.get('space', 'Unknown'),
                    'url': metadata.get('url', ''),
                    'labels': labels_str,
                    'parent_title': metadata.get('parent_title', ''),
                    'page_id': page_id
                })
                seen_pages.add(page_id)
                
                if len(matching_results) >= limit:
                    break
        
        if not matching_results:
            warning = f" (–ø—Ä–æ–≤–µ—Ä–µ–Ω–æ {scanned_count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)" if scanned_count >= MAX_SCAN_LIMIT else ""
            return f"‚ùå –°—Ç—Ä–∞–Ω–∏—Ü —Å –º–µ—Ç–∫–æ–π '{label}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ{warning}"
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        response = [f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(matching_results)} —Å—Ç—Ä–∞–Ω–∏—Ü —Å –º–µ—Ç–∫–æ–π '{label}':\n"]
        for i, page in enumerate(matching_results, 1):
            parent_str = f" (‚Üê {page['parent_title']})" if page['parent_title'] else ""
            response.append(
                f"[{i}] üìÑ {page['title']}{parent_str}\n"
                f"    üìÅ Space: {page['space']}\n"
                f"    üè∑Ô∏è –ú–µ—Ç–∫–∏: {page['labels']}\n"
                f"    üîó {page['url']}\n"
            )
        
        return "\n".join(response)
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –º–µ—Ç–∫–µ: {e}", exc_info=True)
        return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–æ –º–µ—Ç–∫–µ: {str(e)}"

@mcp.tool()
def confluence_list_spaces() -> str:
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤ Confluence –¥–ª—è –ø–æ–º–æ—â–∏ –≤ –≤—ã–±–æ—Ä–µ.
    
    –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç—Ç–æ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –∑–Ω–∞–µ—Ç, –≤ –∫–∞–∫–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –∏—Å–∫–∞—Ç—å,
    –∏–ª–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø–µ—Ä–µ–¥ –≤—ã–∑–æ–≤–æ–º confluence_semantic_search.
    
    Returns:
        –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤ —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–∞–∂–¥–æ–º
    """
    try:
        if qdrant_client is None:
            return "‚ùå –û—à–∏–±–∫–∞: RAG —Å–∏—Å—Ç–µ–º–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞."
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑—É–º–Ω—ã–π –ª–∏–º–∏—Ç –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è OOM
        MAX_SCAN_LIMIT = 10000
        from qdrant_storage import get_all_points
        all_points = get_all_points(limit=MAX_SCAN_LIMIT, include_payload=True, collection=QDRANT_COLLECTION)
        all_data = {
            'metadatas': [p.get('metadata', {}) for p in all_points.get('points', [])]
        }
        spaces = {}
        
        for metadata in all_data.get('metadatas', []):
            space_name = metadata.get('space', 'Unknown')
            if space_name:
                spaces[space_name] = spaces.get(space_name, 0) + 1
        
        if not spaces:
            return "‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ."
        
        result = "üìö –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ Confluence:\n\n"
        for space_name, count in sorted(spaces.items(), key=lambda x: x[1], reverse=True):
            result += f"  ‚Ä¢ **{space_name}**: {count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n"
        
        scanned_count = len(all_data.get('metadatas', []))
        if scanned_count >= MAX_SCAN_LIMIT:
            result += f"\n‚ö†Ô∏è –ü–æ–∫–∞–∑–∞–Ω—ã –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –∏–∑ –ø–µ—Ä–≤—ã—Ö {MAX_SCAN_LIMIT} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."
        
        return result
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤: {e}")
        return f"‚ùå –û—à–∏–±–∫–∞: {e}"

@mcp.tool()
def confluence_health() -> str:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –∏–Ω–¥–µ–∫—Å–∞ –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã.
    
    Returns:
        –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç–∞—Ç—É—Å–µ —Å–∏—Å—Ç–µ–º—ã
    """
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ RAG –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ —Å–µ—Ä–≤–µ—Ä–∞
        if qdrant_client is None:
            return "‚ùå –û—à–∏–±–∫–∞: RAG —Å–∏—Å—Ç–µ–º–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ —Å–µ—Ä–≤–µ—Ä–∞."
        
        # –ü–æ–¥—Å—á—ë—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É–µ–º count() –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏)
        try:
            from qdrant_storage import get_qdrant_count
            total_docs = get_qdrant_count(QDRANT_COLLECTION)
        except Exception:
            # Fallback: –æ—Ü–µ–Ω–∫–∞ —á–µ—Ä–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫—É
            from qdrant_storage import get_all_points
            all_points = get_all_points(limit=10, include_payload=True, collection=QDRANT_COLLECTION)
            data = {'points': all_points.get('points', [])}
            total_docs = len(data.get("ids", []))
            if total_docs == 10:
                total_docs = "10+"  # –ë–æ–ª—å—à–µ 10, —Ç–æ—á–Ω–æ–µ —á–∏—Å–ª–æ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ
        
        status = "‚úÖ –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç"
        if total_docs == 0:
            status = "‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å –ø—É—Å—Ç - –æ–∂–∏–¥–∞–Ω–∏–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏"
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Query Rewriting
        rewrite_stats = get_rewriter_stats()
        rewrite_info = (
            f"üìù Query Rewriting: {rewrite_stats['total_requests']} –∑–∞–ø—Ä–æ—Å–æ–≤, "
            f"–∫—ç—à: {rewrite_stats['cache_hit_rate']} ({rewrite_stats['cache_hits']}/{rewrite_stats['total_requests']})"
        )
        
        return (
            f"{status}\n"
            f"üìä –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ: {total_docs}\n"
            f"üîß –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {EMBED_MODEL}\n"
            f"üíæ –ü—É—Ç—å –∫ –ë–î: {CHROMA_PATH}\n"
            f"üîÑ –†–µ–∂–∏–º: {'Ollama' if USE_OLLAMA else 'HuggingFace'}\n"
            f"{rewrite_info}"
        )
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ health check: {e}", exc_info=True)
        return f"‚ùå –û—à–∏–±–∫–∞: {str(e)}"

if __name__ == "__main__":
    logger.info("MCP on 0.0.0.0:8012")
    try:
        mcp.run(transport="streamable-http", port=8012, host="0.0.0.0")
    except KeyboardInterrupt:
        pass
